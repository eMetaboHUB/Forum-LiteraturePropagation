---
title: "Draft"
author: "Delmas Maxime"
date: "20/01/2021"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(DT)
library(ggridges)
```

## La distribution Beta 

Pour la loi Beta, la densité de probabilité est définie lorsque $\alpha, \beta > 0$ et $x \in [0,1]$ par: $$f(x; \alpha, \beta) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$$

$B(\alpha, \beta)$ est la constante de normalisation pour que la probabilité totale soit 1, ainsi $B(\alpha, \beta) = \int_{0}^{1} u^{\alpha - 1} (1 - u)^{\beta - 1} du$. 

One note ainsi $X \sim Beta(\alpha, \beta)$

La loi beta est donc très pratique lorsqu'il s'agit de modéliser des probabilités puisqu'elle est définit entre $[0,1]$

## La binomiale 

La loi binomiale est notamment utilisé pour représenter la probabilité d'observer un nombre $k$ de succès sur une série de $n$ tentatives.
$X \sim Bin(n,p)$
$P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$

## Lien entre la beta et la loi binomiale

Comme on peut le constater, le numérateur de la loi Beta est similaire à ce que l'on peut retrouver dans la loi Binomiale: $x^{\alpha - 1}(1 - x)^{\beta - 1}$ ressemble à $p^k (1 - p)^{n - k}$.
Si on considère que $x$ dans la loi beta est l'équivelent de $p$ dans la loi binomiale, en considérant aussi que $\alpha - 1$ est le nombre de succès $k$ et $\beta - 1$ le nombre d'échec $n - k$, les deux tendent à représenter la probabilité d'observer $k$ succès sur $n$ tentatives. Bien sur les deux ne représentent pas exactement la même chose, mais l'inuition est là.

De même, dans le calcul de $P(X = k)$, c'est la partie $p^k (1 - p)^{n - k}$ qui est véritavblement essentielle, $\binom{n}{k}$ étant simplement une constante lorsqu'on observe une expérience.

La moyenne d'une variable suivant une loi beta est $E[X] = \frac{\alpha}{\alpha + \beta}$. Toujours en imaginant que $\alpha$ et $\beta$ représente respectivement le nombre de succès et d'échec, l'espérance d'une variable suivant une $Beta$ peut s'interpréter comme la moyenne de succès. Une autre manière de voir la fonction Beta est aussi en reparamétrant la fonction avec les paramètres $\mu$ et $\sigma$ tel que: $\mu = \frac{\alpha}{\alpha + \beta}$ et $\sigma = \frac{1}{\alpha + \beta}$. Ainsi, $\alpha$, le nombre de succès vaut $\frac{\mu}{\sigma}$ et $\beta$ le nombre d'échec vaut  $\frac{(1 - \mu)}{\sigma}$.

On peut retrouver dans la literature une autre manière, simplement $\sigma = (\alpha + \beta)$, et donc $\alpha = \mu \sigma$ et $\beta = (1 - \mu) \sigma$

L'idée avec cette re-paramétrisation c'est que $\mu$ représente la probabilié moyenne de succès (l'espérance moyenne de succès) et $\sigma$ est appelé le paramètre de dispersion, qui va nous permettre de gérer la variabilité, l'incertitude, que l'on a autour de cette moyenne. En constatant que $\sigma$ est directement fonction de $\alpha + \beta$, on voit que l'on exprime notre variabilité autour de cette moyenne en fonction du nombre d'expériences totales réalisées : nb.succès + nb.échecs. Plus le nombre d'expériences est élevé, plus la variabilité est faible. Petite précision pour ne pas confondre: $\sigma$ est notre paramètre de dispersion, mais dans la literature on pourra entendre parler de $\phi$ ou de $\rho$ que l'on appelle le paramètre de *sur-dispersion*. Car comme on peut le voir il représente l'augmentation de la variabilité du à l'utilisation de notre $Beta()$, mais attention c'est différent de $\sigma$. Sur la doc de Gamlss on peut voir que la variabilité de notre $Beta-binomiale$ vaut: $Var(Y) = n \mu (1 - \mu)[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}]$ en utilisant les $\mu$ et $\sigma$ paramètrisation. (Sur la doc il donne la sd, mais c'est simplement la racine de la variance). Or on a don cque $(\sigma + 1) = \frac{1 + \alpha + \beta}{\alpha + \beta}$ et donc : $\frac{(n - 1)\sigma}{(\sigma + 1)} = \frac{(n - 1)}{\alpha + \beta} \times \frac{\alpha + \beta}{1 + \alpha + \beta} = \frac{(n - 1)}{1 + \alpha + \beta}$ Ainsi: $[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}] = [1 + \frac{(n - 1)}{1 + \alpha + \beta}]$. C'est alors qu'on pose $\rho$ ou $\phi = \frac{1}{1 + \alpha + \beta}$, donnant ainsi: $[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}] = [1 + (n - 1)\phi]$

Pour notre variabilité on a donc: $Var(Y) = n \mu (1 - \mu) [1 + (n - 1)\phi]$. la propriété très interessant par rapport à $\phi$ c'est donc que lorsque l'on fixe le paramètre de sur-dispersion $\phi = 0$, on a que $Var(Y) = n \mu (1 - \mu)$, ce qui est **exactement** la variabilité d'une binomiale classique! Ceci montre que sans sur-dispersion apporté par notre $Beta()$, notre modèle est simplement une binomiale ! Ce paramètre ajoute donc de la variance à ntore Beta !!! :)

Mais attention, ce que l'on fit avec GAMLSS c'est $\mu$ et $\sigma$, on ne fit **pas** le paramètre de sur-dispersion directement !! Par contre on voit très bien le lien entre $\sigma = \frac{1}{\alpha + \beta}$ et $\phi = \frac{1}{\alpha + \beta + 1}$, les deux sont tout de même largement corrélés quand $\sigma$ est grand, globalement $\phi$ est grand aussi. Donc les deux tendent à refléter la même chose, la dispersion, mais ne sont pas tout à fait égaux  :)

## Les estimateurs

### Estimateurs du Maximum de Vraisemblance (MLE)

La manière classique d'estimé notre paramètre $p$, est d'utilisé l'estimateur du maximum de vraisemblance (**MLE**), avec $p = \frac{k}{n}$

Proof: 
On a observé $k$ succès et $n$ échec, on cherche la valeur de $p$ qui maximise la vraisemblance. 

La fonction de vraisemblance de la loi binomiale étant $L(k; n, p) = \binom{n}{k} p^k (1 - p)^{n - k}$, on cherche donc $p$ tel qu'il maximise la probabilité d'observer $k$ succès parmis $n$ tirages:

$$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} L(k; n, p)$$
$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} \binom{n}{k} p^k (1 - p)^{n - k}$

On va étudier la log-vraisemblance qui sera plus pratique pour dérivé:

$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} \ln{\binom{n}{k}} + \ln{p^k} + \ln{(1 - p)^{n - k}}$

Pour trouver le maximum, par rapport à $p$, il suffit de regarder lorsque que la dérivé s'annule par rapport à $p$:

$\frac{d \ln L(k; n, p)}{dp} = 0$

$k \frac{1}{p} - \frac{1}{(1 - p)} (n - k) = 0$

On multiplie tout par $p(1 - p)$ pour simplifier :

$k \frac{p(1 - p)}{p} - \frac{p(1 - p)}{(1 - p)} (n - k) = 0$

$k (1 - p) - p(n - k) = 0$

$k - pk - pn + pk = 0$

$k - pn = 0$

$p = \frac{k}{n}$

On voit retrouve bien l'estimateur MLE !

L'estimateur du maximum de vraisemblance est donc une valeur fixé, calculé en considérant le résultat observée de l'expérience. Ici, on considère les données comme étant un tirage aléatoire à partir de la population totale. L'incertitude sur cet estimateur peut être calculé à partir de l'erreur d'échantillonnage: Si je refaisait plein de fois l'expérience, qu'elle serait la variabilité que j'observerai sur mon paramètre $\hat{p}_{MLE}$ ?
Intuitivement, on peut voir que plus la taille de mon échantillon $n$ est grand, plus je m'attend à ce que mon paramètre estimé soit proche de la vraie valeur de se paramètre dans la population.


### Bayesian estimator

L'estimateur Bayesien est une autre façon de voir les choses. On va considérer que le paramètre que l'on cherche à estimer $\hat{\theta}$ est une variable aléatoire, et on cherche à étudier la distribution de ce paramètre au vue des données observées. On définit un prior sur la distribution de ce paramètre, c'est à dire une idée de sa distribution sans avoir vue les données. Ensuite, à partir des données, on va mettre à jour notre prior en utilisant la fonction de vraisemblance des données par rapport à notre paramètre. Ce que l'on appelle l'estimateur Bayesien de notre paramètre est alors simplement la moyenne de la distribution **à posteriori** de notre paramètre:

$\hat{\theta} = E[\theta|x] = \int \theta p(\theta|x) d\theta$
 
Ce qu'on cherche c'est donc notre distribution à posteriori $p(\theta|x)$. C'est là que rentre en compte le théorême de Bayes, où :

 $p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d \theta}$

Ici $p(\theta)$ est notre prior, il représente la distribution de $\theta$ a priori, sans connaissance des données. 

$p(x|\theta)$ représente alors la vraisemblance, la proabilité d'observer nos données $x$ sachant le paramètre $\theta$. Au dénominateur, on a l'intégrale sur toutes les valeurs de $\theta$ afin d'obtenir une densité de probabilité (on 'normalise').

Dans le cas de notre expérience binomiale, on connait notre fonction de vraissemblance, c'est: $L(k; n, p) = P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$ c'est la probabilité d'observer $k$ succès sur $n$ tentatives avec un certain paramètre $p$

Une propriété importante est celle des prior conjugués: 
Dans la théorie bayésienne des probabilités, si la distribution postérieure $p(\theta|x)$ est de la même famille de distribution de probabilités que la distribution à priori $p(\theta)$, les distributions **a priori** et **postérieure** sont alors appelées **distributions conjuguées**, et la distribution **a priori** est appelée le **prior conjugué** pour la fonction de vraisemblance $p(x | \theta)$.

Dans le cas de la distribution binomiale, le prior conjugué est la loi Beta ! Ainsi en posant un prior suivant une distribution $Beta$ et en calculant la vraisemblance avec une binomiale, la distribution de porbabilité postérieure suit une loi $Beta$


Ainsi pour le paramètre $p$ de notre binomiale, on a : $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$, avec  :

$p(k|p = x, n) =  \binom{n}{k} x^k (1 - x)^{n - k}$

$p(p = x) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$

Schématiquement cela représente $p(\theta = x| data) = \frac{p(data | \theta = x) p(\theta = x)}{p(data)}$

Étant un prior conjugué, le résultat est : $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy} = \frac{x^{k + \alpha - 1}(1 - x)^{(n - k) + \beta - 1}}{B(k + \alpha, (n - k) + \beta)}$ 

Où la distribution beta étant un prior conjugué de la binomiale, on retrouve une distribtion a posteriori $Beta$ de paramètre $Beta(k + \alpha, (n - k) + \beta)$

On peut donc dire que: $p(p = x|k, n, \alpha, \beta) \sim Beta(k + \alpha, (n - k) + \beta)$

Voir https://en.wikipedia.org/wiki/Conjugate_prior pour les details

Connaissance notre distribution à posteriori et sachant que notre estimateur Bayesien est simplement l'espérance de cette distribution, dans le cas d'une binomiale, notre estimateur est donc simpelment l'espérance de la distribution Beta à posteriori :
$$\hat{p}_{Bayes} = E[p|k,n] = \frac{\alpha + k}{\alpha + \beta + n}$$

En effet $\frac{\alpha + k}{\alpha + \beta + n}$ étant l'espérance d'une distribution $Beta(k + \alpha, (n - k) + \beta)$$

### Lien avec l'estimateur MLE

Comme on peut le voir, $\hat{p}_{Bayes} = \frac{\alpha + k}{\alpha + \beta + n}$ et  $\hat{p}_{MLE} = \frac{k}{n}$, on peut constater qu'en réalité ces deux estimateur sont très proches ! En effet, lorsque le nombre de tentative $n$ va augmenter, les paramètre du prior $\alpha$ et $\beta$ n'auront plus trop d'influence sur l'espérance calculée. Ex: si $\alpha = \beta = 1$ et $k = 100$ et $n = 1000$: 

$\frac{1 + 100}{1 + 1 + 1000} \approx \frac{100}{1000}$

Ainsi nos deux estimateur tendent vers la même chose !!

Une autre propriété qui est très intéressante, est lorsque l'on utilise une loi uniforme comme prior. Pour définir une loi Uniforme avec une Beta, c'est très simple c'est simplement $Beta(1,1)$. Comme on va le voir, en utilisant un prior uniforme, le **MAP** (Maximum a posteriori probability), soit la valeur de notre paramètre la plus probable à posteriori, correspond à l'estimateur MLE ! Attention, on parle du MAP, pas de la moyenne de la distribution car l'estimateur bayesien est différent du MLE. Néanmoins, dans notre distribution à posteriori, la valeur la plus probable est l'estimateur du MLE:

$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(\theta|x)$, soit le $\theta$ qui maximise les proba à posteriori.

$$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(\theta|x) = \underset{\theta}{\operatorname{argmax}} \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d \theta} = \underset{\theta}{\operatorname{argmax}} p(x|\theta)p(\theta)$$

En effet, le dénominateur dans l'équation du posteriori est seulement un facteur de normalisation, mais il n'influe pas sur le maximum.
Deplus, la distribution $Beta(1,1)$ étant complètement uniforme, le facteur $p(\theta)$ est également complètement inutile car il vaut toujours $1$. 

```{r bintest2}
p = seq(0,1, length=100)
plot(p, dbeta(p, 1, 1), ylab="density", type ="l", col=4)
```

Ainsi on se retrouve avec 
$$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(x|\theta)$$

Or $p(x|\theta)$ c'est ma fonction de vraisemblance, donc dans le cas de la loi binomiale, on a que $\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} L(k; n, p)$ ce qui est exactement la définition de mon estimateur du maximum de vraisemblance **MLE**

## Improper priors

Comme on le voit en utilisant une distribution Uniforme (ex $Beta(1,1)$), le MAP de la distribution à posteriori correspond au MLE. Mais il y a d'autres propriété intéressantes. 
On appelle improper prior une distribution à priori qui n'intègre pas à 1. Dans notre exemple, tout va bien $Beta(1,1)$ intègre bien à 1, c'est d'ailleurs simplement un carré de 1 de coté si on simplifie. Mais d'autres distributions comme beta(0,0), ou, plus simplement dans le cas d'une loi normale, si on imagine un prior qui définis une probabilité équivalente pour toutes les valeurs de $\mu$, ceci n'intègre pas à 1 ...

Mais normalement ce n'est pas vraiment un problème. Comme on peut le voir dans la règle de Bayes, en fait, la likehood ($p(x|\theta)$) est pondéré par le prior ($p(\theta)$), c'est un poids. La fonction de likehood, quant à elle, n'est généralement pas positive pour dans tout l'espace, elle atteint un maximum pour une certaine valeur et tend vers 0 lors que l'on s'éloigne de cette valeur. Ainsi en réalité même si on multiplie par le prior, l'intégrale au dénominateur est finie car la likehood tendra vers 0 pour des valeurs éloignés de $\theta$. Ainsi, même si le prior n'est pas une vrai distribution, on va quand même faire une pondération entre de numérateur à certain $\theta$ et le dénominateur. Ainsi même en utilisant un prior improper, la distribution à posteriori peut être une bonne distribution.

### Using Beta(1,1)

Lorsque l'on utilise la $Beta(1,1)$, en plus que le MAP soit équivalent au MLE, il y a une autre propriété: Notre distribution a posteriori est proportionnelle à la vraisemblance.

En effet, si on a $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$, avec notre prior $p(p = x)$ issue d'une $Beta(1,1)$, c 'est donc que pour tout $x \in [0,1]$, $p(x) = 1$. Ainsi ce facteur est complètement annecdotique dans notre évaluation de la propriété à posteriori, qui équivaut donc à : 

$$p(p = x|k, n) = \frac{p(k|p = x, n)}{\int p(k|p = y, n) dy}$$ 

On voit bien que notre distribtion à posteriori est donc complètelement proportionnelle à la vraisemblance $p(k|p = x, n)$. Lorsque l'on utilise un prior non-informatif ($Beta(1,1)$), la distribution à posteriori est uniquement influé par les données, donc la vraisemblance, car notre prior ne donne aucune indication particulière ! Ainsi en choisissant un prior non-informatif on utilise uniquement l'information des données et notre distribution à postériori suit la distribution de notre vraisemblance !



## En conclusion: 

- L'estimateur Bayesien de mon paramètre $p$ est $\hat{p}_{Bayes} = \frac{\alpha + k}{\alpha + \beta + n}$. Il s'agit de la moyenne de la distribution à posteriori de $p$, soit $E[p|k,n]$ obtenu en utilisant le prior conjugué $Beta(\alpha, \beta)$. Où d'après la règle de Bayes: 
$$p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$$

$p(k|p = x, n)$ étant ma fonction de vraisemblance (Likehood), la probabilité d'observée mon nombre de succès $k$ sachant que $p=x$ ($n$ étant fixé). Cela suit donc une binomiale et ainsi: $p(k|p = x, n) =\binom{n}{k} x^k (1 - x)^{n - k}$

$p(p = x)$ est mon prior, c'est à dire la distribution que j'ai de $p$ sans avoir obervé de données. Il suit une distribution $Beta(\alpha, \beta)$ et donc $p(p = x) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$

- L'estimateur du MLE et l'estimateur Bayesienne converge vers la même valeur à mesure que le nombre de tentative augmente -> Quand la taille des échantillons considéré augmente, on est naturellement plus précis sur le paramètre estimé et donc les deux tendent vers la même valeur.

- En utilisant un prior non-informatif: La valeur la plus probable dans la distribution a posteriori (MAP) est équivalent à l'estimateur du MLE ! ET la distribution à posteriori est directement proportionnelle à la vraisemblance ! Ce sont nos données qui donne la distribution à posteriori sans aucune influence du prior.



## Exemple des propriétés:

On va reprendre l'idée du premier exemple, mais cette fois on va se placer dans une situation réelle. On observe nos données et on va chercher à estimer $p$

- On observer $k = 20$ succès parmis $n = 100$ tentatives.
Trouver $p$ en ulisant l'estimateur du maximum de vraisemblance, consiste à trouver $p$ tel qu'il maximise $P(X = 20) = \binom{100}{20} p^{20} (1 - p)^{100 - 20}$

L'estimateur du maximum de vraisemblance est donc $\hat{p}_{MLE} = \frac{k}{n} = \frac{20}{100} = 0.2$

En effet, si on cherche à ploter comment varie la vraisemblance ($P(X = 20)$) en fonction de p, voilà ce que l'on obtient :

```{r bintest3}
p.test <- seq(0,1,0.0001) 
v.vrais <- dbinom(x = 20, size = 100, prob = p.test)
max <- p.test[which(v.vrais == max(v.vrais))]
surf.inf <- sum(v.vrais[0:(which(v.vrais == max(v.vrais)) - 1)])
surf.sup <- sum(v.vrais[(which(v.vrais == max(v.vrais)) + 1):length(v.vrais)])
ggplot(data.frame(proba = p.test, Likehood = v.vrais), aes(x = proba, y = Likehood)) +
  geom_line() + 
  theme_classic() + 
  geom_vline(xintercept = max, 
                color = "red", size=1) +
  geom_point(data=data.frame(proba = max, Likehood = max(v.vrais)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  geom_text(aes(x = max + 0.1, label=paste("MLE = ", max), y=0.11), colour="red")
```
Sur cette figure, on cherche le point avec la vraisemblance la plus élevé.Comme prédit par le MLE, on trouve $p = 0.2$, notre estimateur MLE désigne bien le *maximum* de vraisemblance

Ce que l'on visualise ici c'est la vraisemblance par rapport au paramètre $p$. Attention, ce c'est pas une distribution de probabilité ! c'est simplement la vraisemblance ..

Quelques petites remarques importante: On voit que la distribution de la vraisemblance atteint un maximum à $p = 0.2$, mais sa distribution n'est pas symétrique autour de cette valeur. Si on essaye d'estimer l'aire (en sommant les valeurs à gauche ou à droite de la ligne) à gauche du MLE, on à ~ `round(surf.inf, 2) ` et à droite on a `round(surf.sup, 2) `. 

Si on utilise un estimateur Bayesien avec un prior uniforme, $beta(1,1)$, alors notre distribution à postériori srea proportionnelle à la vraisemblance, elle aura la même forme, la moyenne que l'on va estimer sera intuitivement légère supérieure au MLE observée du fait de la non-symétrie dans l'exemple.

Pour pouvoir visualiser correctement notre esimtateur Bayesien, on va transformé le graph ci-dessus en un histograme. Pour se faire, pour chaque valeur de p, on va généré 1000 tirages aléatoires suivant une binomiale avec ce paramètre $p$ et $n = 100$. Ensuite on va construire l'histogramme de la distribution des probabilités $p$ lorsque le $k$ obtenu égal 20 pour visualisé leur distribution. On replot également une ligne vertical à 0.2

```{r bintest4}
v.p <- seq(0,1,0.001)
data <- data.frame(X = c(), p = c())
for(p in v.p){
  sampling <- rbinom(n = 1000, size = 100, prob = p)
  data <- rbind(data, data.frame(X = sampling, p = rep(p, 1000)))
}
data <- data %>% filter(X == 20)
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
    geom_vline(xintercept = 0.2, 
                color = "red", size=1)
```


On voit que cela suit la même allure que la distribution des vraisemblances construite précedement.

Connaissant les propriétés évoqués précedemment, on sait qu'en utilisant un prior non-informatif $B(1,1)$, notre distribution à posteriori sera proportionnelle à la vraisemblance, et, son MAP sera équivalent au MLE. 
Sachant que $k = 20$ et $n = 80$, en utilisant un prior $B(1,1)$, on sait que notre distribution de $p$ à posteriori:

$p(p = x|k, n, \alpha, \beta) \sim Beta(k + \alpha, (n - k) + \beta)$

et donc

$p(p = x|20, 100, 1, 1) \sim Beta(20 + 1, (100 - 20) + 1)$

$$p(p = x|data) \sim Beta(21, 81)$$
Donc on refait le même plot en superposant :

```{r bintest5}
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
  stat_function(fun = dbeta, args = list(shape1 = 21, shape2 = 81), aes(colour = "Beta(21,81)"), size=1.5, color = "red")
```

Comme on peut le voir notre distribution $Beta(21,81)$ fit parfaitement la distribution de nos probabilité $p$ lorsque $k = 20$, correspondant à la distribution de la vraisemblance par rapport à $p$. 

Notre distribution à posteriori est bien proportionnelle à la vraisemblance, elle a la même allure. Elle est en quelque sorte sa version normalisée, nous donnant une distribution de probabilité.

Si on cherche le MAP associé, que l'on peut trouver en calculant le mode associé à cette distribution $Beta$, on obtient que:

$MAP = \frac{\alpha - 1}{\alpha + \beta - 2} = \frac{21 - 1}{21 + 81 - 2} = \frac{20}{100} = 0.2$

Ce qui correpond effectivement à notre MLE !

On peut donc calculer la valeur de notre estimateur bayesien: $\hat{p}_{Bayes} = E[p|k, n, \alpha, \beta)]$, soit l'espérance de notre distribution à posteriori

On calcule donc l'espérance de notre distribution $Beta(21,81)$

$\hat{p}_{Bayes} = E[p|k, n, \alpha, \beta)] = \frac{21}{21 + 81} \approx 0.2059$


Maintenant on va tout ploter en même temps :

```{r bintest6}
map = 0.2
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,0.5) +
  geom_point(data=data.frame(proba = map, Likehood = dbeta(map,21,81)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  annotate(geom = "text", x= map + 0.1, y = dbeta(map,21,81) + 0.1, label="MAP <-> MLE ") +
  stat_function(fun = dbeta, args = list(shape1 = 21, shape2 = 81), aes(color = "Beta(21,81)"), size = 1.5, color = "red") +
  geom_vline(aes(xintercept = map, 
                color = "MLE"), size = 2) + 
  geom_vline(aes(xintercept = (21/(21+81)), 
                color = "BAYES"), size = 2) +
  scale_color_manual(name = "Estimators", values = c(MLE = "blue", BAYES = "green", "Beta(21,81)" = "blue"))

```


Comme attendu, l'estimateur Bayesien est légèrement supérieur à l'estimateur MLE. Cela est notamment due au fait qu'avec l'estimateur Bayesien on estime l'espérance de la distribution, où comme on l'a vue, celle ci n'est pas symétrique. L'intégrale est plus grande à droite du MAP qu'a gauche, ainsi, on voit que la moyenne estimé va être légère supérieure au MAP avec l'estimateur bayesien. Néanmoins c'est un bon estimateur de la moyenne de la distribution observée, puisque effectivement si on cherche à calculer la moyenne derrière cet histogramme on obtient `r mean(data$p)` ce qui est proche du $\hat{p}_{Bayes} \approx 0.2059$


Aussi, on a évoqué que cette différence disparaissait à mesure que le nombre de tentatives observées augmente.
Si on refait le même calcul est prenant en compte dix fois plus d'observations, voilà ce que l'on obtient:

```{r bintest7}
v.p <- seq(0,1,0.001)
data <- data.frame(X = c(), p = c())
for(p in v.p){
  sampling <- rbinom(n = 1000, size = 1000, prob = p)
  data <- rbind(data, data.frame(X = sampling, p = rep(p, 1000)))
}
data <- data %>% filter(X == 200)
map = 0.2
ggplot(data, aes(x = p)) + 
  theme_classic() +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
  geom_point(data=data.frame(proba = map, Likehood = dbeta(map,201,801)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  annotate(geom = "text", x= map + 0.1, y = dbeta(map,201,801) + 0.1, label="MAP <-> MLE ") +
  stat_function(fun = dbeta, args = list(shape1 = 201, shape2 = 801), aes(color = "Beta(201,801)"), size = 1) +
  geom_vline(aes(xintercept = map, 
                color = "MLE"), size = 0.5) + 
  geom_vline(aes(xintercept = (201/(201+801)), 
                color = "BAYES"), size = 0.5) +
  scale_color_manual(name = "Estimators", values = c(MLE = "red", BAYES = "green", "Beta(201,801)" = "blue"))

```

On observe $k = 200$ succès pour $n = 1000$ tentatives

L'estimateur MLE est toujours égal à 0.2: $\hat{p}_{Bayes} = 0.2$

On utilise toujours un prior non-informatif $Beta(1,1)$, mais cette fois notre distribution posterireure suit donc une $Beta(201, 801)$

Le MAP $\frac{201 - 1}{201 + 801 - 2} = \frac{200}{1000} = 0.2 = \hat{p}_{Bayes}$ est toujours équivalent au MLE

Mon estimateur bayesien vaut désormais: $\frac{201}{201 + 801} \approx 0.2006$ ce qui se rapporche beaucoup donc du MLE. Comme vue précédemment, à mesure que $k$ et $n$ sont grand, notre prior définit par notre $Beta(\alpha, \beta)$ devient de plus en plus négligeable, on beaucoup croire les données !



## Les modèles de mélanges :

La fonction de densité de proababilité d'un modèle de mélange peut être définis tel que: $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$ avec $\sum_i k_i = 1$

-  Les poids $k_i$ représente la proportion de chaque distribution dans la distribution totale

Dans un modèle de mélange, c'est comme si chaque $x$ avec une probabilité $k_i$ d'être tiré depuis la ième distribution $f_i$, et dans chaque distribution $f_i$, la proabilité d'observer $x$ est donnée par $f(x, \theta_i)$


Si on étudie un modèle de mélange de distibution Beta, on a donc $$f(x; \alpha_1, \beta_1, \alpha_2, \beta_2, ..., \alpha_n, \beta_n = \sum_{i}^{n} k_i f(x; \alpha_i, \beta_i)$$

Calculer les probabilité à posteriori sur un modèle de mélange Beta est assez similaire au cas classique. C'est  assez rapide à calculer notamment grâce au fait que la loi Beta est le prior conjugué de la binomiale !

Dans le cas classique on utilisait la règle de Bayes avec :

$p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$

$n$ étant fixé, on pourrait même le retiré du "sachant que"

Maintenant on va utiliser une écriture plus rigoureuse et définir les $p(...)$ avec les fonction de densité $f$, plus correcte pour écrire des modèles de mélange. On reformule donc notre règle de bayes en : $f_{post}(p|k, n) = \frac{f_{bin}(k|p, n) f_{prior}(p)}{\int f_{bin}(k|p, n) f_{prior}(p) dp}$

$f_{bin}(k|p, n)$ étant notre fonctione likehood

Ainsi, la seule différence par rapport à ce qui a été présenté précédemment, est que $f_{prior}(p)$ est maintenant une distribution formée d'un mélange de $Beta$.

$f_{prior}(p;  \alpha_1, \beta_1, \alpha_2, \beta_2, ..., \alpha_n, \beta_n) = \sum_{i}^{n} k_i f(p; \alpha_i, \beta_i)$

On a donc que : 

$$f_{post}(p|k, n) = \frac{f_{bin}(k|p, n) f_{prior}(p)}{\int_{0}^{1} f_{bin}(k|p, n) f_{prior}(p) dp}$$
$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\int_{0}^{1} \sum_{i}^{n} k_i  f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp}$$
On va noter $C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$

On va utiliser $C_i$ pour simplifier l'expression. Au dénominateur, on remarque que l'on fait l'intégrale d'une somme. Grâce à la *sum rule*, on sait que l'intégrale d'une somme, c'est la somme des intégrales, donc va va pouvoir simplifier en utilisant $C_j$, le $k_j$ pouvant sortir de l'expression car c'est une constante. Pour le dénominateur on a donc que $\int_{0}^{1} \sum_{i}^{n} k_i  f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)(p) dp  = \sum_{i}^{n} k _i \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp = \sum_{i}^{n} k _i C_i$

$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\sum_{i}^{n} k _i C_i}$$
On voit qu'au numérator on a une somme de $f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)$ pondérés par les $k_i$. Or c'est exactement le numérator des probabilités à postériori indépendament pour chaque $i$. Ainsi, si on divise par $C_i$, on va obtenir $\frac{f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp}$, ce qui est notre probabilité à posteriori pour la compodante $i$ ! Vue que la $Beta$ est un prior conjugué de la binomiale, on sait que cette distribution est très facile a calculé, on va donc faire apparaitre à nouveau des $C_i$ pour pouvoir simpliquer notre expression avec les probabilité à posteriori de chaque composante $i$ :
$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} (k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) C_i)/C_i}{\sum_{i}^{n} k _i C_i}$$

On peut donc simplifier en :

$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i C_i f_i^{(0)}(p|k, n))}{\sum_{i}^{n} k _i C_i}$$
On note ainsi $f_{i}^{(0)}(p|k, n)$ la distribution à posteriori de la ième composante, afin de la distinguer de la distribution à psoteriori globale du moldèle de mélange.


Maintenant on va à nouveau simplifier l'écriture de cette expression afin de retrouver une expression qui ressemble à un modèle de mélange. On va poster $W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i}$ et on reformule :

$$f_{post}(p|k, n) = \sum_{i}^{n} W_i f_i^{(0)}(p|k, n)$$
On retrouve ainsi une expression dans laquelle la distribution à posteriori de notre modèle de mélange est une somme pondéré par des poids $W_i$ des distribution à posteriori de chacune des $i$ distribution formant ce mélange. Avec :

$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i}$, 

$f_i^{(0)}(p|k, n) = \frac{f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{C_i}$, 

$C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$


Alors, $f_i^{(0)}(p|k, n)$ étant simplement la distribution à posteriori pour la composante $i$, on sait que :

$$f_i^{(0)}(p|k, n) = Beta(\alpha_i + k, \beta_i + (n - k))$$ 


Ne reste plus qu'a déterminer $W_i$ !

Pour simplifier $W_i$, le mieux est de commencer par simplifier $C_i$

$C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$

On peut aussi remarque que $C_i$ définit une distribution Beta binomiale ! $C_i = f_{BetaBin}(k; \alpha_i, \beta_i, n)$

$C_i = \int_{0}^{1}  \binom{n}{k} p^k (1 - p)^{n - k} \times \frac{p^{\alpha_i - 1}(1 - p)^{\beta_i - 1}}{B(\alpha_i, \beta_i)} dp$

$\iff C_i = \int_{0}^{1}  \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)}  p^{k + \alpha_i - 1} (1 - p)^{n - k + \beta_i - 1}dp$

On peut aussi sortir les constantes de l'intégrale 

$C_i = \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)} \int_{0}^{1} p^{k + \alpha_i - 1} (1 - p)^{n - k + \beta_i - 1}dp$

La fonction bêta $B(x,y) = \int_{0}^{1} p^{x - 1} (1 - p)^{y - 1} dp$, on a donc :

$C_i = \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)} \times  B(k + \alpha_i, n - k + \beta_i)$

$\iff  \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}$

Alors pour savoir comment calculer ça, il faut retourner à l'équivalence numérique de la fonction bêta, avec : 

$B(x,y) = \frac{(x + y)}{xy}\frac{x! y!}{(x + y)!}$, alors :

$C_i = \binom{n}{k} \times \frac{k + \alpha_i + n - k + \beta_i}{(k + \alpha_i)(n - k + \beta_i)} \frac{(k + \alpha_i)! (n - k + \beta_i)!}{(k + \alpha_i + n - k + \beta_i)!} \times \frac{\alpha_i \beta_i}{(\alpha_i + \beta_i)}\frac{(\alpha_i + \beta_i)!}{\alpha_i ! \beta_i !}$

Ce qu'il faut voir ici c'est que les factorielle au numérateur et au dénominateur dans les deux partie de l'expression vont se simplifier en utilisant les autres facteurs. Par exemple la partie $\frac{k + \alpha_i + n - k + \beta_i}{1} \frac{1}{(k + \alpha_i + n - k + \beta_i)!} $ va se simplifier car le dénominateur de la partie de gauche représente le premier élément de la factorielle, ce qui va donc donner la factorielle à une valeur en dessous, soit $\frac{k + \alpha_i + n - k + \beta_i}{1} \frac{1}{(k + \alpha_i + n - k + \beta_i)!} = (k + \alpha_i + n - k + \beta_i - 1)!$

On a donc finalement : 
$C_i = \binom{n}{k} \times \frac{k + \alpha_i + n - k + \beta_i}{(k + \alpha_i)(n - k + \beta_i)} \frac{(k + \alpha_i)! (n - k + \beta_i)!}{(k + \alpha_i + n - k + \beta_i)!} \times \frac{\alpha_i \beta_i}{(\alpha_i + \beta_i)} \frac{(\alpha_i + \beta_i)!}{\alpha_i ! \beta_i !} = \binom{n}{k} \frac{(k + \alpha_i - 1)! (n - k + \beta_i - 1)!}{(k + \alpha_i + n - k + \beta_i - 1)!} \times \frac{(\alpha_i + \beta_i - 1)!}{(\alpha_i - 1)! (\beta_i - 1)!}$

$$C_i = \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)} = \binom{n}{k} \frac{(k + \alpha_i - 1)! (n - k + \beta_i - 1)!}{(k + \alpha_i + n - k + \beta_i - 1)!} \times \frac{(\alpha_i + \beta_i - 1)!}{(\alpha_i - 1)! (\beta_i - 1)!}$$


L'expression $C_i = \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}$ est plus concise est facile à écrire. A voir si on peut le calculer directement avec des fonctions prédéfinis, sinon on connait donc la formule pour le déterminer à la main.

On peut donc donner une expression générale de $W_i$: 

$$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i} = \frac{k_i \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\sum_{i}^{n} k _i \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}} = \frac{\binom{n}{k} k_i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\binom{n}{k}  \sum_{i}^{n} k _i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}} = \frac{k_i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\sum_{i}^{n} k _i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}$$

En résumé :

$$W_i = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$

$$f_i^{(0)}(p|k, n) =  Beta(\alpha_i + k, \beta_i + (n - k))$$

$$f_{post}(p|k, n) = \sum_{i}^{n} W_i f_i^{(0)}(p|k, n)$$

Pour les propriétés associés aux modèles de mélange, tel que l'espérance et la variance, se référer à https://en.wikipedia.org/wiki/Mixture_distribution

Par exemple dans un modèle de mélange de la forme $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$

$E[x] = \sum_{i}^{n} k_i \mu_i$, avec $\mu_i$ l'espérance de la ième composante $f(x; \theta_i)$ On fait donc ici simplement la moyenne des moyennes ! Dans le cas d'une distribution $Beta(\alpha, \beta)$, l'espérance étant simplement $\frac{\alpha}{\alpha + \beta}$, l'espérance d'un modèle de mélange est très rapide à calculer.



On prend comme exemple un modèle de mélange initial : $f(p) = 0.5 f(p; 2,4) + 0.5 f(p; 4,2)$ qui constitue notre distribution à priori sur notre paramètre $p$

Pour notre distribution on a donc une espérance de $p$, $E[p]_{prior} = 0.5\frac{2}{2+4} + 0.5 \frac{4}{4 + 2} = 0.5$

Là dessus on réalise une observation, on observe  $k = 8$ succès sur $n = 10$ tentatives

On plot cette distribution du prior:

```{r}
p <- seq(0, 1, 0.01)
f <- 0.5 * dbeta(p, 2, 4) + 0.5 * dbeta(p, 4, 2)

plot(dbeta(p, 2, 4), type = 'l', col = 'blue')
lines(dbeta(p, 4, 2), col = 'green')
lines(f, col = 'red')
legend("topleft", legend=c("Beta(2,4)", "Beta(4,2)", "Mix"),
       col=c("blue", "green", "red"), lty = 1:2, cex=0.8)
```

On va donc calculer les distrution à posteriori :

$$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i} = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$

Donc 

$W_1  = \frac{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4)}{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4) + 0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}$

$W_2 \frac{0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4) + 0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}$

$f_1^{(0)}(p|k, n) =  Beta(2 + 8, 4 + (10 - 8))$

$f_2^{(0)}(p|k, n) =  Beta(4 + 8, 2 + (10 - 8))$


On calcule :

```{r, echo = TRUE}
# On commence par calculer C1 et C2 se sera plus pratique :

C.1 <- beta(8 + 2, 10 - 8 + 4)/beta(2,4)
C.2 <- beta(8 + 4, 10 - 8 + 2)/beta(4,2)

W.1 <- 0.5 * C.1/(0.5 * C.1 + 0.5 * C.2)
W.2 <- 0.5 * C.2/(0.5 * C.1 + 0.5 * C.2)

p <- seq(0, 1, 0.01)
f.1 <- dbeta(p, 2 + 8, 4 + (10 - 8))
f.2 <- dbeta(p, 4 + 8, 2 + (10 - 8))
```

On a W.1 = `r W.1` et W.2 = `r W.2`

On plot :
```{r}

f.post <- W.1 * f.1 + W.2 * f.2

plot(f.post, type = 'l', col = 'purple')
lines(f, col = 'red')

legend("topleft", legend=c("Mix.prior", "Mix.posterior"),
       col=c("red", "purple"), lty = 1:2, cex=0.8)
```




## Précisions 
Petites remarques sur le calcul des $W_i$: 
On sait que $$W_i = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$

Or, pour de grande valeur de $\alpha_i$ et $\beta_i$, ce qui est souvent le cas avec des composés, $B(\alpha_i, \beta_i)$ tend facilement vers 0. En effet la valeur devient tellement faible, que l'on ne peut pas la calculer précisément ...
Cela entraine une division par 0 et les calculs sont faussés. 
On propose ainsi de passer par le log pour pouvoir calculer complètement $W_i$. En effet, même si $C_i$ est donc difficile à calculer, on peut calculer $log(C_i)$ qui lui sera plus facilement calculable. Il suffit d'utiliser une fonction $logBeta(a,b)$. La fonction $Beta$ classique impliquant des factorielles et des puissances, en calculant le log, cela facilite grandement les calculs ! 
Aussi, on sait que $W_i$ sera entre 0 et 1 et sera une propotion donc il sera relativement facile de retrouver sa valeur, et c'est donc à cette étape là qu'il faut repasser à l'échelle classique avec l'exponetiellee

Ainsi on va dire que $W_i = exp(log(W_i))$ on cherche doc $log(W_i)$

$log(W_i) = log(\frac{k_i C_i}{\sum_{i}^{n} k _i C_i}) = log(k_i C_i) - log(\sum_{i}^{n} k _i C_i)$

Or $log(k_i C_i) = log(k_i) + log(C_i)$ avec $k_i$ étant une proba, il faut juste s'assure de ne pas considérer les éléments où $k_i = 0$, mais de toutes manière il n'influe pas la distribution, on peut les écarter sans soucis.

Ensuites: $log(C_i) = Log(B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)) = log(B(k + \alpha_i, n - k + \beta_i)) - log( B(\alpha_i, \beta_i)) = LogBeta(k + \alpha_i, n - k + \beta_i)) - LogBeta(\alpha_i, \beta_i))$ En python on a par exemple utiliser la fonction scipy.special.betaln pour calculer la fonction LogBeta.

L'utilisation du log nous permet de calculer des très faible valeurs ! C'est comme quand on utilise la LogVraisemblance au lieu de la vraisemblance classique car sinon le prosuit des proba sera trop faible et serait estimé à 0. En utilisant le log du produit des proba on arrive à l'estimer, nous c'est pareil !

Une fois que l'on a calculer tout les $log(k_i C_i)$ il faut calculer leur somme pour ensuite normaliser et obtenir $W_i$, on cherche donc $log(\sum_{i}^{n} k _i C_i)$ Attention: Le log d'une somme n'est pas la somme des Log !!
Heuresement il existe une technique pour calculer cela, en passant notamment par l'exponontielle à l'interieur de la somme :
$log(\sum_{i}^{n} k _i C_i) = log(\sum_{i}^{n} exp(log(k_iC_i)))$. On connsait en effet $log(k_i C_i)$, et en utilisant par exemple la fonction scipy.special.logsumexp on va pouvoir calculer le log de la somme: $log(\sum_{i}^{n} k _i C_i)$ :)
Ensuite on fait simplement $log(k_i C_i) - log(\sum_{i}^{n} k _i C_i)$

Ensuite on a juste à repasser à l'exponentielle ! Étant une proportion, il est beaucoup plus facile de revenir à ce moment là




Pour la CDF (Cumulative Distribution Function) c'est aussi très simple:

Si $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$ alors on cherche : $P(x \le z)$

$P(x \le z) = \int_{0}^{z} f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) dx = \int_{0}^{z} \sum_i k_{i}f(x; \theta_i) dx$

Or l'intégrale d'une somme étant la somme des intégrale et $k_i$ ne dépendant pas de $x$, on a :

$$P(x \le z) = \sum_i k_{i} \int_{0}^{z}f(x; \theta_i) dx$$

Ainsi, on pour obtenir la CDF d'une distribution de mélange , il suffit de combiner les CDF des composantes du modèles par rapport aux poids $k_i$, en gros: 

$$P(x \le z) = \sum_i k_{i} P_{i}(x \le z)$$


## Précisions (2)

Dans notre cas on cherche à déterminer $f_{post}(p|k, n)$ doit la distribution du paramètre $p$ en fonction de nos données observée. Mais en réalité ces données sont observées sur un composé particulier, pour lequel on va disposer d'un prior spécifique de ce composé. Donc, si on voulait être complètement rijgoureux, il faudrait également préciser que l'on conditionne par rapport à la specie considéré $S_i$, on se place dans la condition particulière ou l'on étudie $S_i$. Heureusement Grâce au théroême des probabilité conditionnelle, ça ne change rien à la formule ! En effet, si on note $Q_i(p|k,n)$ la distribution de probabilité de $p$ par rapport à nos observations $k,n$ et conditionné par rapoort au fait que l'on observe la specie $S_i$, on a que :


Pour simplifier l'écriture on dégage le $n$. En fait, même dans les formule du haut, je suis pas sur que pour faire propre il faille écrire le $n$ dans la formule... Car même si c'est un paramètre de la loi Binomiale, il est fixé et n'a pas de 'proba' derrière ... Car au final le vrai paramètre de la loi Binomiale c'est surtout $p$ Enfin bon c'est du détail ^^

$Q_i(p|k) = \frac{Q_i(k|p)Q_i(p)}{Q_i(k)} = \frac{p(k|p \cap S_i) p(p| S_i)}{p(k|S_i)} = \frac{p(k|p \cap S_i) \frac{p(p \cap S_i)}{p(S_i)}}{p(k|S_i)} = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(k|S_i)}$

$p(k|S_i)$ est bien sur donc égal à $p(k|S_i) = \int p(k|p \cap S_i) p(p| S_i) dp$, masi on garde la forme comptacte pour simplifier !

Donc $Q_i(p|k) = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(k|S_i)} = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(p \cap S_i)/p(S_i)} = \frac{p(k \cap p \cap S_i)}{p(p \cap S_i)} = p(p | k, S_i)$

Et donc on retrouve bien que l'on cherche la distibution de $p$ par rapport aux comptages observée sachant qe l'on étudie $S_i$. Tout cela est garanti par le théorême des probabilités conditionnelles.




## Donc en résumé notre approche :

1- Déterminer les probas de transitions: On cherche à déterminer la proportion de marcheurs partant de l’ensemble des autres métabolites z, qui arrivent sur notre métabolites cible x. Pour la calculer, on chercher premièrement à déterminer pour chaque métabolites son PPR, sans considérer le métabolites en lui même. 
Pour cela, on travaille, sur un réseau modifié, dans lequel le métabolite cible a été retiré. Le vecteur de restart de notre PPR va alors être composé des voisins directs de notre mlétabolites cible. De cette manière, a chaque restart, on va recommencer à partir de l'un des voisins de notre noeud cible, ce qui est comme si on était reparti de notre noeud cible et que l'on avait directement emprunté une transition vers l'un de ces voisins. L'important est que l'on ai une probabilié à 0 pour notre noeud cible car pour la suite on ne souhaite pas qu'il est d'influence sur notre prior.

Donc on pose :

$$M = \alpha P + (\alpha* a + (1 - \alpha) e)v^\intercal$$
Tout les vecteurs sont des vecteurs colonnes.
- $P$ est notre matrice de probabilité obtenue en ayant préalablement supprimé la colonnes et la ligne correspond à l'index de notre noeud cible
- $\alpha$ est le damping factor: la probabilité de transité vers un voisin. $(1 - \alpha)$ est la probabilité de restart
- $a$ est un vecteur colonne égal à 1 pour les noeuds puits (sink nodes)
- $e$ est un vecteur colonne de $1$
- $v$ est le vecteur colonne contenant les probabilité de restart conduisant aux voisins du noeud cible.


En utilisant la méthode de la puissance itérée (pas besoin de sortir des algo shiny nos graph sont suffisament petit pour que la power method s'applique)

Le vecteur de probabilité obtenu s'interprête comme étant la distribution stationnaire des probabilités liée à notre marche aléatoire. On peut interpréter ce vecteur comme la proportion du temps passé sur chaque composé $i$ en réalisant une marche partant (et recommençant avec un facteur $(1-\alpha)$) des voisins de notre noeud cible. Si on imagine un cas simpliste avec un vecteur final $(0,0,0,0.5,0.3,0.2)$, on peut également imaginer que si 100 marcheurs partaient des voisins de mon noeud cible, au cours de la marche aléatoire on verrait en moyenne 50 marcheuurs sur le noeud 4, 30 sur le noeud 5 et 20 sur le noeud 6.

Ainsi pour chaque noeud ce vecteur me donne les probabilitées d'une marche aléatoire partant des voisins de ce noeud. De cette manière le noeud en lui même n'est jamais visité car il est préalablement supprimer du réseau et ainsi on ne passe jamais par le noeud cible au cours de notre marche aléatoire. On simule en quelque sorte que l'on transite directement de celui-ci vers un de ces voisins en utilisant le vecteurt de restart correspondant à ces voisins.

On va donc utiliser ce vecteur pour calculer le vecteur des probabilités, des proportions, des marcheurs qui se trouve sur notre noeud cible au cours d'une marche aléatoire partant des autres noeuds du réseau. Dans le cas précédent, le noeud cible était l'émetteur de l'information et les autres noeuds les récepteurs: les marcheurs partaient de notre noeuds et se diffuser dans le réseau. Maintenant, on considère que tous les autres noeuds envoient des marcheurs dans le réseau, et on veut chercher à déterminer, parmis tous ceux qui arrivent sur notre noeud cible, qu'elle proportion est originaire du noeud A, du noeud B, etc .... Ici, ce sont les autres noeuds qui émettent l'information et le noeud cible qui capte l'info des autres noeuds ! C'est beaucoup plus dans la philosophie que l'on recherche, où on souhaite construire un prior à partir de l'information des voisins du noeuds. On veut donc que ce soit les autres noeuds qui "diffuse" leur info.

Pour constuire ce nouveau vecteur de probabilité, on va simplement pondéré la probabilité, le temps passé, par l'ensemble des autres noeuds sur le noeud cible $x$. En convertissant mes proba et nombre de marcheurs, si je multiplie tout les vecteurs obtenus précédement par 100 par exemple: j'obtiens pour chacun, le nombre attendu de marcheurs qui, partant des voisins de leur noeud de départ, se trouve sur le noeud $x$. Par exemple, je pourrais observer que sur ce noeud $x$, on a 40 marcheurs qui proviennent du noeud A, 20 marcheurs du neoud B, 30 de C et 10 de D. Ainsi pour ce noeud je pourrais construire le vecteur $(0.4,0.2,0.3,0.1)$. Ainsi lorsqu'un marcheur se trouve sur mon noeud, il a 40% de chance d'être originaire du noeud A, 20% de chance d'être originaire du noeud B etc ...
Quand on dispose de notre matrice des vecteur de PPR en colonne, il s'agit simplement de calculer les proportion des valeurs sur chaque ligne !
C'est exactemement ce que l'on veut représenter pour, l'information a priori: on veut que l'information apporté par le noeud X au prior, soit proportionnelle à la proportion de l'information arrivé sur le noeud cible qui est originaire de X. 
Comme précédement, sachant que mes proba sont à 0 pour le noeud en lui-même, dans le calcul de ce nouveau vecteur il ne compte toujours pas ! :)

Cf. figure propagation

Globalement les deux approches ont des résultats proches (SFT ou FOT) mais il semble que :
SFT soit plus sensible à ces voisins “hub” c a d ces voisins qui ont une forte connectivité (le marcheur partant du noeud cible passe effectivement plus souvent par eux)
FOT est plus sensible à ces voisins directe et pénalise les Hubs (les marcheurs partant des voisins directs passe souvent par le noeud cible et ceux partant des noeuds ont tendance à se perdre

L'idée étant d'utiliser ce vecteur FOT de probabilité comme vecteur de pondération dans notre mélange de Beta.


Notre modèle de mélange se décompose en plusieurs étapes: 

On étudie une association entre une specie $s$ et un MeSH $m$

**L'hypothèse de base: On suppose que tous les composés discuttent de tous les MeSH ! On fait ainsi l'hypothèse que toutes probabilité ** $p(m|s) > 0$**, c'est à dire pour tout composé, la probabilité qu'un de ces articles mentionnent n'importe quel MeSH est strictement postive. Récipriquement, on dit que la probabilité qu'un article parlant d'un composé, ne metionne JAMAIS un terme MeSH quelconque est nulle. Néanmoins ces probabilités peuvent être extrêment faibles pour certains MeSH, on en discutte jamais et très grande pour d'autres: dès que je parle de mon composé j'ai de grande chance de parler également de mon mesh. On suppose ainsi que toutes les fois où l'on observe une coocurence à 0 entre un composé et un MeSH, c'est simplement car on a pas assez d'échantillons d'articles pour représente notre composé. On imagine que si je piochais 1 millions de nouveaux articles discuttant de mon composé, je verrai enfin une coocurence avec mon MeSH. Elle sera très très rare et non significative, mais sa probabilité sera > 0**

Étape 1: Poser une distribution à priori pour les MeSH :

Pour chaque MeSH, on cherche à poser une distribution à priori sur la probabilité $p(m|s)$. On peut par exemple estimé cette distribution en utilisant les probabiltié observées $p(m|s)$ pour toutes nos species, en ne considérant que celles > 0 (pas les cooc nulles), du fait de notre hypothèse. Le problème c'est que cette distribution serait facile à estimer pour des MeSH à fort corpus qui sont annotés dans beaucoup de species, mais pour des MeSh rares, annotés sur 1 ou 2 composés il va être très difficile de poser une distribution ...
On pourrait imaginer poser un prior non-informatif, mais ce serait sur-estimé les probabilités pour les MeSH à faible corpus ...

Néanmoins, on peut observer une hypothèse assez simple: Plus un MeSH possède un fort corpus, plus les probabilités $p(m|s)$ qui lui sont associées auront tendance a être élevé. En effet, si mon MeSH est annotés sur 100000 articles il très peu probable que ces articles soient distribués de manièrement individuelle sur 100000 composés, mais plutôt que certains composés auront de très forte probabilité de les observer et aussi que de manière générale les composés auront plus de chance de discutter de ce MeSH que d'un autre MeSh plus rare. Pour visualiser cela on peut par exemple regarder l'évolution des proba $p(m|s)$ par rapport à la taille du corpus des MeSH $m$: 

![Évolution des proba p(m|s) par rapport à la taille de corpus des MeSH m](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/Fig/plot_proba_vs_corpus_mesh.png)

Comme on peut le voir il semble y avoir une dépendance linéaire par rapport au log de la taille du corpus. Cette hypothèse est assez vraisemblable, plus mon MeSH est annoté dans un grand nombre d'article, plus la probabilité d'en discutter quand je parle d'un composé est élevé.

Le fait d'utiliser une regression linéaire va nous permettre d'utiliser nos MeSH et nos species bien annotés pour construire le modèle et ensuite on pourra prédire la distribution des probabilités pour des MeSH à très faible corpus en utilisant comme estimateur leur taille de corpus et en se basant sur la linéarité du modèle.

Pour poser le modèle on a besoin de données fiables. On ne va utiliser les species avec trop peu de literature associés car les probabilités que l'on estime à partir de ces species sont biaisé. Par exemple si une specie n'a que 2 article annotés, n'importe quel MeSH aura un proba de 0, 0.5 ou 1, ce qui biase complètement notre distribution. Ainsi, **on ne va considérer que les species avec plus de 500 articles annotés.**

Pour les MeSH, c'est un peu le même problème finalement, si on considère des MeSh avec des corpus trop faibles, il sont annotés pour trop peu de species. Or il faut suffisament de species pour pouvoir estimer une distribution. On a donc regardé la distribution du nombre de species annotés par rapport à la taille des corpus :


![Log MeSH corpus by nb species](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/MeSh_corpus_by_nb_species.png)


Comme on peut le voir sur cette figure, il y a effectivement une dépendance entre la taille du corpuzs MeSH et le nombre de species annotées. Si on veut considérer les MeSH avec en moyenne au moins 100 species annotés, on va devoir filtrer nos MeSH en ne sélectionnant que ceux avec $> 1000$ articles annotés (exp(7.07))

Donc nos filtre sur les données sont :

- corpus species > 500 
- corpus MeSH > 1000

Voici un violin plot des data utilisées:

![residuals by fitted values sigma](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/violin_plot.png)


Après on a fait d'autres test en faisant notamment varier le filtre sur le corpus MeSH, les résultats ne varie pas beaucoup ...

On utilise donc un modèle de regression beta-binomiale.
Dans ce modèle, on dipose du nombre de succès pour chaque individus obtenus après un certain nombre de tentatives. Ce nombre de succès suit une distribution binomiale.
Dans notre cas on note comme un succès l'évènement où un article discutant du composé cible, discute également du MeSH. On a donc que :

$$y_i \sim Bin(n, p_i)$$

- $y_i$ étant le nombre d'article associé au composé qui discute également du MeSH $i$, c'est **la co-occcurence**
- $n$ c'est le nombre total d'article associé au composé étudié
- $p_i$ c'est la probabilité de succès, c'est à dire la probabilité qu'un article discuttant de la specie étudié, discute également du MeSH $i$. On peut également noté $p_i$, $p(M_i|S)$, la probabiltié d'observer le MeSH $i$, sachant que l'on étudie la specie $S$.

l'inconvénient avec $y_i$ c'est qu'il s'agit d'un comptage, d'un nombre entier (par ex: 3 succès ou 3 de cooc dans mon cas), c'est une quantité particulière, et on ne peut donc pas modéliser directement ce $y_i$ en utilisant une regression, puisque dans notre regression on va une droite ce qui implique une infinité de valeurs prédites, même des valeurs négatives, et pas uniquement des entiers, des nombre de succès. Notre variable étant particulière, ce n'est pas $y$ que l'on va chercher à prédire directement. Or, en supposant que $y$ est distribué selon une distribution binomiale, on sait que l'observation $y_i$ est plus ou moins vraisemblable en fonction du paramètre $p_i$. Par exemple $y_i=50$ quand $n=20$ est très vraisemblable pour $p_i=0.5$ et peu pour $p_i = 0.9$ Deplus, ce paramètre $p_i$ est continue, il n'est pas discret comme l'observation $y_i$, il peut prendre une infinité de valeurs entre [0,1] (Cf. fonction de lien).
Ainsi, dans notre modèle, ce n'est pas $y_i$ que l'on va directement chercher à prédire, mais $p_i$ en supposant que $y_i \sim Bin(n, p_i)$. On va notamment se baser sur la vraisemblance des données pour prédire $p_i$

**Ainsi, contrairement à une regression classique, la variable que l'on prédit avec notre modèle, n'est pas directement la variable que l'on observe, mais un paramètre de la distribution dont est issue notre variable observée.** Néanmoins, on pourra donc estimer, comme dans une regression classique, la moyenne attendue de notre variable observée connaissant le paramètre prédit de notre distribution, c'est notamment ce que l'on fera pour les résidus

Dans un modèle beta-binomial, on suppose que notre paramètre $p_i$ est également une variable aléatoire qui suit une distribution $Beta(\alpha_i, \beta_i)$. Cet ajout au modèle permet notamment de représenter l'incertitude sur la probabilité $p_i$ lorsque le nombre de tentative n'est pas très grand.



$$y_i \sim Bin(n, p_i)$$
$$p_i \sim Beta(\alpha_i, \beta_i)$$

On modélise donc les coocurences tel que

$Y \sim BetaBin(n, \alpha_i, \beta_i)$

$p(Y_i = y_i) =  \binom{n}{y_i}  \frac{1}{B(\alpha_i, \beta_i)} \times  B(\alpha_i + y_i, \beta_i + (n - y_i))$

$\iff  p(Y_i = y_i) = \binom{n}{y_i}  \frac{B(\alpha_i + y_i, \beta_i + (n - y_i))}{B(\alpha_i, \beta_i)}$


On modélise ainsi les comptages de succès $Y_i$ par rapport à $\alpha_i$ et $\beta_i$. Grâce à $\alpha_i$ et $\beta_i$, on va pouvoir estimé la probabilité attendu $\hat{p_i}$, à partir de laquelle on pourra alors déterminer le nombre de succès attendu $E[Y]$ avec $n \hat{p_i}$

**Attention:** On ne pourrait **pas** utiliser les probabilité calculées pour essayer de poser une distribution Beta dessus. En effet :
- Nos observations sont discrêtes ce sont des comptages
- La distribution Beta est une variables continue, OR, créer des probabilité à partir de nos observations (discrètes) ne rend pas notre variables continue pour autant !! En effet, j'aurait toujours un certain set déterminé de valeurs possible que peuvent prendre mes probabilités, elle ne sont pas continues, ce sera toujours discret. Je pourrais par exemple calculer la proba que P(p = 1/2) si j'ai observer x fois le nombre de cooc égal à la moitié de mon corpus. Or ceci est impossible pour une variable continue. 

Ainsi, il faut  


Lorsque l'on pose notre modèle, on va effectué une reparamétrisation de la distribution beta, en terme de moyenne $\mu$ et de sur-dispersion (la variabilité derrière $p$) $\sigma$. On pose alors $\mu$, la probabilité moyenne de succès, $\mu = \frac{\alpha}{\alpha + \beta}$ et $\sigma = \frac{1}{\alpha + \beta}$. On peut donc re-paramétrer notre $Beta$ pour un MeSH $i$ tel que $p_i \sim Beta(\frac{\mu_i}{\sigma_i}, \frac{(1 - \mu_i)}{\sigma_i})$. Même si cela semble compliqué les choses, en fait lorsque l'on veut poser des prior ou autre c'est plus facile de réfléchir en terme de probabilité moyenne et dispersion qu'en terme de $\alpha$ et de $\beta$. Donc par défaut on transforme nos paramètres avant de les mettre dans le modèle. Pour avoir une explication (une référence pour cette reparamétrisation), il suffit de regarder le document The gamlss.family distributions. Dans la formulation de la densité de probabilité de leur BB, ils définissent en fonction des deux paramètres $\mu$ et $\sigma$. En comparant avec la formule classique de la BB, on peut facilement re-identifier les termes et voir que $\frac{1}{\sigma}$ correspond à $\alpha + \beta$, donc effectivement $\sigma = \frac{1}{\alpha + \beta}$ et on voit aussi que $\frac{\mu}{\sigma}$ correspond au terme $\alpha$ dans l'équation classique, donc que $\mu(\alpha + \beta) = \alpha$, alors on retrouve bien que $\mu = \frac{\alpha}{\alpha + \beta}$ 

Ainsi après reparamétrisation : $p_i \sim Beta(\frac{\mu_i}{\sigma_i}, \frac{(1 - \mu_i)}{\sigma_i})$

Dès lors, $p_i$ étant une variable aléatoire, ce n'est plus $p_i$ que l'on cherche à prédire avec notre modèle mais $\mu_i$ et $\sigma_i$ qui définisse la distribution de $p_i$



Donc comme on l'a vue précédemment on voit qu'il semble que notre probabilié $p_i$ soit linéairement reliée à la taille du corpus du MeSH étudié, que l'on va noter $M_i$. Aussi, même si ça ne se voit pas très bien sur la Figure la variabilité semble également être linéairement relié à la taille du corpus MeSH. Vu que c'est en échelle log, là où la variabilité du nuage de points nous semble constante en réalité elle augmente !

Ainsi, on souhaite modéliser les **2** paramètres de notre distribution en fonction de la variable explicative. Pour cela, on va devoir utilisé des méthodes de style VGAM ou GAMLSS. Dans les modèle de glm 'classique', on ne peut pas estimer $\sigma$ en fonction d'une variable, seul la moyenne $\mu$ peut être fonction des variables. Les modèles de types GAMLSS ou VGAM permettent de relaxer cette restriction. C'est pour cela que l'on va utiliser GAMLSS.

(https://stats.stackexchange.com/questions/312434/how-can-gamlss-relax-the-glm-exponential-family-assumption)

Si on bin, on le voit bien :  

```{r}
d <- data.frame(bin = c("[0,100)", "[100,500)", "[500,1e+03)", "[1e+03,5e+03)", "[5e+03,1e+04)", "[1e+04,5e+04)", "[5e+04,1e+05)", "[1e+05,5e+06]"), var = c(0.00000218, 0.0000142 ,0.00000864, 0.0000291, 0.0000517, 0.000201, 0.000476, 0.00255))
DT::datatable(d)
```

On va donc chercher à expliquer notre moyenne de succès $\mu_i$ et notre sur-dispersion (variabilité $\sigma_i$) en fonction de la taille des corpus MeSH $M_i$ Dans notre modèle on pose donc que :

$\mu_i = \beta^{(\mu)}_0 + \beta^{(\mu)}_1 \times log(M_i)$

$\sigma_i = \beta^{(\sigma)}_0 + \beta^{(\sigma)}_1 \times log(M_i)$


Or $\mu_i$ devant être compris entre [0,1] et sigma devant être supérieur à 0, pour que notre modèle prédise des valeurs adéquates, on va utiliser la fonction de lien logit pour $\mu$ (noté $g_1()$) et simplement log pour $\sigma$ (noté $g_2()$)

Ainsi on a: 

$g_1(\mu_i) = log(\frac{\mu_i}{(1 - \mu_i)}) = \beta^{(\mu)}_0 + \beta^{(\mu)}_1 \times log(M_i)$

$g_2(\sigma_i) = log(\sigma_i) =\beta^{(\sigma)}_0 + \beta^{(\sigma)}_1 \times log(M_i)$

Aussi par exemple, $\beta^{(\mu)}_0$ représente l'intercept associé à la regression de $\mu_i$ et $\beta^{(\mu)}_1$ le facteur associé au log de corpus MeSH $log(M_i)$

En résumé on a donc:

$Y \sim BetaBin(n, \frac{\mu_i}{\sigma_i}, \frac{(1 - \mu_i)}{\sigma_i})$ avec 

$g_1(\mu_i) = \beta^{(\mu)}_0 + \beta^{(\mu)}_1 \times log(M_i)$

$g_2(\sigma_i) = \beta^{(\sigma)}_0 + \beta^{(\sigma)}_1 \times log(M_i)$

Le but du modèle est donc d'estimer les paramètres $\beta^{(\mu)}_0, \beta^{(\mu)}_1,  \beta^{(\sigma)}_0, \beta^{(\sigma)}_1$ tel qu'ils prédisent les $\mu_i$ et $\sigma_i$ qui maximise la vraisemblance des données observées !!

En gros pour ce faire on remplace $\mu_i$ et $\sigma_i$ dans l'équation de vraisemblance de la beta-binomiale par leur expression dans le modèle (en inversant la fonction de lien, par ex on inverse le logit pour retrouver juste $\mu_i$ et pas $g_1(\mu_i)$) Ensuite quand on a l'expression de la vraisemblance en fonction des coefficient $\beta^{\mu,\sigma}_{0,1}$ on cherche à trouver les coefficients qui maximise la vraisemblance !!

Avec nos données actuellement nous avons posé le modèle et voici les résultats obtenus :

Mu link function:  logit

Mu Coefficients:
                      
Estimate Std.Error t-value Pr(>|t|)   

(Intercept)          -14.263497   0.012734 -1120.1   <2e-16 ***

log(TOTAL_PMID_MESH)   0.836309   0.001319   634.2   <2e-16 ***

------------------------------------------------------------------

Sigma link function:  log

Sigma Coefficients:

Estimate Std.Error t-value Pr(>|t|)    

(Intercept)          -13.125658   0.018286  -717.8   <2e-16 ***

log(TOTAL_PMID_MESH)   0.759816   0.001906   398.6   <2e-16 ***


Ce modèle a donc été poser sur des données filtrées (corpus specie > 500 et corpus MeSH > 1000)
On a donc également caclulé la vraisemblance sur les données totales, on a obtenue: -1911716. A savoir que la vraisemblance d'un modèle en utilisant toutes les données (donc la vraisemblance max) est de -1893467, donc c'est relativement très proche ! Cependant on sait que toutes les données ne sont pas bonnes a utiliser pour poser le modèle.

Nous avons également étudié les résidus :

Alors attention dans une regression beta-binomiale, même si ce sont $\mu_i$ et $\sigma_i$ qui sont prédits par le modèle, les résidus sont calculés par rapport à la valeur $Y_i$ observée, notre co-occurence, la beta-binomiale se définissant par rapport à $Y_i$. 

$Y \sim BetaBin(n, \frac{\mu_i}{\sigma_i}, \frac{(1 - \mu_i)}{\sigma_i})$ 

De manièrement générale, on estime les résidus en comparant la valeur $Y_i$ observée par rapport à la valeur prédite par le modèle, qui est $E[Y|n,\alpha_i, \beta_i]$ (soit $\frac{n \alpha_i}{\alpha_i + \beta_i}$ ou $n \mu_i$) et l'on standardise par rapport à l'écart-type de la variabilité attendu $V[Y|n,\alpha_i, \beta_i]$.

- Résiduals by fitted values :

Par rapport à mu-prédit :
![residuals by fitted values mu](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_fitted_values_mu.png)

sans les signifs: 

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_fitted_values_mu_no_signif.png)

Pour sigma :

![residuals by fitted values sigma](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_fitted_values_sigma.png)


sans les signifs: 
![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_fitted_values_sigma_no_signif.png)

On peut constater que :

-  Les résidus les plus elevés sont associés à des relations significatives entre des species et des MeSH. En effet, nous modélisons la distibution globale des co-occurences associés aux MeSH ($P(Y = y_i)$) avec notre beta-binomiale, néanmoins la plupart du temps l'association entre le composé et le MeSH n'est pas significative et relativement du au hasard avec par exemple 1 ou 2 coocurences. Les co-occurences sont donc généralement faible, sauf pour les associations significatives, qui sont en quelques sortes des out-layers de nos distributions modélisé ! C'est d'ailleurs pour ça qu'ells sont considéré comme significative, car leur co-occurences est largement supérieure à la co-occurence moyenne observée chez les autres composés.
Ainsi on voit que les associations que l'on modélise "mal" avec notre modèle sont les associations significatives, mais c'est exactement ce que l'on souhaite, puisqu'on veut modéliser la distribution générale ! Donc sur nos distribution modélisation les coocurences associés à ces associations significatives.

On a aussi chercher à ploter les résidus par rapport à notre variable explicative: 

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_by_corpus_MeSH.png)

Sans les signifs: 

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_by_corpus_MeSH_no_signif.png)

Comme on peut le constater (on le voit aussi sur les graph précédents), il semble que nos résidus soiçent plus élevé pour les valeurs faibles de $\mu$, de $\sigma$ et même de la taille du corpus. On sait que ces éléments sont liés dans notre modèle: plus on a une taille de corpus faible, plus on attend pour ce MeSH une proba moyenne $\mu$ faible et une variabilité $\sigma$ faible. On attend donc également des coocurences moyenne faibles.

En fait je pense que pour les MeSH à faible corpus, la coocurence moyenne prédite par le modèle $E[Y|n,\alpha_i, \beta_i]$ est relativement faible, voir proche de 0. Or, la valeur observée pour ces éléments est au minimum de 1 dans les données, car on a au moins observée 1 article. Ainsi, plus les taille de corpus MeSH sont faible, plus on va prédire une moyenne $\mu$ faible, plus le nombre de succès attendu $E[Y|n,\alpha_i, \beta_i]$ sera faible, mais sera toujours comparer au minimum à 1, alors que celui-ci peut tendre vers 0 ! Je pense que c'est pour cela que l'on voit que les résidus sont fort vers les faibles valeurs de $\mu$ et diminue un peu ensuite à mesure que $\mu$ ou la taille de corpus MeSH augmente. A partir de là, les valeurs prédites seront plus élevées et seront surtout comparer à des valeurs 'du même ordre'.


On peut également regarder les résidus par index :

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/residuals_by_index.png)

pareil, on observe rien de particulier.


Pour la qualitée de fit du modèle :

On peut voir avec les histogrammes ou accentuer les différences en regardant à l'échelle log:
Pour les données observée, on a réalisé une représentation des distribution de proba par bins :

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/data_disribution_by_bin.png)

Pour les données prédites, on a réalisé un échantillonage en fonction de la moyenne des taille de MeSH par bin et on a refait cette distribution :

![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/predictions_disribution_by_bin.png)


On peut comparer les deux: 


![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/comparison_disribution_by_bin.png)


Plusieurs choses à noter :
- Pour toutes nos distribution prédites on a $\alpha$ < 1 ce qui signifie que notre distribution n'a pas une courbe en cloche, possède une densité infinue en 0.
En fait, il se trouve que c'est la distribution qui fit le mieux nos données. Ce sont notamment les données significative, généralement des fortes probabilité, qui forment la longue tail de la distribution des porba observées tendant vers 1. On a une distribution qui suivrait presque une loi de puissance en quelque sorte. Pour notre fit beta, la distribution qui correspond le mieux a ces données là est une distribution avec $\alpha$ < 1. Exemple de la tail :
![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/Specie500_MeSH_1000/dist_full_data/Dist_MeSH_450000_550000.png)

La distribution qui fit le mieux cette distribution semble devoir adopté une densité infinie en 0. On peut atteindre une courbe en cloche si jamais on supprime toutes ces données significatives mais ce ne sont **pas** des fausses données, ces observations font vraiment partie de notre distribution il faut donc les conservées.

Voilà pourquoi sur nos distribution on observe une tail vers de faibles valeurs de $p$, c'est car la densité tend vers l'infini à mesure que $p$ se rapproce de 0. Cet effet est fort pour de faible valeur de taille de corpus comme on peut le constater, et assez faible, tend même à une courbe en cloche pour les fortes valeurs de taille de corpus. Le fait que cet effet soit d'autant plus important avec la taille de corpus ne contredit pas notre hypothèse, où on s'attend effectivement que les vrais probabilités soient plus faible que celle que l'on a observéees.

Deplus nos distributions semblent légèrement décalées vers la droites par rapport aux distribution observées. Ceci en aussi du aux valeurs de probabilité très fortes qui vont avoir un fort *leverage*, c'est à dire qu'elle vont faire levier pour tirer la courbe vers elle et ainsi rapprocher leur fitted-value de leur vrai valeur pour ces points. Ce sont ainsi des points qui ont une forte influence potentielle sur la slope de la courbe, et qui vont avoir tendance à tirer la distribution vers le haut. Néanmoins comme précedement, ce sont de vrais observations donc il faut les prendre en compte. C'est donc ces points qui font que notre distribution tend vers la droite par rapport à la distribution observée, ils tirent la courbe vers eux. Cependant, même si nos distribution tendent plus vers des valeurs élevées, du à notre tail, la moyenne est cependant toujours plus faible dans nos prédictions (Cf. ggridges) 
Mais globalement pour notre prior c'est pas trop mal on arrive tout de même à représenter la tendance qu'il y a sur les probabiltiés. 

En utilisant le package VGAM, on a calculé les hat-values correspondantes pour visualiser le leverage :

Exemple pour mu
![residuals by fitted values mu no-signif](/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/Fig/mu_hat_values.png)


Comme on peut le voir, les valeur de hat values pour $\mu$ les plus élevé se situe pour les point aux extrême de la taille de corpus avec les proba les plus hautes. Même si on supprimait ces points pour le fit, les autres points dans cette direction prennent le relais et on a toujours cet effet de leverage. Si on supprime carrément tout les point au dela de 100.000 de taille de corpus par exemple c'est sur ces éléments que l'on aura un décalage à la fin. Après cela peut être une solution mais c'est difficile de justifier que l'on sort ces données pour le fit ...

**A tester lorsque l'on aura une méthode de validation pour la precision/recall des données -> tester le modèle qui permet d'obtenir les meilleurs résultats 

J'ai aussi fait des graphs pour représenter les distributions observées et prédites à différentes taille de corpus. (Cf dist_dull_data)


Donc une fois que l'on a le modèle, on est capable de prédire pour n'importe quel MESH, connaissant la taille de son corpus, une distribution à priori des probabilités $p(m|s)$. Cette distribution indiquera une probabilité moyenne d'autant plus faible que le MeSH aura un faible corpus (MeSH rare).

Pour tout MeSH $i$ que l'on étudie, on va donc prédire les valeurs de $mu_i$ et de $\sigma_i$ et en déduire les valeur de $\alpha_i$ et $\beta_i$

Cette distribution représente notre première distribution à priori.
Ainsi, pour tout les composés du réseau, à l'exception de notre composé cible, on va commencer par estimer la distribution à posteriori associé au MeSH étudié. Pour chaque composé, la distribution à posteriori est simplement donné par la beta:  $Beta(\alpha_i + COOC_i, \beta_i + (N - COOC_i))$

- $COOC_i$: la co-occurence entre le composé et le MeSh $i$
- $N$: la taille de corpus associé au composé

L'avantage d'utiliser le prior de notre modèle et que :

- Si le composé n'a pas beaucoup de literature associé, sa distibution suivra beaucoup la distribution à priori obtenu sur l'ensemble de composés

- Si le composé a beaucoup de literature, la distribution à priori n'aura pas d'influence sur la distribution à posteriori qui suivra les observations faites pour ce composé.

Ainsi, si mon composé cible est malheureusement entouré de composé sans trop de littérature, la distribution a priori que l'on utilisera pour celui-ci sera un mélange de distribution proche de mon prior d'origine, et donc c'est ~ comme si j'avais directement utilisé mon prior du modèle sur mon composé cible. En rechanche si mon composé cible est entouré de composé à fort corpus, sont prior sera un mélange de ces distribution très 'précises'

Donc pour construire la distribution à priori de mon composé cible, on va simplement construire une distribution de mélange en utilisant :

- Les probabilité obtenus à partir de mes marches aléatoires FOT comme facteur $k_i$
- Les distribution à posteriori obtenus avec les observation et le prior du modèle sur mes voisins


Ensuite on met simplement à jour cette distribution de mélange à priori, avec les observations sur mon composé cible !


L'idée clef est simplement que les distributions à posteriori obtenus pour mes voisins, en se basant sur un prior issus du glm et de leurs comptages, vont être utilisés pour construire un modèle de mélange qui servira de distribution à priori pour mon composé cible !

j'obtiens alors pour mon composé cible, une distribution de mélange à posteriori.
On peut donc déterminer la probabilité moyenne $p(m|s)$ pour notre composé cible et chercher à la comparer avec la probabilité d'observer le MeSH de manière générale $p(m)$

Ma probabilité moyenne $p(m|s)$ peut être considérer comme mon estimateur Bayesien de la moyenne ;)
Pour estimer à quel point notre probabilité moyenne est supérieure à $p(m)$ on peut regarder la CDF par rapport à cette proba.


## Nouvel estimateurs du prior :
Ref: John_K_Kruschke-Doing_Bayesian_Data_Analysis-EN.pdf pages.129-130 

Nous devons estimé les distributions à priori qu'un article discutte du MeSH pour nos composés,k mais la regression ne semble pas être la solution la plus adapté, car le modèle semble difficilement se fitter.

Cependant, on peut imaginer poser un à prior théorique en formulant une hypothèse génératrice :
- L'hypothèse la plus simple que l'on puisse formuler sur notre prior est que l'on estime, à priori, qu'il n'y a pas de liens particulier (de relation directe) entre notre composé notre MeSH. A priori on peut largement considérer que ces éléments sont indépendants et que si il y a une relation, ce sont nos données qui devront le montrer. L'apriori indiquerait donc une indépendances entre nos évènemenet "l'article parle du composé X" et "l'article parle du MeSH Y", puiqu'a priori on a aucun indice sur leur relation.

Sachant cela, on s'attend donc à priori, que la probababilité $p(m|s) = p(m)$ sous l'hypothèse d'indépendance, donc on peut estimer que notre distribution à priori devrait être centré autour de la probabilité générale d'observer le MeSH, soit $p(m)$

Intuitivement, on pourrait donc utiliser la moyenne par exemple, pour fixer le paramètre $\mu$ de notre distribution et ainsi posé notre prior. Cependant, les probabilités $p(m)$ d'observer les MeSH sont généralement assez faible (98\% < 0.01 par exemple) 
Cependant, la valeur moyenne $\mu$ d'une distribution, affecte ce que l'on appelle la *skewness* de la distribution. La *skewness* représente le degré d'asymétrie de la distribution. Pour une *skewness* nulle, la distribution est symétrique, pour des positive de *skewness*, elle présente une *right tail* et pour des valeurs négatives une *left tail*

Dans le cas de la distribution beta, dans le cas non-symmétrique lorsque $\alpha \neq \beta$ (ce qui est notre cas), on a que :
$\lim_{\mu \to 0} \gamma = \infty$

Dans le cas particulier où $\alpha = \beta$, la moyenne, la mediane et le mode sont égaux

Ainsi, à mesure que la moyenne $\mu$ tend vers 0, notre distribution de vient de plus en plus asymétrique, ce qui est a un effet important sur  la relation entre la moyenne et le mode de la distribution. Lorsque la distribution est symétrique, la moyenne est égale au mode de la distribution, soit la valeur la plus probable. Dans le cas d'une distribution asymétrique la moyenne s'éloigne du mode et se décale vers la longue tail, c'est l'asymétrie. Quelques exemples :

```{r}
mu.1 <- 0.5
mu.2 <- 0.05
mu.3 <- 0.95
o <- 100
p <- seq(0, 1, 0.0001)
for(mu in c(mu.1, mu.2, mu.3)){
  alpha <- mu * o
  beta <- (1 - mu) * o
  mode <- (alpha - 1)/(alpha + beta - 2)
  g <- ggplot(data.frame(proba = p, density = dbeta(p, alpha, beta)), aes(x = proba, y = density)) + 
    geom_line() +
    ylim(c(0,20)) +
    theme_classic() + 
    stat_function(fun = dbeta,
                args = list(shape1 = alpha, shape2 = beta),
                geom = "area",
                fill = "#F8766D",
                xlim = c(mu, 1)) +
    stat_function(fun = dbeta,
            args = list(shape1 = alpha, shape2 = beta),
            geom = "area",
            fill = "#619CFF",
            xlim = c(0, mu)) +
    geom_vline(xintercept = mu, colour = "black") +
    annotate("text", x = mu + 0.1, label = paste0("Mean\n", "P(p < Mean) = ", round(pbeta(mu, alpha, beta), 2), "\n", "P(p > Mean) = ", round(1 - pbeta(mu, alpha, beta), 2)), y = 10, color = "black") +
    geom_vline(xintercept = mode, colour = "blue") +
    annotate("text", x = mu + 0.1, label = "Mode", y = 13, color = "purple")
  plot(g)
}

```

En prenant des cas plus prononcés :
$\alpha < 1$, le mode n'est plus définis, la distribution est **J**-shaped, la densité de probabilités tend vers l'infini en se rapprochant de 0

```{r}
mu.1 <- 0.5
mu.2 <- 0.005
mu.3 <- 0.995
o <- 100
p <- seq(0, 1, 0.0001)
for(mu in c(mu.1, mu.2, mu.3)){
  alpha <- mu * o
  beta <- (1 - mu) * o
  g <- ggplot(data.frame(proba = p, density = dbeta(p, alpha, beta)), aes(x = proba, y = density)) + 
    geom_line() +
    ylim(c(0,100)) +
    theme_classic() + 
    stat_function(fun = dbeta,
                args = list(shape1 = alpha, shape2 = beta),
                geom = "area",
                fill = "#F8766D",
                xlim = c(mu, 1),
                ylim = c(0, 100)) +
    stat_function(fun = dbeta,
            args = list(shape1 = alpha, shape2 = beta),
            geom = "area",
            fill = "#619CFF",
            xlim = c(0, mu),
            ylim = c(0, 100)) +
    geom_vline(xintercept = mu, colour = "black") +
    annotate("text", x = mu + 0.1, label = paste0("Mean\n", "P(p < Mean) = ", round(pbeta(mu, alpha, beta), 2), "\n", "P(p > Mean) = ", round(1 - pbeta(mu, alpha, beta), 2)), y = 10, color = "black") +
    annotate("text", x = mu + 0.1, label = "Mode", y = 13, color = "purple")
  plot(g)
}

```

Comme on peut le voir à mesure de $\mu$ diminue, on a l'asymétriue de la distribution s'accentue ! 
On passe par exemple pour 

- $\mu = 0.05, s = 100$: $P(p \le \mu) = 0.56, P(p > \mu) = 0.44$

- $\mu = 0.005, s = 100$: $P(p \le \mu) = 0.68, P(p > \mu) = 0.32$ (quasi 2/3 - 1/3)


Ainsi dans notre cas, si on essaye de poser cette distribution à priori en fixant la moyenne $\mu$ comme paramètre, on risque de poser des distributions très asymétriques, généralement même avec un $\alpha < 1$. Ainsi la grande partie de la masse des probabilités se trouvera inférieure à la moyenne et cette asymétrie peut biaisé nos résultats. En effet, en voulant initialement centré à la moyenne on a pas de prior sur si les probabilité serait plutôt inférieure ou supérieure à celle-ci, on voudrait quelque chose. 
Deplus notre 'test' final consiste à chercher pour notre distribution à posteriori, $P(p <= p(m))$, on cherche qu'elle est la probabilité qu'a posteriori notre probabilité estimé soit inférieure à celle d'observer le MeSH de manière générale, plus cette proba ($P(p <= p(m))$) est faible, plus on est sur de notre proposition. Or dans ce cas, notre prior à déjà tendance à favoriser le rejet alors qu'il n'y pas de raisons apparente de considérer cela à priori.

Ainsi dans l'idée que notre distribution à priori décrive l'hypothèse d'indépendance, il semble alors plus correct d'utiliser le mode plutôt que la moyenne pour poser notre prior. Ainsi, dans l'idée du prior, on souhaite alors que la valeur $p(m)$ soit la valeur la plus probable dans notre distribution. En utilisant le mode pour construire notre distribution, on va donc poser un modèle dans lequel le pic de notre distribution sera centré autour de $p(m)$. 

Aussi, le fait de poser notre modèle autour du mode, impose que notre distribution sera unimodale, avec une valeur précise du mode, ce qui n'est pas le cas quand $\alpha < 1$ par exemple, où les densités de proba tendent vers l'infini quand les probas se rapproche de 0. Utiliser le mode pour poser notre modèle, impose donc que pour notre prior: $\alpha, \beta > 1$.
Ceci impose également que la densité sera nulle en 0 et en 1, ce qui est en accord avec notre hypothèse selon laquelle toute proba $p(m|s) > 0$.

On note le mode $\omega$, $\omega = \frac{\alpha - 1}{\alpha  + \beta - 2}$.
On pose également un second paramètre $s = \alpha + \beta$, la sample size, où *concentration* qui représente le *spread*, l'étendu de notre distribution. Plus $s$ est grand, plus la variance de la distribution sera faible, basé sur beaucoup d'observations, et quand $s$ sera faible, au contraire l'allure de la distribution sera très étendue, avec une grande variance. 

On a donc :

$\alpha = \omega(s - 2) + 1$ et $\beta = (1 - \omega) (s - 2) + 1$

C'est une paramétrisation connue de la beta décrite dans  John_K_Kruschke-Doing_Bayesian_Data_Analysis-EN.pdf pages.129-130  et sur Wikipedia ^^

Pour la paramètre $s$, la concentration, il va représenter la variance que l'on souhaite intégrée dans notre prior. Plus $s$ sera faible plus notre distribution à priori sera "large" et plus $s$ sera grand, on notre prior aura l'allure d'un pic !

Pour choisir une valeur initiale, on peut utiliser la mediane des tailles de corpus de nos species, en se considérant que celles avec au moins 1 article annoté.  On trouve alors 235, on peut donc mettre 200 comme valeur intiale.
Je pense qu'il sera intéressant de faire varier ce paramètre, car il gère la variance de notre prior



**MAIS EN FAIT C'EST TOUJOURS PAS LA SOLUTION !!!** 

Lors effectivement on obtinedra toujours une distribution unimodale etc ... **MAIS**, pour la répartition des probabilité on va obtenir l'effet inverse ... C'est à toujours que l'on aura tendance à favoriser les probabilité supérieure à $p(m)$ et en fait, je trouve que c'est pire que l'obtiens de la moyenne car ça augmentera le taux de Faux positif.. Ainsi, on préfère utiliser la moyenne $\mu$ qui certe va augmenter notre taux de faux négatifs, mais c'est moins grave que les faux positifs !!


Exemple 

On pose $s = 200$
Cas où l'on utilise $\mu$ pour poser notre prior

```{r}
data <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/data_specie100_mesh100_tspvalue.csv")
dt <- data[data$TOTAL_PMID_MESH > 9000 & data$TOTAL_PMID_MESH < 11000 & data$ts.p.value > 0.1, ]
s <- 11028.3
mu <- mean(dt$TOTAL_PMID_MESH)/N
alpha <- mu * s
beta <-  (1 - mu) * s

cc <- data.frame(p = c(dt$p, rbeta(10000, alpha, beta)), Type = c(rep("Observed", nrow(dt)), rep("Sampled", 10000)))

ggplot(cc, aes(x = log(p), y = 1, color = Type)) + 
  geom_density_ridges2(aes(fill = Type), alpha = .5, color = "white") + 
  scale_fill_cyclical(
    values = c("#ff0000", "#0000ff"),
    name = "Option", guide = "legend"
  ) +
  theme_ridges()
```

Quand on utilise le mode :

```{r}
mode <- mean(dt$TOTAL_PMID_MESH)/N
alpha <- mode * (s - 2) + 1
beta <-  (1 - mode) * (s - 2) + 1

cc <- data.frame(p = c(dt$p, rbeta(10000, shape1 = alpha, shape2 = beta)), Type = c(rep("Observed", nrow(dt)), rep("Sampled", 10000)))

ggplot(cc, aes(x = log(p), y = 1, color = Type)) + 
  geom_density_ridges2(aes(fill = Type), alpha = .5, color = "white") + 
  scale_fill_cyclical(
    values = c("#ff0000", "#0000ff"),
    name = "Option", guide = "legend"
  ) +
  theme_ridges()
```



Donc on utilisera $\mu$ pour poser notre distribution, avec le paramètre $s$ :

- $\alpha  = \mu s$ 
- $\beta  = (1 - \mu) s$

Maintenant, est-ce qu'on peut améliorer l'estimation de $s$ ?
Plot :

```{r}
N <- 8877780
data_mesh <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/mesh_pmid.csv")
cid_stats_assos <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/cid_mesh_stats_assos2.csv")
data_specie <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/species_cid_pmid.csv")
data_specie_mesh <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/species_cid_mesh_pmid.csv")
data <- data_specie_mesh %>% left_join(data_mesh, by = "MESH") %>% left_join(data_specie, by = "SPECIE") %>% mutate(p = COOC/TOTAL_PMID_SPECIE)
# On ajoute les stats sur les assos CID - MESH
cid_stats_assos <- cid_stats_assos %>% mutate(ID = paste0(CID, MESH)) %>% dplyr::select(ID, p.value, odds_ratio, q.value)
data <- data %>% mutate(ID = paste0(CID, MESH)) %>% left_join(cid_stats_assos, by = "ID") %>% dplyr::select(-ID)

data_plot <- data %>% group_by(MESH) %>% mutate(MEDIAN = median(TOTAL_PMID_SPECIE)) %>% dplyr::select(MESH, TOTAL_PMID_MESH, MEDIAN) %>% distinct()
ggplot(data_plot, aes(x  = log(TOTAL_PMID_MESH), y = log(MEDIAN))) + geom_point() + theme_classic()
```

En utilisant l'ensemble des associations, j'ai tracé le nuage de point montrant la mediane des tailles de corpus des composés en fonction de la taille corpus des MeSH (en log) et on voit une tendance très claire !
Alors pour que ce soit plus évident il faut considérer les tailles de corpus de MeSH à partir de 500 par exemple car avant on voit beaucoup de variabilité, il n'y a pas becoup d'associations pour calculer cette mediane.
À mesure que la taille du corpus augmente, et donc que l'on représente des termes de plus en plus larges (Coronary Occlusion -> Cardiovascular diseases), le corpus des composés associés médian est plus faible. En gros j'ai l'impression que ça montre que lorsque l'on parle de MeSH assez "rare", ce sont des composés avec beaucoup de literature qui arrive à les capter et que les composés "rare" ont donc tendance à rarement aborder des sujets  peu fréquement discutés. De la même manière, plus on regarde des composés avec des corpus forts, décrivant des concepts fréquents/broads, plus on va retrouver des composés peu fréquent dans les composés qui le discutte. Je pense que ça explique également pourquoi je vois beaucoup de variabilité pour les MeSH à fort corpus, ils sont dicutté par un large panel de composés avec des tailles de corpus différentes.

C'est pour les associations générale c'est pas valable si on considère que les signif ou autre. Aussi, c'est pas un biais des species du réseau, j'observe le même pattern sur les composés PubChem.

Ceci corrobore également l'observation que l'on avait fait précédemnt où il semblait que la variabilité de mes proba estimées était croissante par rapport à la taille de corpus :


```{r}
data_plot <- data %>% group_by(MESH) %>% mutate(VAR = var(p)) %>% dplyr::select(MESH, TOTAL_PMID_MESH, VAR) %>% distinct()
ggplot(data_plot, aes(x  = log(TOTAL_PMID_MESH), y = log(VAR))) + geom_point() + theme_classic()

```

Or, le paramètre qui va nous permettre de régler la variabilité c'est bien $s$, la taille de sample que l'on choisit pour notre prior.
Ce que l'on a constaté c'est que la mediane des species associées aux MeSH était décroissante par rapport à la taille du corpus MeSH. Les MeSH les plus rares sont abordés par des composés à fort corpus tandis que les MeSH fréquents, broads, sont abordés par l'ensemble des species, avec des faibles comme des forts corpus.

Ainsi pour les MeSh à faible corpus, sachant que ce sont des composés très fréquent qui en parle, le paramètre $s$ derrière la distribution à priori sera plus élevé, une variabilité faible, plutôt que pour les MeSH à fort corpus pour lesquel on admettra une plus faible valeur de $s$ et donc une plus grande variabilité

On va donc fitter la taille du corpus $s$ que l'on souhaite utiliser pour notre prior par rapport à la taille du corpus du MeSH. On fit en log ce sera mieux et en utilisant seulement les MeSH > 2000:

```{r}
data_fit <- data %>% filter(TOTAL_PMID_MESH > 2000) %>% group_by(MESH) %>% mutate(MEDIAN = log(median(TOTAL_PMID_SPECIE)), LOG_SIZE_MESH = log(TOTAL_PMID_MESH)) %>% dplyr::select(MESH, LOG_SIZE_MESH, MEDIAN) %>% distinct()
fit <- lm(data = data_fit, formula = MEDIAN ~ LOG_SIZE_MESH)
ggplot(data_fit, aes(x = LOG_SIZE_MESH, y  = MEDIAN)) + geom_point() + geom_line(data = data.frame(x = data_fit$LOG_SIZE_MESH, y = fit$fitted.values), aes(x = x, y = y, color = "red")) + theme_classic()
```

j'aime bien aussi comment ils parent de ce paramètre $s$ (appelé $k$ dans le texte suivant, traduit depuis la Ref: John_K_Kruschke-Doing_Bayesian_Data_Analysis-EN.pdf pages.129-130 ) :
la valeur que nous choisissons pour le κ peut être pensée de cette façon : C'est le nombre de nouveaux coups de pièce de monnaie dont nous aurions besoin pour nous faire osciller entre les nouvelles données et la croyance antérieure concernant μ. Si nous n'avions besoin que de quelques nouveaux coups de pièce de monnaie pour faire osciller nos croyances, alors nos croyances antérieures devraient être représentées par un petit κ. Si nous avons besoin d'un grand nombre de nouveaux mouvements pour nous éloigner de nos convictions antérieures sur μ, alors nos convictions antérieures valent un très grand κ.

Dans le cas de MeSH à faible corpus, je vois que les composés qui ont vu ces MeSH sont des composés à fort corpus, donc je sais avec sufisament de certitude qu'ils doivent être rare car même en piochant un grand nombre d'article je les voyais rarement. Ainsi je veux un prior fort, qui se base sur une forte taille de corpus. Dans le cas inverse, pour les MeSH à fort corpus, je les ai observés à la fois pour de composés à fort corpus mais aussi pour des composés à plus faible corpus sur lesquels on aura donc une plus grande variabilité, je souhaite donc avoir un prior plus faible basé sur moins d'effectifs.


TEST :
```{r}
data_ts <- read_csv("/home/mxdelmas/Documents/Thèse/building_database/FORUM/kg-explorer/data/prior_models/data_specie100_mesh100_tspvalue.csv")
dt <- data_ts[data_ts$TOTAL_PMID_MESH > 50000 & data_ts$TOTAL_PMID_MESH < 55000 & data_ts$ts.p.value > 0.1, ]
s <- exp(predict.lm(fit, data.frame(LOG_SIZE_MESH = c(log(mean(dt$TOTAL_PMID_MESH))))))
mu <- mean(dt$TOTAL_PMID_MESH)/N
alpha <- mu * s
beta <-  (1 - mu) * s

cc <- data.frame(p = c(dt$p, rbeta(10000, alpha, beta)), Type = c(rep("Observed", nrow(dt)), rep("Sampled", 10000)))

ggplot(cc, aes(x = log(p), y = 1, color = Type)) + 
  geom_density_ridges2(aes(fill = Type), alpha = .5, color = "white") + 
  scale_fill_cyclical(
    values = c("#ff0000", "#0000ff"),
    name = "Option", guide = "legend"
  ) +
  theme_ridges() +
  ggtitle(paste("MESH Aortic Stenosis, Subvalvular (D001020), taille = 3690 articles\nMoyenne observée = ", mean(dt$p), "\nMoyenne du prior = ", mu))



```
Alors on pourrait dire qu'il est "semi-théorique" car la moyenne de la distribution est fixé théoriquement par p(M), par contre le paramètre sigma de taille de l'échantillon, j'ai fais une regression "Mediane corpus composés" ~ "Corpus MeSH" pour le déterminer.
Vue que l'on a pas d'apriori sur le paramètre taille de l'échantillon de la distribution, on l'estime par la mediane des tailles de corpus des composés qui discuttent de MeSH avec des tailles de corpus similaires. Ainsi :

    MeSH à faible corpus: On impose une grosse taille d'échantillon, comme ce que l'on voit sur nos données ce sont essentiellement des composés à fort coprus qui les observent, on a ainsi peu de variabilité attendue
    MeSH à fort corpus: On impose une taille d'échantillon plus faible pour représenter le fait que même des composés à faibles corpus peuvent y être associé, on augmente la variabiltié attendue

## Simplification du modèle :

Notre modèle où l'on souhaite modéliser le paramètre *s* en fonction de la taille du corpus MeSH est peut être biaisé :

D'un coté en formulant les choses de cette manière, le modèle "semble" fonctionner correctement, mais quand j'essaie de me l'expliquer, je trouve qu'il y a des éléments dont je ne suis pas certains. Est-ce qu'au final ces différences de variabilité que j'observe et que je veux représenter, est-ce que c'est pas simplement des artefacts, justement due aux différences que j'ai sur les tailles de corpus de mes MeSH. Mais, si tout mes composés était a peu près représentés de la même manière dans la litterature (le cas idéal), est-ce que je verrais également ça ? Pas sur ...
Il y a une phrase dans un cours (Cf. Ref) que j'ai lu qui dit : "la valeur que nous choisissons pour le κ (le paramètre de taille d'échantillon) peut être pensée de cette façon : C'est le nombre de nouveaux coups de pièce de monnaie dont nous aurions besoin pour nous faire osciller entre les nouvelles données et la croyance antérieure concernant μ. Si nous n'avions besoin que de quelques nouveaux coups de pièce de monnaie pour faire osciller nos croyances, alors nos croyances antérieures devraient être représentées par un petit κ. Si nous avons besoin d'un grand nombre de nouveaux mouvements pour nous éloigner de nos convictions antérieures sur μ, alors nos convictions antérieures valent un très grand κ."
Je vais poser s (le paramètre de taille d'échantillon) comme un hyperparamètre en disant: Je pose un prior en considérant qu'il a été construit sur la base de s articles, ainsi on peut s'attendre à ce que seul les composé avec au moins s articles seront en mesures de faire changé d'avis mon prior. C'est à dire que rajouter au moins autant d'observations qu'il en a été utilisé pour construire le prior
De la sorte, j'impose une variabiltié sur le prior de tout les MeSH, $V(X) = \frac{\mu (1 - \mu)}{(1 + s)}$. Ainsi elle sera nécéssairement croissante en fonction de mu (jusqu'a 0.5 mais celui-ci n'est jamais atteint) et sinon sera décoissante par rapport à s
Ça me semble relativement propre ...
On connait facilement la moyenne et la variance de notre prior. On l'imagine donc comme étant issus de pseudo observations. On a donc un prior construit avec *\mu s* pseudo-succès et $(1 - mu)s$ pseudo échecs
Après peut être que c'est un hyperparamètre que l'on a envie de faire varié pour rendre plus ou moins "fort" le prior ....

**Cependant,** vue que dans la majorité de nos cas notre alpha sera < 1, et pour la beta il semble que l'on puisse interpréter alpha - 1 en terme de pseudo count, et non alpha ! : Cf Wiki :
In Bayesian inference, using a prior distribution Beta(αPrior,βPrior) prior to a binomial distribution is equivalent to adding (αPrior − 1) pseudo-observations of "success" and (βPrior − 1) pseudo-observations of "failure" to the actual number of successes and failures observed, then estimating the parameter p of the binomial distribution by the proportion of successes over both real- and pseudo-observations. A uniform prior Beta(1,1) does not add (or subtract) any pseudo-observations since for Beta(1,1) it follows that (αPrior − 1) = 0 and (βPrior − 1) = 0. 
Donc en fait si mon composé n'a pas d'observations succès, son nombre de pseudo-counts success sera négatif !!!
Peut être que dans ce cas il vaut mieux le dégager du prior ...
Pour ré-équilibrer les weights on a juste à recalculer les proportions sans lui ... et comme ça on ne le prends plus en compte. Si aucun voisin ne reste, peut être que l'on ne peut rien faire ...

A voir avec Pablo et tout !!

Ce qui dis Wikipedia : 
The Haldane prior Beta(0,0) subtracts one pseudo observation from each and Jeffreys prior Beta(1/2,1/2) subtracts 1/2 pseudo-observation of success and an equal number of failure. This subtraction has the effect of smoothing out the posterior distribution. If the proportion of successes is not 50% (s/n ≠ 1/2) values of αPrior and βPrior less than 1 (and therefore negative (αPrior − 1) and (βPrior − 1)) favor sparsity, i.e. distributions where the parameter p is closer to either 0 or 1. In effect, values of αPrior and βPrior between 0 and 1, when operating together, function as a concentration parameter. 

Après comment choisir un *s* par défaut ?


Alors j'ai peut être trouvé une méthode pas trop mal pour ce donner une idée :

Je récupère des associations significatives de FORUM (q.value <= 1e-6), sans weakness et parlant de diseases, j'en échantillonnage 1 million par exemple 

Vue que dans notre modèle thérorique mu est fixé par la taille du corpus du MeSH, seul s, la taille d'échantillon peut varier

On sélectionne plusieurs valeurs possible de s, j'ai pris:  c(10,20,30,40,50,60,70,80,90,100,150,200,250,500,750,1000,1500,2000)

Pour chaque valeur de s, on calcule la distribution à posteriori que l'on obtiendrait pour chaque observation en utilisant un prior construit avec s

Pour chaque distribution à posteriori obtenu, on calcule la valeur P(p <= mu), qui est la probabiltié selon ma distribution que la valeur de p soit inférieure à la probabilité d'observer le MeSH de manière générale. Plus elle est faible plus on est certain que notre association est signficative, notre proba est très certainement supérieure à la proba d'observer le MeSH de manière générale. C'est notamment comme ça que je test l'asssociation avec le modèle de mélange à la fin.

Enfin, on calcule la proportion d'associations pour lesquelles P(p <= mu) indique toujours que l'asso est significative. Cela indique que notre prior n'a pas écrasé notre observation et que ceux qui était significatif l'est toujours selon la distribution

J'ai vérifié initialement et en utilisant un prior non-informatif 100% des assos sont retrouvé donc pas de biais :wink:


Et on peut faire pareil pour els assos Négatives !!! 

Ça nous donne une idée de comment notre paramètre *s* influe sur les taux de faux-positifs et négatifs ;)

### Nouvelle manère de voir les choses :

En fait c'est pas si différent mais dans mon approche actuelle il y a peut être quelque chose de mal gérer au niveau de mes poids
- initialement j'utilise ma probabilité FOT, qui représente en gros la proportion de marcheurs qui arrivent sur mon noeud cible. La chose étant, mes distribution à posteriori représente la probabilité qu'un article parlant du composé $i$ discutte du MeSH $M$. Ainsi, d'un coté je discutte de marcheurs qui représente en gros des paquets d'information et de l'autre coté on a des articles, c'est pas vraiment la même chose... En fait je pense que pour que mon modèle soit vraiment fluide, il faudrait que j'utilise les probabilité qu'un *article* du composé $i$ arrive sur mon composé cible. De cette manière, mon poids dans la mixture correpondent à la probabilité qu'un article arrivé sur le composé cible proviennent du composé $i$, et les proba associé correspondent à la probabilité qu'un article provenant du composé $i$, discutte de mon MeSH. Cela semble collé.

C'est pablo qui a soulevé le problème. En effet mes probabilité jointe, qu'un article arrivé mon composé cible discutte du composé $i$ **ET** du MeSH $M$ snt assez flou car ce que je fais c'est une moyenne à la fin ... "Define the full joint probability of each node, which is not the same as averaging the outputs of the other nodes"

**ATTENTION: ** Quand je faisait mon test de Fisher, je considéré mes composés entre eux, et mes MeSH entre eux, comme étant **des variables différentes** et non **des modalités** d'une même variables !! Dans mon test de Fisher l'évènement: l'article discutte du composé $i$ est une variable binaire: l'article discutte ou non du composé $i$, pareil pour les MeSH ! On avait jamais à prendre en compte le cas où un article parlait de plusieurs composés ou MeSH ... MAIS là, si on veut les considéré comme des modalités d'une même variable ça va être différent !!!!

Ainsi, on considère la variable **composé** comme étant les modalité d'une même variable du type: "la mention dans la literature, implique le composé $i$" **MAIS** la variable MeSH est toujours du type binaire, la mention implque ou non, le mesh $m$. Ainsi, on duplique les associations uniquement par composé mais pas par MeSH.
J'ai fait un schema pour mieux comprendre.

Mais par contre: si on divise également nos mentions par MeSH: a ce moment toutes les variables deviennent des variables à modalité et à ce moment là, je poourrait utiliser une distribution de dirichlet comme prior !!!

Ainsi, peut être qu'il faut que l'on discutte en terme d'association ! "Une asociation implique mon composé et mon MeSh" Ainsi, ce que l'on diffuse depuis les composés dans le réseau, ce sont des citation, des mentions, d'un composé avec un MeSH dans un article

Cela change donc la probabilité simple d'observer 1 MeSH !!!
En effet, désormais on ne cherche plus à savoir la probabilité d'un article discutte du MeSH, mais à savoir la probabilité qu'une "mention" dicutte du MeSH
Néanmoins on peut faire la même hypothèse en terme d'indépendance ! La proabilité qu'une mention implique le MeSH $M$, sachant qu'elle implique dle composé $i$ est égale à la proba générale qu'une mention implique le MeSH $M$.

Le modèle posé ne change pas :

pour un composé $C_i$, j'ai $N_i$ article associé, formant $N_i$ mention de ce composés avec $k_i$ impliquant également mon MeSH $M$
Pour une mention quelconque impliquant ce composé $C_i$, il a une probabilité $p_i$ qu'elle implique mon MeSH $M$. Si on note $X$ la variable aléatoire suivant un loi de Bernouli avec $X = 1$: "la mention parle du MeSH $M$" et $X = 0$: "la mention ne parle pas du MeSH $M$". J'ai $X \sim Be(p_i)$
Cet épreuve est répété $N_i$ fois, et donc j'ai une suite de variables $X_1^i,X_2^i, ..., X_{N_i}^i$ qui suivent cette loi de Bernouli $Be(p_i)$

On peut donc dire que leur somme pour le composé $i$, $\sum_{k = 1}^{N_i} X_k^i$, suit une loi Binomiale de même paramètre $p_i$, $Bin(p_i, N_i)$

Le fait est que je dispose également d'un apriori sur ce paramètre $p_i$ représente par ma distribution Beta: $Beta(\mu \nu, (1 - \mu) \nu)$ où $\nu$ est un hyperparamètre fixé et $\mu$ représente la probabilité générale, sur l'ensemble des mentions observées, qu'une mention implique le MeSH $M$



On peut donc redéfinit notre modèle et nos probabilité marginale :

Les évènements:

- $m$: la mention parle/implique le mesh $m$
- $c_i$: la mention parle/implique le composé $c_i$
- $I = k$ la mention a atteint le composé cible $j$

$f(m|I = k) = \sum_{c= 1}^{N} p(m|c_i)p(c_i|I = k)$, soit la probabilité d'observer une mention impliquant le mesh $m$ parmis celles arrivées sur le composé d'intérêt $k$. 
$p(c_i|I = k)$ représentant la probabilité qu'une mention, arrivée sur le composé cible $k$, provienne de $c_i$, c'est les poids dans le modèle.
$p(m|c_i)$ representant la probabilité qu'une mention parlant du composé $c_i$, implique également le mesh $m$. Ceci étant indépendant de l'évènement $I = k$: la mention arrive sur le d'intérêt $k$. en gros le fait de savoir que la mention est sur le composé d'intérêt n'influe pas les proba qu'elle implique que MeSH et donc : $p(m|c_i, I = k) = p(m|c_i = k)$
$f(m|I=k)$ représente donc la probabilité qu'une mention arrivant sur le composé cible $k$ mentionne le mesh $m$.

En effet: 
$f(m, c_i) = p(m|c_i)p(c_i)$
$f(m, c_i|I=k) = p(m|c_i, I=k)p(c_i|I=k) = p(m|c_i)p(c_i|I=k)$ car indepéndace à $p(m|c_i, I=k) = p(m|c_i)$
Et donc si on somme pour tous les $c_i$, on a bien que $f(m|I=k) = \sum_{c=1}^{N} p(m|c_i)p(c_i|I=k)$



Ensuite, à partir de cette distriubution, on calcule la probabilité à posteriori comme avant en ajoutant les observations qui sont propre au composé cible !

Don en gros les choses qui changent par rapport àce que j'avais prévu intialement sont :

- On considère des mentions et non des articles : Donc on calcule les probabilité marginales $p(m)$ avec la somme des co-occurences (qui représente les mentions) et non plus en utilisant le nombre total d'article.
$N_i$ est le nombre total de mentions de composé $i$, ce qui correspond toujours à son nombre d'article annotés
$N_i^m$ est le nombre de mentions du composé $i$ impluquant également le mesh $m$, c'est toujours la coocurence.
$p(m|c_i) = \frac{N^m_i}{N_i}$, c'est la probabilité qu'une mention du composé $i$, implique le mesh $m$, c'est toujours la co-coocurence sur la taille du corpus du composé
$p(c_i) = \frac{N_i}{\sum_j N_j}$, cette probabilité n'est plus calculé en utilisant l'Univers, le nombre d'articles distincts, mais, en utilisant la somme du nombre de mentions (aka nombre d'articles) annotés à chaque composé
$p(m) = \sum_i p(m|c_i) p(ci) = \sum_i \frac{N_i^m}{Ni} \frac{N_i}{\sum_j N_j} = \sum_i \frac{N_i^m}{\sum_j N_j}$

- Ainsi mes poids dans ma mixture : $p(c_i|I = k) = \frac{p(c_i, I=k)}{p(I=k)} = \frac{p(c_i, I=k)}{\sum_jp(c_j, I=k)} = \frac{P(I=k|c_i)p(c_i)}{\sum_j P(I=k|c_j)p(c_j)}$
Cela représente la probabilité qu'une mention arrivé sur le composé $k$ provienne du composé $C_i$

### Comment définit on $P(I = k|c_i)$ ?

Pour définir cette probabilité, on souhaite appliquer plusieurs contraintes :
- 1) Un composé ne peut pas contribuer à lui-même
- 2) chaque composé à un certain **voisinnage d'influence** c'est à dire un set de composé pour lesquels il est légitime qu'il contribue à la construction du prior. Cette deuxième contrainte à notamment due être mise en place car il se trouve que sinon, les STARs pouvant globalement contribuer à tous les éléments du réseau, peuvent être des contributeurs majoritaires dans des zones où aucun voisin n'a de littérature. Pour autant ces STARs se situent généralement bcp plus loin dans le réseau et ne sont pas pertinentes pour contruire le prior de ces composés. Dans ces cas là, on préfèrera utiliser le prior par défaut. C'est pour cela que l'on définie une zone d'influence pour chaque composé.

La solution retenue est de créer une matrice de containte afin de sélectionner a quel autre composé, un composé peut il contribuer de manière légitime. Aussi, on empêche toujours un composé de contribuer à lui-même.
Donc, pour contruire cette matrice de contrainte, on va demandé à ce que la probabilité d'atteindre un noeud $k$ depuis un potentiel contributeur $i$ (hors du noeud cible) soit au moins supérieure à $\frac{1}{n -1}$ (n étant le nombre total de noeud, on fait -1 car on compte pas le noeud lui-même) c'est à dire à la probabilité de sélectionner ce noeud au hasard. De manière plus formelle, on demande donc que :
$\frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$ soit que la probabilité d'atteindre $k$ depuis $i$ (sans considéré les marches retournant à $i$) est supérieure à la proba de choisir $k$ au hasard parmis tout les noeud du réseau.

Ceci équivaut donc aussi à $\frac{\pi_{k,i}}{(1 - \pi_{i,i})} (n - 1) > 1$ ce qui offre une interprétation un peu plus intuituve. Imaginons que chaque composé dispose de $n - 1$ cartons d'invitation, donc 1 pour chaque autre noeud dans le réseau, c'est comme si on simulait la propagation de ces cartons suivant la marche définie et ensuite on ne récupère que les composé qui ont reçu au moins un carton d'invitation. Ainsi les composés qui peuvent influé $k$ sont les composé pour lesquels $k$ à reçu au moins un carton d'invitation provenant d'eux.

En se positionnant sur le composé cible, on va donc chercher à déterminer l'ensemble des noeuds qui semblent légitime, d'après cette contrainte, pour contribuer à son prior.
On va appelé les composés qui respectent cette régle les **contributeurs** de $k$ (les noeuds pouvant influencer $k$) que l'on notera $CTB_k$. 
Ainsi, $\forall i \in CTB_k, i \neq k: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$. Vue que l'on empêche les composés de contribuer à eux-même, le composé $k$ ne peut pas faire partie de ses contributeurs donc il ne contribue toujours pas au prior ! On peut donc également dire que pour être selectionné comme contributeur d'un composé, il faut atteindre ce composé via une marche aléatoire, avec une probabilité qui soit supérieure à celle d'être selectionnée dans le cas d'une selection au hasard des contributeurs (aussi $\frac{1}{n -1}$).

Symétriquement, on peut définir le **voisinnage d'influence**, noté $IN_i$ de chaque composé $i$ comme étant le set de composé qu'il peut légitimement influencer au vue des contraintes que l'on a posé !

 $\forall k \in IN_i, k \neq i: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$

On impose que $i \notin IN_i$ 


Comme on le faisait avant, on va donc re-estimer les probabilité pour qu'elles ne se concentre sur les composé que $i$ peut légitimement influé. On va donc dire que :

$\forall k \in IN_i: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$ On re-proportionne les probas en se basant uniquement sur le set que $i$ peut influé.

$\forall k \notin IN_i: P(I = k|C_i) = 0$ avec $i \notin IN_i$ 

Réciproquement,

$\forall i \in CTB_k: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$

Sinon, 

$\forall i \notin CTB_k: P(I = k|C_i) = 0$ avec $k \notin CTB_k$


C'est comme ça que donc définies les probabilités $P(I = k|C_i)$ que l'on va ensuite utilisées pour calculer les poids dns notre modèle : $P(C_i|I = k)$

====================

Donc :

- Ainsi, pour $i \notin CTB_k$ (dont $k$ lui-même donc):

$p(c_i|I = k) = \frac{P(I=k|c_i)p(c_i)}{\sum_j P(I=k|c_j)p(c_j)} = 0$ car $P(I = k|c_i) = 0$ 

- Pour $i \in CTB_k$:

$p(c_i|I = k) = \frac{P(I=k|c_i)p(c_i)}{\sum_{j \in CTB_k} P(I=k|c_j)p(c_j)} = \frac{\frac{\pi_{k,i} }{\sum_{v \in IN_i} \pi_{v,i}} \frac{N_i}{N} }{\sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} \frac{N_j}{N} } = \frac{\frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}} N_i}{\sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} N_j}$

Pour calculer $P(I = k)$ au dénorminateur, on a juste besoin de faire la somme sur les $j \in CTB_k$ car si $j \notin CTB_k$, alors $P(I = k|c_j) = 0$

Ainsi, on calcule les poids en utilisant uniquement les probabilités que des contributeurs légitimes envoient une de leurs mentions vers le composé cible. Néanmoins, il peut arriver que seule une sous-partie (ou voir même aucun des contributeurs légitime) n'est de mentions à partager car il ne dispose pas de littérature. A ce moment là, la somme des $p(c_i|I = k)$ pour tous les $c_i$ est nulle, c'est à dire qu'aucune mention venant des contributeurs de $k$ ne peut arrivé sur $k$ car ils n'ont pas de mention à partagé. A ce moment là, on utilise le prior par défaut. Dans ce cas précis, on peut constater que $P(I = k) = 0$ car $P(I = k) = \sum_{j \in CTB_k} P(I=k|c_j)p(c_j) = \sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} \frac{N_j}{N}$ Or si tous les $N_j = 0$ pour les $j \in CTB_k$, alors on a effectivement $p(I = k)$ et donc il est déjà impossible qu'une mention arrive sur le composé $k$

La matrice $pi$ contient les vecteurs de résultats du PPR rangées **en colonnes** ! Ainsi, $\pi_{k,i}$ représente la probabilité attendu d'atteindre le noeud $k$ dans le réseau pour une marche aléatoire de type PPR partant du noeud $i$.

Donc voilà pour les poids ! :)

- Indépendance conditionnelle de  $p(m|c_i, I=k) = p(m|c_i)$
En effet, si on voulait déterminer $p(m|c_i, I=k)$ on ferait $p(m|c_i, I=k) = p(m|c_i) = \frac{N_i^m}{N_i} = $

Donc finalement avec toutes ses probabilités, on va pouvoir faire la même chose ! C'est juste que maintenant notre prior parle en terme de mentions et non plus d'articles mais l'idée d'indépendance qu'il soutient est toujours la même. Sous hypothèse d'indépendance $p(m|c_i) = p(m)$, le fait que la mention implique/provienne du composé $c_i$ n'a aucun effet sur le fait qu'elle implique le MeSH $m$, donc on peut toujours mettre notre $\mu$ à $p(m)$

Ensuite on ajoute effectivement des observations. Pour chaque composé $c_i$, initialement j'ai un apriori sur $p(m|c_i)$, dans lequel $\mu = p(m)$. Mais j'observe ensuite mes observations $N$ et $N_i$ qui me permettent de mettre à jour ma probabilité $p(m|c_i)$, qui est toujours indépendance de $I=k$. Ensuite je peux utiliser cette probabilité pour construire mon prior pour le composé cible. Je cherche alors $p(m|I=k)$ qui est la probabilité qu'une mention parle du MeSH $m$ sachant qu'elle est arrivé sur le composé cible ! Alors, $p(m|I=k) = \sum_{c= 1}^{n} = p(m|c_i)p(c_i|I=k)$, où $p(c_i|I=k)$ est simplement la proba qu'une mention ayant atteint le composé cible $k$, provienne de $c_i$ !


$\sigma_i$ étant simplement la probabilité qu'une mention atteigne le composé cible (évènement $I$), sachant qu'ell est partie du composé $c_i$

### Ce qui change :

Désormais, les poids dans le modèle de mélange sont également fonction de la taille du corpus du composé (en terme de nombre de *mentions* à des articles)
Ceci implique que **les composés sans literature ne participent PAS à la construction des priors**

Ainsi, si un composé est entouré uniquement de composés non-informatif (en utilisant $\alpha = 0$ par exemple), il se peut que l'on ne puisse pas lui définir de prior car tous ces voisins sont non-informatif. A ce moment là, on affiche un Warning: "Neiborhood literature information does not reach the targeted compound. You should increase the damping factor. Use default prior." et on va réaliser l'analyse en considérant comme un prior, le prior général associé au MeSH, déterminé en fonction de $\mu$ et $\nu$. Ainsi, il n'y **pas** de distribution de mélange construite, c'est un prior simple !
On renvoie les même valeurs que pour une analyse classique (Mean, CDF, log2FC) mais pas le *Log2CDFRatio* puisque notre prior est exactement le même que celui qui est prévu de manière générale ..

Une autre chose à bien voir est que même si l'on utilise la taille de corpus dans le calcul des poids, on utilise seulement le corpus effectif associé à chaque composé, on ne compte pas les articles *imaginaires* qui sont issus du prior général dans ce calcul ! Ces articles *imaginaires*, dont le nombre est gérer par l'hyperparamètre $\nu$, ne sont utilisés que pour éviter des biais par rapport à des voisins qui aurait peu de literature et dont les probabilités seraient peu fiables. Ainsi en réalité ce prior général n'est appliqué que sur les composés qui disposent de literature car les autres composés n'ayant pas de littérature et donc un poids nul, ne sont pas pris en compte dans l'analyse.


L'avantage par rappport à l'ancienne version est que l'on perd le coté ultra conservateur de la méthode. Avant, sachant que les poids était simplement calculé en fonction du PPR, un métabolite avec 10 articles associés pouvait avoir autant de poids dans le prior qu'un composé avec 10000 article. Ce qui fait que même dans des cas où 9 voisins sur 10 par exemple indiquaient un lien avec le MeSH, il suffisant qu'un seul des voisins ne l'indique pas pourque le modèle rejette l'association. Maintenant que les poids de le modèle sont calculé en fonction de la littérature, il faudra que ce dernier voisin est beaucoup de littérature pour faire penché la balance.


### Loi géométrique et longueur moyenne des marches

Ref: Cf. cours/stats.oagerank.pdf

Pour modéliser la taille moyenne des marches, on peut utiliser la loi géométrique.
La loi géométrique  définit la probabilité $P(X = k)$ de la distribution du nombre $k$ d'échecs pour une série d'expérience de Bernouli, avant le premier succès :
$P(X = k) = (1 - p)^k p$ pour $k \in \{1,2, ..., n\}$
Ici $p$ est la probabilité de succès, on réalise $k$ échecs avec une proba $(1 - p)$ avant le premier succès avec une proba $p$
Dans notre cas, on définit comme succès, le fait de restart notre marche, la marche donc d'une marche. Donc $p = (1 - \alpha)$
Dans notre PPR classique, on a donc $P(X = k) = \alpha^k (1 - \alpha)$
La moyenne de cette distribution est alors $\frac{\alpha}{(1 - \alpha)}$

Sachant que notre PPR restart sur notre noeud cible, cela nous montre donc la taille moyenne d'une marche partant de notre noeud cible. ON en en moyenne visiter $k$ noeud, avant de revenir sur notre noeud de départ. 

On peut également constater que le ratio entre les probabilité à $k+1$ et $k$ va être constant et fonction de $\alpha$ $\frac{P(X = k)}{P(X = k+1)} = \frac{\alpha^k (1 - \alpha)}{\alpha^{k+1} (1 - \alpha)} = \frac{\alpha^k}{\alpha^{k+1}} = \frac{1}{\alpha}$
Par exemple donc pour $\alpha = 0.1$ on a donc 10 fois plus de chance de faire une marche de $X = k$ que de $X = k+1$ 

### Résultats de validation

Les résultats de validation montre les ROC curves par rapport aux différentes indices utilisés: CDF, Log2FC, ou le score.

- Globalement Score et Log2FC semble donner le même score lorsque l'on considère notre set négatifs comme étant des associations sous-représentées (OR < 0.1 & q.value = 1). C'est la manière la plus simple de définir le set de négatif, car dans le cas là on limite de taux de faux-négatif en utilisant ce dont le odd-ratio tend à représenter une sous-représentation.

- Néanmoins, le score que l'on a développé semble pertinent pour du moins ranker les associations

Les effet des facteurs alpha et sample_size sont également nets.

On a réalisé les barplots de TPR et FPR en utilisant différents seuil sur le Log2FC: 1,2,3

Globalement, le taux de True-positive et de FPR diminue avec alpha et également avec sample_size.

Comme attendu, plus on va chercher des articles loin dans le réseau plus :
  1) On prend le risque de récupèrer des articles de composé 'trop aloignés' pour être également associé à notre concept d'intérêt
  2) Si le prior tend à confirmer l'association, il faut que nos voisins les plus proches est beaucoup de litterature pour rester majoritaire dans le prior

Pour le paramètre sample_size, comme on peut le constater plus il est élevé plus notre TPR et FPR diminue, le prior devenant trop puissant pour faire changer d'avis. Vue qu'il représente en gros l'indépendance, on a aucun positif.


### Focus analysis
Comme l'analyse globale que l'on a réalisée dans FORUM, la principale faiblesse de notre approche est que nous testons des relations et construisons des aprioris sur des relations sans connaitre la nature de cette relation. 
On pourrait donc très bien imaginer que 2 composés contribuent de manière équivalente à un prior, mais que l'un indique en fait une utilisation thérapeutique, alors que l'autre est un biomarker. Ici les deux composés contribuent de manière équivalente à l'apriori que l'on va poser sur la relation, mais décrivent deux relations différentes, que l'on utiliserait de manière équivalente pour construire un prior sur une relations sans nature précise, ce qui est problématique
Il faut donc arriver à extraire un set d'article associés à une relation spécifique (therapeutiques, biomarqueurs, etc ...) entre la molécule et la maladie
Pour cela on va utiliser les qualifiers associés aux termes MeSh des maladies pour notamment distinguer les mentions de composés dans des artciles dans des cas thérapeutiques, on de diagnostiques par exemple.

https://www.nlm.nih.gov/mesh/qualifiers_scopenotes.html

Quelques qualifiers qui pourrait nous être utiles: 

Tout n'est pas lié aux relations chemicals - **diseases**, mais 

- Chemically Induced:  	Used for biological phenomena, **diseases**, syndromes, congenital abnormalities, or symptoms caused by endogenous or exogenous substances.

- Complications: Used with **diseases** to indicate conditions that co-exist or follow, i.e., co-existing diseases, complications, or sequelae.

- Diagnosis: Used with **diseases** for all aspects of diagnosis, including examination, differential diagnosis and prognosis. Excludes diagnosis using imaging techniques (e.g. radiography, scintigraphy, and ultrasonography) for which "diagnostic imaging" is used.

- Drug Therapy:	Used with **disease** headings for the treatment of disease by the administration of drugs, chemicals, and antibiotics. For diet therapy and radiotherapy, use specific subheadings. Excludes immunotherapy for which "therapy" is used.

- Therapy: Used with **diseases** for therapeutic interventions except drug therapy, diet therapy, radiotherapy, and surgery, for which specific subheadings exist. The concept is also used for articles and books dealing with multiple therapies.

- Metabolism: Used with organs, cells and subcellular fractions, organisms, and **diseases** for biochemical changes and metabolism. It is used also with drugs and chemicals for catabolic changes (breakdown of complex molecules into simpler ones). For anabolic processes (conversion of small molecules into large), BIOSYNTHESIS is used. For enzymology and pharmacokinetics, use the specific subheadings.

- Used with drugs, biological preparations, and physical agents for their use in the prophylaxis and treatment of disease. It includes veterinary use.

- Toxicity: Used with drugs and chemicals for experimental human and animal studies of their ill effects. It includes studies to determine the margin of safety or the reactions accompanying administration at various dose levels. It is used also for exposure to environmental agents. Poisoning should be considered for life-threatening exposure to environmental agents.


Après dans notre cas ce sont les diseases qui sont représentés par des MeSH  et donc pour lesquelles on peut facilement utiliser les qualifiers. Dans le cas des composés, certes ils peuvent être décrits à l'aide de meSH indéxant la publication, néanmoins dans notre cas, c'est en utilisation les NCBI Eutilities que l'on va pouvoir les déterminer.
  

Si on regarde un peu les résultats

HE/Metabolism VS (HE/Metabolism - HE/Therapy&Co)

Pas de grosse diff, les molécules sont relativement les même mais apparaisse dans des ordres légèrement différents.


Lorsque l'on regarde sans filtre sur les qualifiers (les assos classiques) il y a deux molécules qui arrivent en top liste mais qui pourtant sont beaucoup bas dans la liste lorsque l'on regarde seulement metabolism par exemple, c'est "bilirubin" et "acetaminophen/paracetamol"

Pour "acetaminophen/paracetamol", on le retrouve notamment attaché au qualifier diagnostic, mais non pas car cette molécule est dosé pour diagnostiqué la maladie, mais car on diagnostique des HE chez des patients ayant reçu de forte dose de paracétamol car cela peut entrainé des liver failures dont l'HE.

Bilirupin quant à elle est bien dosé pour diagnostiquer l'HE, mais pourtant elle n'est pas mentionné souvent lorsque l'on discutte du métabolisme de l'HE

Il faut peut être voir à se focaliser plus sur 'diagnostic' mais en intégrant également des filtres sur "Complication" et "Chemically induiced" pour bien spécifier que l'on veut parler de diagnostic au sens : on dose la molécule pour diagnostiquer la pathologie.




### Limite sur le damping factor alpha: 

Lorsque l'on réalise la truncation de la matrice d'adjascence, on supprime les noeud cible et tout les liens qu'il créée, il est possible que l'on crée des noeuds flottants, si par exemple le noeud cible était le seul voisin de ce noeud.
Dans la construction de la matrice de proba, ce problème est réglé par les sink node: si aucun noeud n'est lié à mon noeud, alors toutes ces proba seront construite à l'aide de alpha. Même si alpha = 1, ça fonctionne.


Mais la chose ce complique si au lieu de créée un noeud flottant, par suppression du noeud cible, on crée un sous-composante du graph ! Par exemple une partie des voisins du noeud target forment une chaine. lors que l'on supprime le noeud target, on se retrouve avec un chaine 'flottante' qui n'est plus relié au reste du graph ! 
Le problème c'est qu'elle ne sont pas considéré comme des sink nodes, car chaque membre de la châine a au moins 1 voisin ...
Ainsi ce qui d'habitude corrige se problème ne peut pas intervenir ici car ce ne sont pas des sink nodes , ainsi dans le calcul de la matrice de proba $M = alpha * P + (alpha * a + (1 - alpha) * e) @ v.T$ 
La partie $(alpha * a + (1 - alpha) * e) @ v.T$ est égal à 0 car ce ne sont pas des sink nodes et alpha = 1. Ainsi il n'y a pas d'ajout de proba de restart qui nous assure de pouvoir redémarrer sur les voisins du noeud et donc d'avoir une structure connexe !!
Or si on a pas de structure connexe, on ne peut pas assurer la convergence du PPR !

Ainsi pour assurer la convergence du PPR et donc que notre graph, même après suppression de notre noeud target forment toujours une composante connexe, on a besoin de la probabilité de restart, car ce restart sur les voisins du noeuds cible, compense la suppression du noeud !
Ainsi il faut poser alpha entre [0, 1[ soit 0 compris mais 1 non-compris !!


Deplus, même dans le cas où l'on pose alpha = 0, si le noeud formait le seul chemin à travers nos voisins, grâce aux probabilité de restart on reconnecte tous les voisins entre eux


EDIT: Finalement, on observe ce phénomène pour toute valeur de alpha qui tend vers 1. On peut observer la création de pseudos sous-composantes dans les graphs lorsque l'on va utiliser des valeurs de alpha proche de 1 (0.99, etc ...)

l'inconvénient est que la création de ces sous-composantes perturbe le calcul des poids. Lorsqu'une sous-composante est crée (par retrait du noeud d'intérêt) une partie, proportionnelle au nombre de voisins formant cette sous-composante, des mentions de la cible est "piégée" dans cette sous composante ...
Si par exemple pour un composé à 3 voisins, la suppression de celui-ci entraine la création d'une clique avec deux de ses voisins, c'est 66% de ces mentions (2/3) qui seront piégés dans cette cliques.
Alors certe, le PPR convergera mais on aura 66% des mentions piégé dans la clique.
Ceci peut avoir une influence importante sur le calcul des poids associés à ces voisins appartenant à la clique. 
En effet, du au fort damping factor, ils vont recevoir un faible pourcentage des mentions venant de tous les autres composé MAIS vont recevoir 33% des mentions venant du composé cible précédent ! Ce qui fait que ce composé devient forcément majoritaire dans les poids, même si sa taille de corpus est beaucoup plus faible que ces autres voisins.
Pour donner un exemple, dans le cas des poids associés au composé M_C01041 dans Recon2.03_Compound_Graph, le composé le plus majoritaire dans son prior pour un alpha proche de 1, est M_dhdascb. Or M_dhdascb est un voisin direct de M_C01041 comme M_ascb_L qui pourtant à 5x plus de mentions annotés que M_dhdascb. 
Les deux étant des voisins direct, c'est M_ascb_L qui devrait être majoritaire dans le calcul des poids car il a beaucoup plus de litterature. Or, due à la création d'une pseudo-clique lors de la suppression de M_dhdascb avec un alpha proche de 1, 33% de ces mentions viennent enrichir le prior de M_C01041, ce qui fait que M_dhdascb est majortaire.

Pour contrer ce biais, on propose une autre version du calcul des probabilités.
La suppression temporaire du noeud dans le graph peut entrainer la formation de pseudo-clique, car lorsque alpha sera proche de 1, les voisins ne seront reliés entre eux que par la probabilité de restart qui sera très faible.
Or, en ne reliant que par la probabilité de restart, on ne considère pas que de base, il existe une connexion entre eux à travers le composé cible et donc on sous-estime énormément les probabilité de passer de l'un à l'autre. La suppression temporaire du noeud cible, nous a en fait supprimer des chemins qui connecter nos voisins entre eux. Simplement les relier à l'aide de la proba de restart sous-estime beaucoup les proba de transité entre voisins lorsque alpha est proche de 1.

On va donc réaliser le PPR, toujours en utilisant un vecteur de restart par rapport aux voisinnage, car :
  - C'est le voisinnage autour du composé cible qui nous intéresse
  -  Cela permet de maintenair la propriété que si alpha = 0, seul les voisins directs comptent !

On va réaliser le PPR classique sans supprimer le noeud d'intérêt !!
En revanche à la fin on va re-estimé les probabilités. En effet, on s'interresse à la proportion du temps passé, aux probabilité d'être sur un noeud, mais HORS du noeud cible.  
Ainsi, à la fin du calcul, on attribue la valeur 0 au noeud cible dans le vecteur de probabilité stationnaire et on re-calcule les probabilité. Ce vecteur re-estimé représente alors les probabilités, la proportion du temps passé sur les noeuds, hors du noeud cible !
Ce qui à nouveau nous permettra d'estimer à l'aide des poids la proportion de mentions partant du noeud cible qui vont aller influencer le prior des voisins, sans que le noeud ne s'influence lui même !!

Globalement le petit changement que j'ai pu observer en regardant les probabilités que l'on obtenaient sur notre graph de test avec l'urée (Cf. data/cytoscape/SFT_V2.cys) c'est qu'en utilisant le graph complet, cela semble favoriser le voisinnage proche de composé (ce que l'on souhaite en plus) car le fait de conserver le noeud cible, permet de garder des chemins entre nos voisins à traves ce cible au cours de la marche, qui avant été perdu quand on supprimait le noeud.


Les résultats semblent très peu affecter par ce changement dans la méthode. Par exemple pour les résultat sur l'encephalopathy hepatique, on a une corrélation de 0.9999948 entre les valeurs de Scores obtenus entre les deux méthodes !



 ## Notes sur la nouvelle approche :
 
### Problématique :

Pour donner un petit exemple, j'ai étudié le comportement du prior avec un damping factor = 0.1 pour le composé M_CE5236. Globalement c'est un composé en bout de chaîne qui se trouve dans une région ou aucun de ces voisins est annotés dans la littérature. La problème c'est que du coup, son principal contributeur (87%) c'est l'acide formique qui est bieeeeeen plus loin dans le réseau (shortest.path = 6)
Sur la schéma du graph (Cf notes/exemple_MCE5236_M_for.png), les couleurs représente une échelle pour la taille du corpus: blanc = pas d'article et rouge = STAR
Les deux composés en mode en jaune sont en bas,  M_CE5236 et en haut dans le centre du réseau l'acide formique  

Ce que l'on montre ici, c'est que dans le cas de composé situés dans des régions sans littérature ou avec des voisins proches avec une poignée d'articles, les éléments qui peuvent contribuer le plus au prior que l'on va construire pour ce composé peuvent être des composé à forte littérature (des STARs) mais qui se trouve très éloigné du composé d'intérêt. Dans le cas de M_CE5236, l'acide formique est situé à 6 réactions du composé ! Et pourtant il représente quasiment la totalit de l'information du prior. il y a donc un problème. Dans ces cas là on doit être capable de déterminer que l'on a construit le prior avec des composés très éloignés du composé d'intérêt et donc peu pertinent, ou alors, on abandonne la construction du prior dans ce cas là et on utilise le prior par défaut.

La première solution est simple, il suffirait par exemple d'estimer la distance à laquelle se trouve les contributeur du composé d'intérêt. La première chose que l'on peut proposer est de :
1) Calculer les shortest-path entre tous les composés avec Dijkstra par exemple.
2) Estimé une moyenne, pondérée par les poids dans le prior, de la distance entre les contributeurs et le composé d'intérêt. Cette information permettrait de juger de la légitimité des contributeurs pour construire le prior.

Néanmoins, parmis tout nos contributeur, l'un d'entre eux peut être une STAR un peu loin qui va biaisé cette moyenne car il contribura de manière significative au prior. Pour autant d'autre composé dans le voisinnage avec certe moins de litterature sont pourtant pertinents pour construire ce prior.
L'idée donc de donné un indice pour estimé cela ne semble pas pratique ...

La deuxième solution que j'ai imaginé est que dans mon modèle de mélange je pourrais ajouté notre prior "fictif" composé de $\nu$ articles avec une proba $\mu$ de parlé de MeSH, mais cette fois-ci non pas pour faire du shrinkage sur les proba, mais pour qu'il représente une composante de base dans les mentions qui composent le mélange. Ainsi de base, j'aurai au moins un set de $\nu$ articles dans ceux qui contribues à mon prior. Alors, les autres composés du réseau peuvent contribuer à ce prior, si ils ont suffisament de litterature, ils écrasent ce prior et prennnent le relai, sinon il ne sont pas majoritaire et le prior de base reste la composante essentielle dans le prior. Le problème avec cette solution est qu'elle va, je pense, faire re-émerger un des premier biais de la méthode. Intialement j'utilisais les poids issus du PPR et on la méthode était très conservative car dès lors qu'un des contributeur n'était pas favorable à l'association, même s'il n'avait pas beaucoup d'articles, vue que son poids était uniquement déterminé en fonction de son PPR, ils pouvait avoir une bonne influence sur le prior. Ainsi il suffisait qu'un composé avec 10 articles n'est pas vue le MeSH, bien que 5 autres des voisins soient en faveur de la relation pour que celui-ci biaise le prior en apportant au moins x% de chances que l'on ne discutte pas de ce MeSH. Avec l'utilisation des mentions et la prise en compte de la taille des corpus dans le poids, on avait réglé ce problème.
Mais on pourrait le re-créer en ajoutant ce set d'article *imaginaires* comme composante dans le modèle de mélange. Par exemple, si 1000 articles arrivent pour construire mon prior depuis les autres composé, ce set d'article *imaginaires* représenterait quand même 10% de mon prior et vue qu'il dirait qu'il n'y a pas de relation, le problème est le même !

La solution qui semble pertinente est de restreindre les contributeur au voisinage à N réactions. C'est un peu la même idée que le damping factor du PPR où l'on souhaite représenté un voisinnage de plus en plus proches, mais le problème est qu'à partir d'une $\alpha > 0$, en fait la probabilité qu'une mention partant de n'importe quel noeud pour arrivée sur n'importe quel autre noeud, est > 0 Et donc finalement tout composé contribue au prior de tout les autres. Quand on est dans des régions bien couvertes en littérature c'est pas un problème car les voisins proches sont largement marjoritaire et la contribution de composé éloigné est infime et négligeable. Mais dans des régions sans littérature, comme avec M_CE5236, ces faibles quantité de mentions qui arrive jusqu'au composé sont tout de même majoritaire.
L'idée serait donc de restreindre les contributeur en fonction de leur distance au composé cible, par exemple on autorise seulement les composés distant à moins de 3 réactions du composé cible à contribuer au prior. Pour cela, on utiliserait par exemple la matrice des plus court chemin et on l'utiliserait pour filtrer au moment du calcule des poids.
Ainsi, si dans un rayon de 3 réactions, aucun composé n'est pas capable de nous approté de l'inforamtion, le prior est abandonné et on utilisera le prior par défaut !

Ce serait cool d'estimé un bon paramètre K comme ça en utilisant le graph que l'on à fait pour la proximité des STARs en fonction de la taille des marches ! Il faut regarder la fonction par contre je vois des trucs byzarre ... Il me dit que M_tststerone atteint M_CE5236 avec une marche de taille 4, alors que Dijktra trouve que le plus court chemin c'est 6 ... donc à revoir ...


Alors il y une question de choix entre considéré le plus court chemin ou des marches ... L'inconvénient d'utiliser strictement le plus court chemin est que dans le cas de chaîne c'est un peu dommage car on sait que les articles vont forcément ce propager dans ce sens là et c'est peut être pertinent d'utiliser un composé même à plus de trois réactions si on se trouve dans une chaine.

Après ça implique par contre de définir ce fitlre autrement avec un % peut être du nombre de marche qui atteigne le composé cible ... mais ça fait un peu redondant avec le PR et ça semble difficle de poser un threshold la dessus sans avoir un truc complètement arbitraire.

Donc peut être testé en premie cet histoire de distance max que peut avoir un contributeur ça reflète un peu ce que je voulais faire avec le damping factor du PPR, mais il semble que ce ne soit pas suffisant dans certains cas. Après il faut je pense garder le PPR, car les random walks ça permet quand même d'approter plus de mentions à un voisin avec lequel on a plus de connections par exemple donc c'est cool !!

=========================================

Solution retenue:  La solution retenue est de créer une matrice de containte afin de sélectionner a quel autre composé, un coposé peut il contribuer de manière légitime. Aussi, on empêche toujours un composé de contribuer à lui-même.
Donc, pour contruire cette matrice de contrainte, on va demandé à ce que la probabilité d'atteindre un noeud (hors du noeud cible) soit au moins supérieure à $\frac{1}{n -1}$ (n étant le nombre total de noeud, on fait -1 car on compte pas le noeud lui-même) c'est à dire à la probabilité de sélectionner ce noeud au hazard. De manière plus formalisé, on demande donc que :
$\frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$ soit que la probabilité d'atteindre $k$ depuis $i$ (sans considéré les marches retournant à $i$) est supérieure à la proba de choisir $k$ au hasard parmis tout les noeud du réseau.

Ceci équivaut donc aussi à $\frac{\pi_{k,i}}{(1 - \pi_{i,i})} (n - 1) > 1$ ce qui offre une interprétation un peu plus inuitutive. Imaginons que chaque composé dispose de $n - 1$ cartons d'invitation, donc 1 pour chaque autre noeud dans le réseau, c'est comme si on simulait la propagation de ces cartons suivant la marche définie et ensuite on ne récupère que les composé qui ont reçu au moins un carton d'invitation. Ainsi les composés qui peuvent influé $k$ sont les composé pour lesquels $k$ à re_u au moins un carton d'invitation.

En se positionnant sur le composé cible, on va donc chercher à déterminer l'ensemble des noeuds qui semblent légitime d'après cette contrainte pour contribuer à son prior.
On va appelé les composés qui respectent cette régle les **contributeurs** de $k$ (les noeuds pouvant influencer $k$) que l'on notera $IN_k$. 
Ainsi, $\forall i \in CTB_k: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$. Vue que l'n empêche les composés de contribuer à eux-même, le composé $k$ ne peut pas faire partie de son voisinnage d'influence donc il ne contribue toujours pas au prior !

Symétriquement, on peut définir le voisinnage d'influence, noté $IN_i$ de chaque composé $i$ comme étant le set de composé qu'il peut légitimement influé au vue des contraintes que l'on a posé !

 $\forall k \in IN_i, k \neq i: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$

Où on impose que $i \notin IN_i$ 


Comme on le faisait avant, on va donc re-estimer les probabilité pour qu'elles ne se concentre sur les composé que $i$ peuvent légitimement influé. On va donc dire que :

$\forall k \in IN_i: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$ On re-proportionne les probas en se basant uniquement sur le set que $i$ peut influé.

$\forall k \notin IN_i: P(I = k|C_i) = 0$ avec obligatoirement $i \notin IN_i$ 


C'est comme ça que don donc définies les probabilités $P(I = k|C_i)$ que l'on va ensuite utilisées pour calculer les poids dns notre modèle : $P(C_i|I = k)$






## Code save proba and weights matrix

# out1 = os.path.join(out_path, "proba_" + str(alpha) + ".csv")
# probabilities.SFT.to_csv(out1, index = False)

# out = os.path.join(out_path, "W_" + str(alpha) + ".csv")
# dd = pd.DataFrame(weights, columns=g.vs["label"])
# dd.to_csv(out, index = False)