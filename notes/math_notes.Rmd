---
title: "Math notes"
author: "Delmas Maxime"
date: "20/01/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(DT)
library(ggridges)
```

## La distribution Beta 

Pour la loi Beta, la densité de probabilité est définie lorsque $\alpha, \beta > 0$ et $x \in [0,1]$ par: $$f(x; \alpha, \beta) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$$

$B(\alpha, \beta)$ est la constante de normalisation pour que la probabilité totale soit 1, ainsi $B(\alpha, \beta) = \int_{0}^{1} u^{\alpha - 1} (1 - u)^{\beta - 1} du$. 

One note ainsi $X \sim Beta(\alpha, \beta)$

La loi beta est donc très pratique lorsqu'il s'agit de modéliser des probabilités puisqu'elle est définit entre $[0,1]$

## La binomiale 

La loi binomiale est notamment utilisé pour représenter la probabilité d'observer un nombre $k$ de succès sur une série de $n$ tentatives.
$X \sim Bin(n,p)$
$P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$

## Lien entre la beta et la loi binomiale

Comme on peut le constater, le numérateur de la loi Beta est similaire à ce que l'on peut retrouver dans la loi Binomiale: $x^{\alpha - 1}(1 - x)^{\beta - 1}$ ressemble à $p^k (1 - p)^{n - k}$.
Si on considère que $x$ dans la loi beta est l'équivelent de $p$ dans la loi binomiale, en considérant aussi que $\alpha - 1$ est le nombre de succès $k$ et $\beta - 1$ le nombre d'échec $n - k$, les deux tendent à représenter la probabilité d'observer $k$ succès sur $n$ tentatives. Bien sur les deux ne représentent pas exactement la même chose, mais l'inuition est là.

De même, dans le calcul de $P(X = k)$, c'est la partie $p^k (1 - p)^{n - k}$ qui est véritavblement essentielle, $\binom{n}{k}$ étant simplement une constante lorsqu'on observe une expérience.

La moyenne d'une variable suivant une loi beta est $E[X] = \frac{\alpha}{\alpha + \beta}$. Toujours en imaginant que $\alpha$ et $\beta$ représente respectivement le nombre de succès et d'échec, l'espérance d'une variable suivant une $Beta$ peut s'interpréter comme la moyenne de succès. Une autre manière de voir la fonction Beta est aussi en reparamétrant la fonction avec les paramètres $\mu$ et $\sigma$ tel que: $\mu = \frac{\alpha}{\alpha + \beta}$ et $\sigma = \frac{1}{\alpha + \beta}$. Ainsi, $\alpha$, le nombre de succès vaut $\frac{\mu}{\sigma}$ et $\beta$ le nombre d'échec vaut  $\frac{(1 - \mu)}{\sigma}$.

On peut retrouver dans la literature une autre manière, simplement $\sigma = (\alpha + \beta)$, et donc $\alpha = \mu \sigma$ et $\beta = (1 - \mu) \sigma$

L'idée avec cette re-paramétrisation c'est que $\mu$ représente la probabilié moyenne de succès (l'espérance moyenne de succès) et $\sigma$ est appelé le paramètre de dispersion, qui va nous permettre de gérer la variabilité, l'incertitude, que l'on a autour de cette moyenne. En constatant que $\sigma$ est directement fonction de $\alpha + \beta$, on voit que l'on exprime notre variabilité autour de cette moyenne en fonction du nombre d'expériences totales réalisées : nb.succès + nb.échecs. Plus le nombre d'expériences est élevé, plus la variabilité est faible. Petite précision pour ne pas confondre: $\sigma$ est notre paramètre de dispersion, mais dans la literature on pourra entendre parler de $\phi$ ou de $\rho$ que l'on appelle le paramètre de *sur-dispersion*. Car comme on peut le voir il représente l'augmentation de la variabilité du à l'utilisation de notre $Beta()$, mais attention c'est différent de $\sigma$. Sur la doc de Gamlss on peut voir que la variabilité de notre $Beta-binomiale$ vaut: $Var(Y) = n \mu (1 - \mu)[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}]$ en utilisant les $\mu$ et $\sigma$ paramètrisation. (Sur la doc il donne la sd, mais c'est simplement la racine de la variance). Or on a don cque $(\sigma + 1) = \frac{1 + \alpha + \beta}{\alpha + \beta}$ et donc : $\frac{(n - 1)\sigma}{(\sigma + 1)} = \frac{(n - 1)}{\alpha + \beta} \times \frac{\alpha + \beta}{1 + \alpha + \beta} = \frac{(n - 1)}{1 + \alpha + \beta}$ Ainsi: $[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}] = [1 + \frac{(n - 1)}{1 + \alpha + \beta}]$. C'est alors qu'on pose $\rho$ ou $\phi = \frac{1}{1 + \alpha + \beta}$, donnant ainsi: $[1 + \frac{(n - 1)\sigma}{(\sigma + 1)}] = [1 + (n - 1)\phi]$

Pour notre variabilité on a donc: $Var(Y) = n \mu (1 - \mu) [1 + (n - 1)\phi]$. la propriété très interessant par rapport à $\phi$ c'est donc que lorsque l'on fixe le paramètre de sur-dispersion $\phi = 0$, on a que $Var(Y) = n \mu (1 - \mu)$, ce qui est **exactement** la variabilité d'une binomiale classique! Ceci montre que sans sur-dispersion apporté par notre $Beta()$, notre modèle est simplement une binomiale ! Ce paramètre ajoute donc de la variance à ntore Beta !!! :)

Mais attention, ce que l'on fit avec GAMLSS c'est $\mu$ et $\sigma$, on ne fit **pas** le paramètre de sur-dispersion directement !! Par contre on voit très bien le lien entre $\sigma = \frac{1}{\alpha + \beta}$ et $\phi = \frac{1}{\alpha + \beta + 1}$, les deux sont tout de même largement corrélés quand $\sigma$ est grand, globalement $\phi$ est grand aussi. Donc les deux tendent à refléter la même chose, la dispersion, mais ne sont pas tout à fait égaux  :)

## Les estimateurs

### Estimateurs du Maximum de Vraisemblance (MLE)

La manière classique d'estimé notre paramètre $p$, est d'utilisé l'estimateur du maximum de vraisemblance (**MLE**), avec $p = \frac{k}{n}$

Proof: 
On a observé $k$ succès et $n$ échec, on cherche la valeur de $p$ qui maximise la vraisemblance. 

La fonction de vraisemblance de la loi binomiale étant $L(k; n, p) = \binom{n}{k} p^k (1 - p)^{n - k}$, on cherche donc $p$ tel qu'il maximise la probabilité d'observer $k$ succès parmis $n$ tirages:

$$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} L(k; n, p)$$
$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} \binom{n}{k} p^k (1 - p)^{n - k}$

On va étudier la log-vraisemblance qui sera plus pratique pour dérivé:

$\hat{p}_{MLE} = \underset{p}{\operatorname{argmax}} \ln{\binom{n}{k}} + \ln{p^k} + \ln{(1 - p)^{n - k}}$

Pour trouver le maximum, par rapport à $p$, il suffit de regarder lorsque que la dérivé s'annule par rapport à $p$:

$\frac{d \ln L(k; n, p)}{dp} = 0$

$k \frac{1}{p} - \frac{1}{(1 - p)} (n - k) = 0$

On multiplie tout par $p(1 - p)$ pour simplifier :

$k \frac{p(1 - p)}{p} - \frac{p(1 - p)}{(1 - p)} (n - k) = 0$

$k (1 - p) - p(n - k) = 0$

$k - pk - pn + pk = 0$

$k - pn = 0$

$p = \frac{k}{n}$

On voit retrouve bien l'estimateur MLE !

L'estimateur du maximum de vraisemblance est donc une valeur fixé, calculé en considérant le résultat observée de l'expérience. Ici, on considère les données comme étant un tirage aléatoire à partir de la population totale. L'incertitude sur cet estimateur peut être calculé à partir de l'erreur d'échantillonnage: Si je refaisait plein de fois l'expérience, qu'elle serait la variabilité que j'observerai sur mon paramètre $\hat{p}_{MLE}$ ?
Intuitivement, on peut voir que plus la taille de mon échantillon $n$ est grand, plus je m'attend à ce que mon paramètre estimé soit proche de la vraie valeur de se paramètre dans la population.


### Bayesian estimator

L'estimateur Bayesien est une autre façon de voir les choses. On va considérer que le paramètre que l'on cherche à estimer $\hat{\theta}$ est une variable aléatoire, et on cherche à étudier la distribution de ce paramètre au vue des données observées. On définit un prior sur la distribution de ce paramètre, c'est à dire une idée de sa distribution sans avoir vue les données. Ensuite, à partir des données, on va mettre à jour notre prior en utilisant la fonction de vraisemblance des données par rapport à notre paramètre. Ce que l'on appelle l'estimateur Bayesien de notre paramètre est alors simplement la moyenne de la distribution **à posteriori** de notre paramètre:

$\hat{\theta} = E[\theta|x] = \int \theta p(\theta|x) d\theta$
 
Ce qu'on cherche c'est donc notre distribution à posteriori $p(\theta|x)$. C'est là que rentre en compte le théorême de Bayes, où :

 $p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d \theta}$

Ici $p(\theta)$ est notre prior, il représente la distribution de $\theta$ a priori, sans connaissance des données. 

$p(x|\theta)$ représente alors la vraisemblance, la proabilité d'observer nos données $x$ sachant le paramètre $\theta$. Au dénominateur, on a l'intégrale sur toutes les valeurs de $\theta$ afin d'obtenir une densité de probabilité (on 'normalise').

Dans le cas de notre expérience binomiale, on connait notre fonction de vraissemblance, c'est: $L(k; n, p) = P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}$ c'est la probabilité d'observer $k$ succès sur $n$ tentatives avec un certain paramètre $p$

Une propriété importante est celle des prior conjugués: 
Dans la théorie bayésienne des probabilités, si la distribution postérieure $p(\theta|x)$ est de la même famille de distribution de probabilités que la distribution à priori $p(\theta)$, les distributions **a priori** et **postérieure** sont alors appelées **distributions conjuguées**, et la distribution **a priori** est appelée le **prior conjugué** pour la fonction de vraisemblance $p(x | \theta)$.

Dans le cas de la distribution binomiale, le prior conjugué est la loi Beta ! Ainsi en posant un prior suivant une distribution $Beta$ et en calculant la vraisemblance avec une binomiale, la distribution de porbabilité postérieure suit une loi $Beta$


Ainsi pour le paramètre $p$ de notre binomiale, on a : $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$, avec  :

$p(k|p = x, n) =  \binom{n}{k} x^k (1 - x)^{n - k}$

$p(p = x) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$

Schématiquement cela représente $p(\theta = x| data) = \frac{p(data | \theta = x) p(\theta = x)}{p(data)}$

Étant un prior conjugué, le résultat est : $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy} = \frac{x^{k + \alpha - 1}(1 - x)^{(n - k) + \beta - 1}}{B(k + \alpha, (n - k) + \beta)}$ 

Où la distribution beta étant un prior conjugué de la binomiale, on retrouve une distribtion a posteriori $Beta$ de paramètre $Beta(k + \alpha, (n - k) + \beta)$

On peut donc dire que: $p(p = x|k, n, \alpha, \beta) \sim Beta(k + \alpha, (n - k) + \beta)$

Voir https://en.wikipedia.org/wiki/Conjugate_prior pour les details

Connaissance notre distribution à posteriori et sachant que notre estimateur Bayesien est simplement l'espérance de cette distribution, dans le cas d'une binomiale, notre estimateur est donc simpelment l'espérance de la distribution Beta à posteriori :
$$\hat{p}_{Bayes} = E[p|k,n] = \frac{\alpha + k}{\alpha + \beta + n}$$

En effet $\frac{\alpha + k}{\alpha + \beta + n}$ étant l'espérance d'une distribution $Beta(k + \alpha, (n - k) + \beta)$$

### Lien avec l'estimateur MLE

Comme on peut le voir, $\hat{p}_{Bayes} = \frac{\alpha + k}{\alpha + \beta + n}$ et  $\hat{p}_{MLE} = \frac{k}{n}$, on peut constater qu'en réalité ces deux estimateur sont très proches ! En effet, lorsque le nombre de tentative $n$ va augmenter, les paramètre du prior $\alpha$ et $\beta$ n'auront plus trop d'influence sur l'espérance calculée. Ex: si $\alpha = \beta = 1$ et $k = 100$ et $n = 1000$: 

$\frac{1 + 100}{1 + 1 + 1000} \approx \frac{100}{1000}$

Ainsi nos deux estimateur tendent vers la même chose !!

Une autre propriété qui est très intéressante, est lorsque l'on utilise une loi uniforme comme prior. Pour définir une loi Uniforme avec une Beta, c'est très simple c'est simplement $Beta(1,1)$. Comme on va le voir, en utilisant un prior uniforme, le **MAP** (Maximum a posteriori probability), soit la valeur de notre paramètre la plus probable à posteriori, correspond à l'estimateur MLE ! Attention, on parle du MAP, pas de la moyenne de la distribution car l'estimateur bayesien est différent du MLE. Néanmoins, dans notre distribution à posteriori, la valeur la plus probable est l'estimateur du MLE:

$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(\theta|x)$, soit le $\theta$ qui maximise les proba à posteriori.

$$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(\theta|x) = \underset{\theta}{\operatorname{argmax}} \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d \theta} = \underset{\theta}{\operatorname{argmax}} p(x|\theta)p(\theta)$$

En effet, le dénominateur dans l'équation du posteriori est seulement un facteur de normalisation, mais il n'influe pas sur le maximum.
Deplus, la distribution $Beta(1,1)$ étant complètement uniforme, le facteur $p(\theta)$ est également complètement inutile car il vaut toujours $1$. 

```{r bintest2, eval=FALSE}
p = seq(0,1, length=100)
plot(p, dbeta(p, 1, 1), ylab="density", type ="l", col=4)
```

Ainsi on se retrouve avec 
$$\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} p(x|\theta)$$

Or $p(x|\theta)$ c'est ma fonction de vraisemblance, donc dans le cas de la loi binomiale, on a que $\hat{\theta}_{MAP} = \underset{\theta}{\operatorname{argmax}} L(k; n, p)$ ce qui est exactement la définition de mon estimateur du maximum de vraisemblance **MLE**

## Improper priors

Comme on le voit en utilisant une distribution Uniforme (ex $Beta(1,1)$), le MAP de la distribution à posteriori correspond au MLE. Mais il y a d'autres propriété intéressantes. 
On appelle improper prior une distribution à priori qui n'intègre pas à 1. Dans notre exemple, tout va bien $Beta(1,1)$ intègre bien à 1, c'est d'ailleurs simplement un carré de 1 de coté si on simplifie. Mais d'autres distributions comme beta(0,0), ou, plus simplement dans le cas d'une loi normale, si on imagine un prior qui définis une probabilité équivalente pour toutes les valeurs de $\mu$, ceci n'intègre pas à 1 ...

Mais normalement ce n'est pas vraiment un problème. Comme on peut le voir dans la règle de Bayes, en fait, la likehood ($p(x|\theta)$) est pondéré par le prior ($p(\theta)$), c'est un poids. La fonction de likehood, quant à elle, n'est généralement pas positive pour dans tout l'espace, elle atteint un maximum pour une certaine valeur et tend vers 0 lors que l'on s'éloigne de cette valeur. Ainsi en réalité même si on multiplie par le prior, l'intégrale au dénominateur est finie car la likehood tendra vers 0 pour des valeurs éloignés de $\theta$. Ainsi, même si le prior n'est pas une vrai distribution, on va quand même faire une pondération entre de numérateur à certain $\theta$ et le dénominateur. Ainsi même en utilisant un prior improper, la distribution à posteriori peut être une bonne distribution.

### Using Beta(1,1)

Lorsque l'on utilise la $Beta(1,1)$, en plus que le MAP soit équivalent au MLE, il y a une autre propriété: Notre distribution a posteriori est proportionnelle à la vraisemblance.

En effet, si on a $p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$, avec notre prior $p(p = x)$ issue d'une $Beta(1,1)$, c 'est donc que pour tout $x \in [0,1]$, $p(x) = 1$. Ainsi ce facteur est complètement annecdotique dans notre évaluation de la propriété à posteriori, qui équivaut donc à : 

$$p(p = x|k, n) = \frac{p(k|p = x, n)}{\int p(k|p = y, n) dy}$$ 

On voit bien que notre distribtion à posteriori est donc complètelement proportionnelle à la vraisemblance $p(k|p = x, n)$. Lorsque l'on utilise un prior non-informatif ($Beta(1,1)$), la distribution à posteriori est uniquement influé par les données, donc la vraisemblance, car notre prior ne donne aucune indication particulière ! Ainsi en choisissant un prior non-informatif on utilise uniquement l'information des données et notre distribution à postériori suit la distribution de notre vraisemblance !



## En conclusion: 

- L'estimateur Bayesien de mon paramètre $p$ est $\hat{p}_{Bayes} = \frac{\alpha + k}{\alpha + \beta + n}$. Il s'agit de la moyenne de la distribution à posteriori de $p$, soit $E[p|k,n]$ obtenu en utilisant le prior conjugué $Beta(\alpha, \beta)$. Où d'après la règle de Bayes: 
$$p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$$

$p(k|p = x, n)$ étant ma fonction de vraisemblance (Likehood), la probabilité d'observée mon nombre de succès $k$ sachant que $p=x$ ($n$ étant fixé). Cela suit donc une binomiale et ainsi: $p(k|p = x, n) =\binom{n}{k} x^k (1 - x)^{n - k}$

$p(p = x)$ est mon prior, c'est à dire la distribution que j'ai de $p$ sans avoir obervé de données. Il suit une distribution $Beta(\alpha, \beta)$ et donc $p(p = x) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}$

- L'estimateur du MLE et l'estimateur Bayesienne converge vers la même valeur à mesure que le nombre de tentative augmente -> Quand la taille des échantillons considéré augmente, on est naturellement plus précis sur le paramètre estimé et donc les deux tendent vers la même valeur.

- En utilisant un prior non-informatif: La valeur la plus probable dans la distribution a posteriori (MAP) est équivalent à l'estimateur du MLE ! ET la distribution à posteriori est directement proportionnelle à la vraisemblance ! Ce sont nos données qui donne la distribution à posteriori sans aucune influence du prior.



## Exemple des propriétés:

On va reprendre l'idée du premier exemple, mais cette fois on va se placer dans une situation réelle. On observe nos données et on va chercher à estimer $p$

- On observer $k = 20$ succès parmis $n = 100$ tentatives.
Trouver $p$ en ulisant l'estimateur du maximum de vraisemblance, consiste à trouver $p$ tel qu'il maximise $P(X = 20) = \binom{100}{20} p^{20} (1 - p)^{100 - 20}$

L'estimateur du maximum de vraisemblance est donc $\hat{p}_{MLE} = \frac{k}{n} = \frac{20}{100} = 0.2$

En effet, si on cherche à ploter comment varie la vraisemblance ($P(X = 20)$) en fonction de p, voilà ce que l'on obtient :

```{r bintest3}
p.test <- seq(0,1,0.0001) 
v.vrais <- dbinom(x = 20, size = 100, prob = p.test)
max <- p.test[which(v.vrais == max(v.vrais))]
surf.inf <- sum(v.vrais[0:(which(v.vrais == max(v.vrais)) - 1)])
surf.sup <- sum(v.vrais[(which(v.vrais == max(v.vrais)) + 1):length(v.vrais)])
ggplot(data.frame(proba = p.test, Likehood = v.vrais), aes(x = proba, y = Likehood)) +
  geom_line() + 
  theme_classic() + 
  geom_vline(xintercept = max, 
                color = "red", size=1) +
  geom_point(data=data.frame(proba = max, Likehood = max(v.vrais)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  geom_text(aes(x = max + 0.1, label=paste("MLE = ", max), y=0.11), colour="red")
```
Sur cette figure, on cherche le point avec la vraisemblance la plus élevé.Comme prédit par le MLE, on trouve $p = 0.2$, notre estimateur MLE désigne bien le *maximum* de vraisemblance

Ce que l'on visualise ici c'est la vraisemblance par rapport au paramètre $p$. Attention, ce c'est pas une distribution de probabilité ! c'est simplement la vraisemblance ..

Quelques petites remarques importante: On voit que la distribution de la vraisemblance atteint un maximum à $p = 0.2$, mais sa distribution n'est pas symétrique autour de cette valeur. Si on essaye d'estimer l'aire (en sommant les valeurs à gauche ou à droite de la ligne) à gauche du MLE, on à ~ `round(surf.inf, 2) ` et à droite on a `round(surf.sup, 2) `. 

Si on utilise un estimateur Bayesien avec un prior uniforme, $beta(1,1)$, alors notre distribution à postériori srea proportionnelle à la vraisemblance, elle aura la même forme, la moyenne que l'on va estimer sera intuitivement légère supérieure au MLE observée du fait de la non-symétrie dans l'exemple.

Pour pouvoir visualiser correctement notre esimtateur Bayesien, on va transformé le graph ci-dessus en un histograme. Pour se faire, pour chaque valeur de p, on va généré 1000 tirages aléatoires suivant une binomiale avec ce paramètre $p$ et $n = 100$. Ensuite on va construire l'histogramme de la distribution des probabilités $p$ lorsque le $k$ obtenu égal 20 pour visualisé leur distribution. On replot également une ligne vertical à 0.2

```{r bintest4}
v.p <- seq(0,1,0.001)
data <- data.frame(X = c(), p = c())
for(p in v.p){
  sampling <- rbinom(n = 1000, size = 100, prob = p)
  data <- rbind(data, data.frame(X = sampling, p = rep(p, 1000)))
}
data <- data %>% filter(X == 20)
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
    geom_vline(xintercept = 0.2, 
                color = "red", size=1)
```


On voit que cela suit la même allure que la distribution des vraisemblances construite précedement.

Connaissant les propriétés évoqués précedemment, on sait qu'en utilisant un prior non-informatif $B(1,1)$, notre distribution à posteriori sera proportionnelle à la vraisemblance, et, son MAP sera équivalent au MLE. 
Sachant que $k = 20$ et $n = 80$, en utilisant un prior $B(1,1)$, on sait que notre distribution de $p$ à posteriori:

$p(p = x|k, n, \alpha, \beta) \sim Beta(k + \alpha, (n - k) + \beta)$

et donc

$p(p = x|20, 100, 1, 1) \sim Beta(20 + 1, (100 - 20) + 1)$

$$p(p = x|data) \sim Beta(21, 81)$$
Donc on refait le même plot en superposant :

```{r bintest5}
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
  stat_function(fun = dbeta, args = list(shape1 = 21, shape2 = 81), aes(colour = "Beta(21,81)"), size=1.5, color = "red")
```

Comme on peut le voir notre distribution $Beta(21,81)$ fit parfaitement la distribution de nos probabilité $p$ lorsque $k = 20$, correspondant à la distribution de la vraisemblance par rapport à $p$. 

Notre distribution à posteriori est bien proportionnelle à la vraisemblance, elle a la même allure. Elle est en quelque sorte sa version normalisée, nous donnant une distribution de probabilité.

Si on cherche le MAP associé, que l'on peut trouver en calculant le mode associé à cette distribution $Beta$, on obtient que:

$MAP = \frac{\alpha - 1}{\alpha + \beta - 2} = \frac{21 - 1}{21 + 81 - 2} = \frac{20}{100} = 0.2$

Ce qui correpond effectivement à notre MLE !

On peut donc calculer la valeur de notre estimateur bayesien: $\hat{p}_{Bayes} = E[p|k, n, \alpha, \beta)]$, soit l'espérance de notre distribution à posteriori

On calcule donc l'espérance de notre distribution $Beta(21,81)$

$\hat{p}_{Bayes} = E[p|k, n, \alpha, \beta)] = \frac{21}{21 + 81} \approx 0.2059$


Maintenant on va tout ploter en même temps :

```{r bintest6}
map = 0.2
ggplot(data, aes(x = p)) + 
  theme_classic() +
  theme(text = element_text(size = 35)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,0.5) +
  geom_point(data=data.frame(proba = map, Likehood = dbeta(map,21,81)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  annotate(geom = "text", x= map + 0.1, y = dbeta(map,21,81) + 0.1, label="MAP <-> MLE ") +
  stat_function(fun = dbeta, args = list(shape1 = 21, shape2 = 81), aes(color = "Beta(21,81)"), size = 1.5, color = "red") +
  geom_vline(aes(xintercept = map, 
                color = "MLE"), size = 2) + 
  geom_vline(aes(xintercept = (21/(21+81)), 
                color = "BAYES"), size = 2) +
  scale_color_manual(name = "Estimators", values = c(MLE = "blue", BAYES = "green", "Beta(21,81)" = "blue"))

```


Comme attendu, l'estimateur Bayesien est légèrement supérieur à l'estimateur MLE. Cela est notamment due au fait qu'avec l'estimateur Bayesien on estime l'espérance de la distribution, où comme on l'a vue, celle ci n'est pas symétrique. L'intégrale est plus grande à droite du MAP qu'a gauche, ainsi, on voit que la moyenne estimé va être légère supérieure au MAP avec l'estimateur bayesien. Néanmoins c'est un bon estimateur de la moyenne de la distribution observée, puisque effectivement si on cherche à calculer la moyenne derrière cet histogramme on obtient `r mean(data$p)` ce qui est proche du $\hat{p}_{Bayes} \approx 0.2059$


Aussi, on a évoqué que cette différence disparaissait à mesure que le nombre de tentatives observées augmente.
Si on refait le même calcul est prenant en compte dix fois plus d'observations, voilà ce que l'on obtient:

```{r bintest7}
v.p <- seq(0,1,0.001)
data <- data.frame(X = c(), p = c())
for(p in v.p){
  sampling <- rbinom(n = 1000, size = 1000, prob = p)
  data <- rbind(data, data.frame(X = sampling, p = rep(p, 1000)))
}
data <- data %>% filter(X == 200)
map = 0.2
ggplot(data, aes(x = p)) + 
  theme_classic() +
  geom_histogram(aes(y = ..density..), binwidth = 0.005, color="black", fill="white") +
  xlim(0,1) +
  geom_point(data=data.frame(proba = map, Likehood = dbeta(map,201,801)), 
             aes(x=proba,y=Likehood), 
             color='red',
             size=3) +
  annotate(geom = "text", x= map + 0.1, y = dbeta(map,201,801) + 0.1, label="MAP <-> MLE ") +
  stat_function(fun = dbeta, args = list(shape1 = 201, shape2 = 801), aes(color = "Beta(201,801)"), size = 1) +
  geom_vline(aes(xintercept = map, 
                color = "MLE"), size = 0.5) + 
  geom_vline(aes(xintercept = (201/(201+801)), 
                color = "BAYES"), size = 0.5) +
  scale_color_manual(name = "Estimators", values = c(MLE = "red", BAYES = "green", "Beta(201,801)" = "blue"))

```

On observe $k = 200$ succès pour $n = 1000$ tentatives

L'estimateur MLE est toujours égal à 0.2: $\hat{p}_{Bayes} = 0.2$

On utilise toujours un prior non-informatif $Beta(1,1)$, mais cette fois notre distribution posterireure suit donc une $Beta(201, 801)$

Le MAP $\frac{201 - 1}{201 + 801 - 2} = \frac{200}{1000} = 0.2 = \hat{p}_{Bayes}$ est toujours équivalent au MLE

Mon estimateur bayesien vaut désormais: $\frac{201}{201 + 801} \approx 0.2006$ ce qui se rapporche beaucoup donc du MLE. Comme vue précédemment, à mesure que $k$ et $n$ sont grand, notre prior définit par notre $Beta(\alpha, \beta)$ devient de plus en plus négligeable, on beaucoup croire les données !



## Les modèles de mélanges :

La fonction de densité de proababilité d'un modèle de mélange peut être définis tel que: $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$ avec $\sum_i k_i = 1$

-  Les poids $k_i$ représente la proportion de chaque distribution dans la distribution totale

Dans un modèle de mélange, c'est comme si chaque $x$ avec une probabilité $k_i$ d'être tiré depuis la ième distribution $f_i$, et dans chaque distribution $f_i$, la proabilité d'observer $x$ est donnée par $f(x, \theta_i)$


Si on étudie un modèle de mélange de distibution Beta, on a donc $$f(x; \alpha_1, \beta_1, \alpha_2, \beta_2, ..., \alpha_n, \beta_n = \sum_{i}^{n} k_i f(x; \alpha_i, \beta_i)$$

Calculer les probabilité à posteriori sur un modèle de mélange Beta est assez similaire au cas classique. C'est  assez rapide à calculer notamment grâce au fait que la loi Beta est le prior conjugué de la binomiale !

Dans le cas classique on utilisait la règle de Bayes avec :

$p(p = x|k, n) = \frac{p(k|p = x, n) p(p = x)}{\int p(k|p = y, n) p(p = y) dy}$

$n$ étant fixé, on pourrait même le retiré du "sachant que"

Maintenant on va utiliser une écriture plus rigoureuse et définir les $p(...)$ avec les fonction de densité $f$, plus correcte pour écrire des modèles de mélange. On reformule donc notre règle de bayes en : $f_{post}(p|k, n) = \frac{f_{bin}(k|p, n) f_{prior}(p)}{\int f_{bin}(k|p, n) f_{prior}(p) dp}$

$f_{bin}(k|p, n)$ étant notre fonctione likehood

Ainsi, la seule différence par rapport à ce qui a été présenté précédemment, est que $f_{prior}(p)$ est maintenant une distribution formée d'un mélange de $Beta$.

$f_{prior}(p;  \alpha_1, \beta_1, \alpha_2, \beta_2, ..., \alpha_n, \beta_n) = \sum_{i}^{n} k_i f(p; \alpha_i, \beta_i)$

On a donc que : 

$$f_{post}(p|k, n) = \frac{f_{bin}(k|p, n) f_{prior}(p)}{\int_{0}^{1} f_{bin}(k|p, n) f_{prior}(p) dp}$$
$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\int_{0}^{1} \sum_{i}^{n} k_i  f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp}$$
On va noter $C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$

On va utiliser $C_i$ pour simplifier l'expression. Au dénominateur, on remarque que l'on fait l'intégrale d'une somme. Grâce à la *sum rule*, on sait que l'intégrale d'une somme, c'est la somme des intégrales, donc va va pouvoir simplifier en utilisant $C_j$, le $k_j$ pouvant sortir de l'expression car c'est une constante. Pour le dénominateur on a donc que $\int_{0}^{1} \sum_{i}^{n} k_i  f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)(p) dp  = \sum_{i}^{n} k _i \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp = \sum_{i}^{n} k _i C_i$

$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\sum_{i}^{n} k _i C_i}$$
On voit qu'au numérator on a une somme de $f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)$ pondérés par les $k_i$. Or c'est exactement le numérator des probabilités à postériori indépendament pour chaque $i$. Ainsi, si on divise par $C_i$, on va obtenir $\frac{f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{\int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp}$, ce qui est notre probabilité à posteriori pour la compodante $i$ ! Vue que la $Beta$ est un prior conjugué de la binomiale, on sait que cette distribution est très facile a calculé, on va donc faire apparaitre à nouveau des $C_i$ pour pouvoir simpliquer notre expression avec les probabilité à posteriori de chaque composante $i$ :
$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} (k_i f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) C_i)/C_i}{\sum_{i}^{n} k _i C_i}$$

On peut donc simplifier en :

$$f_{post}(p|k, n) = \frac{\sum_{i}^{n} k_i C_i f_i^{(0)}(p|k, n))}{\sum_{i}^{n} k _i C_i}$$
On note ainsi $f_{i}^{(0)}(p|k, n)$ la distribution à posteriori de la ième composante, afin de la distinguer de la distribution à psoteriori globale du moldèle de mélange.


Maintenant on va à nouveau simplifier l'écriture de cette expression afin de retrouver une expression qui ressemble à un modèle de mélange. On va poster $W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i}$ et on reformule :

$$f_{post}(p|k, n) = \sum_{i}^{n} W_i f_i^{(0)}(p|k, n)$$
On retrouve ainsi une expression dans laquelle la distribution à posteriori de notre modèle de mélange est une somme pondéré par des poids $W_i$ des distribution à posteriori de chacune des $i$ distribution formant ce mélange. Avec :

$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i}$, 

$f_i^{(0)}(p|k, n) = \frac{f_{bin}(k|p, n) f(p; \alpha_i, \beta_i)}{C_i}$, 

$C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$


Alors, $f_i^{(0)}(p|k, n)$ étant simplement la distribution à posteriori pour la composante $i$, on sait que :

$$f_i^{(0)}(p|k, n) = Beta(\alpha_i + k, \beta_i + (n - k))$$ 


Ne reste plus qu'a déterminer $W_i$ !

Pour simplifier $W_i$, le mieux est de commencer par simplifier $C_i$

$C_i = \int_{0}^{1} f_{bin}(k|p, n) f(p; \alpha_i, \beta_i) dp$

On peut aussi remarque que $C_i$ définit une distribution Beta binomiale ! $C_i = f_{BetaBin}(k; \alpha_i, \beta_i, n)$

$C_i = \int_{0}^{1}  \binom{n}{k} p^k (1 - p)^{n - k} \times \frac{p^{\alpha_i - 1}(1 - p)^{\beta_i - 1}}{B(\alpha_i, \beta_i)} dp$

$\iff C_i = \int_{0}^{1}  \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)}  p^{k + \alpha_i - 1} (1 - p)^{n - k + \beta_i - 1}dp$

On peut aussi sortir les constantes de l'intégrale 

$C_i = \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)} \int_{0}^{1} p^{k + \alpha_i - 1} (1 - p)^{n - k + \beta_i - 1}dp$

La fonction bêta $B(x,y) = \int_{0}^{1} p^{x - 1} (1 - p)^{y - 1} dp$, on a donc :

$C_i = \binom{n}{k}  \frac{1}{B(\alpha_i, \beta_i)} \times  B(k + \alpha_i, n - k + \beta_i)$

$\iff  \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}$

Alors pour savoir comment calculer ça, il faut retourner à l'équivalence numérique de la fonction bêta, avec : 

$B(x,y) = \frac{(x + y)}{xy}\frac{x! y!}{(x + y)!}$, alors :

$C_i = \binom{n}{k} \times \frac{k + \alpha_i + n - k + \beta_i}{(k + \alpha_i)(n - k + \beta_i)} \frac{(k + \alpha_i)! (n - k + \beta_i)!}{(k + \alpha_i + n - k + \beta_i)!} \times \frac{\alpha_i \beta_i}{(\alpha_i + \beta_i)}\frac{(\alpha_i + \beta_i)!}{\alpha_i ! \beta_i!}$

Ce qu'il faut voir ici c'est que les factorielle au numérateur et au dénominateur dans les deux partie de l'expression vont se simplifier en utilisant les autres facteurs. Par exemple la partie $\frac{k + \alpha_i + n - k + \beta_i}{1} \frac{1}{(k + \alpha_i + n - k + \beta_i)!}$ va se simplifier car le dénominateur de la partie de gauche représente le premier élément de la factorielle, ce qui va donc donner la factorielle à une valeur en dessous, soit $\frac{k + \alpha_i + n - k + \beta_i}{1} \frac{1}{(k + \alpha_i + n - k + \beta_i)!} = (k + \alpha_i + n - k + \beta_i - 1)!$

On a donc finalement : 
$C_i = \binom{n}{k} \times \frac{k + \alpha_i + n - k + \beta_i}{(k + \alpha_i)(n - k + \beta_i)} \frac{(k + \alpha_i)! (n - k + \beta_i)!}{(k + \alpha_i + n - k + \beta_i)!} \times \frac{\alpha_i \beta_i}{(\alpha_i + \beta_i)} \frac{(\alpha_i + \beta_i)!}{\alpha_i ! \beta_i !} = \binom{n}{k} \frac{(k + \alpha_i - 1)! (n - k + \beta_i - 1)!}{(k + \alpha_i + n - k + \beta_i - 1)!} \times \frac{(\alpha_i + \beta_i - 1)!}{(\alpha_i - 1)! (\beta_i - 1)!}$

$$C_i = \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)} = \binom{n}{k} \frac{(k + \alpha_i - 1)! (n - k + \beta_i - 1)!}{(k + \alpha_i + n - k + \beta_i - 1)!} \times \frac{(\alpha_i + \beta_i - 1)!}{(\alpha_i - 1)! (\beta_i - 1)!}$$


L'expression $C_i = \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}$ est plus concise est facile à écrire. A voir si on peut le calculer directement avec des fonctions prédéfinis, sinon on connait donc la formule pour le déterminer à la main.

On peut donc donner une expression générale de $W_i$: 

$$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i} = \frac{k_i \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\sum_{i}^{n} k _i \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}} = \frac{\binom{n}{k} k_i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\binom{n}{k}  \sum_{i}^{n} k _i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}} = \frac{k_i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}{\sum_{i}^{n} k _i \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}}$$

En résumé :

$$W_i = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$
    
$$f_i^{(0)}(p|k, n) =  Beta(\alpha_i + k, \beta_i + (n - k))$$

$$f_{post}(p|k, n) = \sum_{i}^{n} W_i f_i^{(0)}(p|k, n)$$

**On peut remarquer que c'est exactement le calcul des poids à posteriori correspond exactement à l'étape E expectation de l'algo EM !** 

On peut donc trouver une interprétation intéressante aux poids $W_i$. Comme on l'a vue, $W_i = \frac{k_i C_i}{\sum_{i}^{n} k_i C_i}$ où $C_i =  \binom{n}{k}  \frac{B(k + \alpha_i, n - k + \beta_i)}{B(\alpha_i, \beta_i)}$ ce qui représente donc la probabilité d'observée $k$ succès parmis $n$ échec lorsque la probabilité $p$ suit une distribution une distribution $Beta(\alpha_i, \beta_i)$. Cette probabilité suit donc une distribution Beta-binomiale. On peut interpréter $W_i$ comme étant le rapport entre la probabilité d'obvserver notre observation ($k$ et $n$) dans la composante $i$, pondérée par son poids d'origine $k_i$, par rapport à la moyenne pondéré des probabilités d'observer notre observation sur toutes les composantes ! En effet, $\sum_{i}^{n} k_i C_i = \frac{\sum_{i}^{n} k _i C_i}{\sum_{i}^{n} k_i}$ car $\sum_{i}^{n} k_i = 1$. C'est pas pratique à interpréter comme ça car au numérateur on conserve le poids $k_i$ qui "brouille" un petit peu les observations. En revanche, si on cherche à la comparer à notre poids d'origine $k_i$ en calcul le rapport $\frac{W_i}{k_i}$, là on va pouvoir le supprimer : $\frac{W_i}{k_i} = \frac{\frac{k_i C_i}{\sum_{i}^{n} k_i C_i}}{k_i} = \frac{k_i C_i}{k_i \sum_{i}^{n} k_i C_i} = \frac{C_i}{\sum_{i}^{n} k_i C_i}$, où $\sum_{i}^{n} k_i C_i$ est toujours considéré comme la moyenne pondérée des probabilités de notre observation sur les différentes composantes.

C'est donc une interprétation intéressante avec $W_i$: en calculant le rapport $\frac{W_i}{k_i}$ celui-ci est équivalent à regarder le rapport entre la probabilité de notre observation sur la composante $i$ par rapport à la moyenne pondérée sur l'ensemble des composantes. Ainsi, plus $W_i$ est supérieur à $k_i$, plus cela nous indique que l'obvservation faite sur le composé cible est concordante avec ce que l'on observe sur la composante $i$: "Est-ce que cette composante sort du lot à posteriori ?" 

De manière plus générale, simplement regarder les composés avec plus grand poids $W_i$ peut nous informer des composantes les plus fortement liées à notre observations dans le modèle. Le fait est que si elles ne sont pas bien différente des poids initiaux, cela peut signifier que tous les autres composés dans le mélange ont ~ le même comportement et que les $C_i$ n'ont pas vraiment joués dans le calcul des poids.  

On pourrait imaginer imaginer plein de mesure pour quantifier tout ça, comme calculer la chute d'entropie sur la distribution des poids avant et après pour comparer les 2 distributions, mais je pense que cela compliquerait les choses. Le mieux est peut être simplement de ressortir les poids à posteriori en plus des poids initiaux pour pouoir les comparer visuellement, en considérant que :

 - Si les composantes ont un comportement similaires, comme par exemple toutes ont une très très faible d'avoir une mention qui discutte du MeSH (e.g ~ 0/10000) ou bien au contraire elles ont toutes une probabilité similaire d'observer le MeSH (e.g ~ 50/10000), alors les probabilités de notre observations sous chacune des composantes (les $C_i$) n'ont que très peu d'influence sur le calcul du poids final car toutes sont similaires et c'est surtout les poids initiaux $k_i$ qui participe.
 
 - En revanche si certaines des composantes ne discutte pas du tout du MeSH et d'autre oui, alors là c'est vraiment  les probabilités de notre observations sous chacune des composantes qui vont beaucoup influé le calcul des poids, en faisant ressortir les composantes qui sont les plus vraisemblable par rapport à nos observations, toujours pondérée par les $k_i$ initiaux, même si ici leur influence peut être beaucoup plus faible. Ex du Manose Mannose-1-p (M_man1p) et Metabolism, Inborn Errors et (D008661)


Pour les propriétés associés aux modèles de mélange, tel que l'espérance et la variance, se référer à https://en.wikipedia.org/wiki/Mixture_distribution

Par exemple dans un modèle de mélange de la forme $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$

$E[x] = \sum_{i}^{n} k_i \mu_i$, avec $\mu_i$ l'espérance de la ième composante $f(x; \theta_i)$ On fait donc ici simplement la moyenne des moyennes ! Dans le cas d'une distribution $Beta(\alpha, \beta)$, l'espérance étant simplement $\frac{\alpha}{\alpha + \beta}$, l'espérance d'un modèle de mélange est très rapide à calculer.



On prend comme exemple un modèle de mélange initial : $f(p) = 0.5 f(p; 2,4) + 0.5 f(p; 4,2)$ qui constitue notre distribution à priori sur notre paramètre $p$

Pour notre distribution on a donc une espérance de $p$, $E[p]_{prior} = 0.5\frac{2}{2+4} + 0.5 \frac{4}{4 + 2} = 0.5$

Là dessus on réalise une observation, on observe  $k = 8$ succès sur $n = 10$ tentatives

On plot cette distribution du prior:

```{r}
p <- seq(0, 1, 0.01)
f <- 0.5 * dbeta(p, 2, 4) + 0.5 * dbeta(p, 4, 2)

plot(dbeta(p, 2, 4), type = 'l', col = 'blue')
lines(dbeta(p, 4, 2), col = 'green')
lines(f, col = 'red')
legend("topleft", legend=c("Beta(2,4)", "Beta(4,2)", "Mix"),
       col=c("blue", "green", "red"), lty = 1:2, cex=0.8)
```

On va donc calculer les distrution à posteriori :

$$W_i = \frac{k_i C_i}{\sum_{i}^{n} k _i C_i} = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$

Donc 

$W_1  = \frac{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4)}{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4) + 0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}$

$W_2 \frac{0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}{0.5\times B(8 + 2, 10 - 8 + 4)/Beta(2,4) + 0.5\times B(8 + 4, 10 - 8 + 2)/Beta(4,2)}$

$f_1^{(0)}(p|k, n) =  Beta(2 + 8, 4 + (10 - 8))$

$f_2^{(0)}(p|k, n) =  Beta(4 + 8, 2 + (10 - 8))$


On calcule :

```{r, echo = TRUE}
# On commence par calculer C1 et C2 se sera plus pratique :

C.1 <- beta(8 + 2, 10 - 8 + 4)/beta(2,4)
C.2 <- beta(8 + 4, 10 - 8 + 2)/beta(4,2)

W.1 <- 0.5 * C.1/(0.5 * C.1 + 0.5 * C.2)
W.2 <- 0.5 * C.2/(0.5 * C.1 + 0.5 * C.2)

p <- seq(0, 1, 0.01)
f.1 <- dbeta(p, 2 + 8, 4 + (10 - 8))
f.2 <- dbeta(p, 4 + 8, 2 + (10 - 8))
```

On a W.1 = `r W.1` et W.2 = `r W.2`

On plot :
```{r}

f.post <- W.1 * f.1 + W.2 * f.2

plot(f.post, type = 'l', col = 'purple')
lines(f, col = 'red')

legend("topleft", legend=c("Mix.prior", "Mix.posterior"),
       col=c("red", "purple"), lty = 1:2, cex=0.8)
```




## Précisions 
Petites remarques sur le calcul des $W_i$: 
On sait que $$W_i = \frac{k_i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}{\sum_{i}^{n} k _i B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)}$$

Or, pour de grande valeur de $\alpha_i$ et $\beta_i$, ce qui est souvent le cas avec des composés, $B(\alpha_i, \beta_i)$ tend facilement vers 0. En effet la valeur devient tellement faible, que l'on ne peut pas la calculer précisément ...
Cela entraine une division par 0 et les calculs sont faussés. 
On propose ainsi de passer par le log pour pouvoir calculer complètement $W_i$. En effet, même si $C_i$ est donc difficile à calculer, on peut calculer $log(C_i)$ qui lui sera plus facilement calculable. Il suffit d'utiliser une fonction $logBeta(a,b)$. La fonction $Beta$ classique impliquant des factorielles et des puissances, en calculant le log, cela facilite grandement les calculs ! 
Aussi, on sait que $W_i$ sera entre 0 et 1 et sera une propotion donc il sera relativement facile de retrouver sa valeur, et c'est donc à cette étape là qu'il faut repasser à l'échelle classique avec l'exponetiellee

Ainsi on va dire que $W_i = exp(log(W_i))$ on cherche doc $log(W_i)$

$log(W_i) = log(\frac{k_i C_i}{\sum_{i}^{n} k _i C_i}) = log(k_i C_i) - log(\sum_{i}^{n} k _i C_i)$

Or $log(k_i C_i) = log(k_i) + log(C_i)$ avec $k_i$ étant une proba, il faut juste s'assure de ne pas considérer les éléments où $k_i = 0$, mais de toutes manière il n'influe pas la distribution, on peut les écarter sans soucis.

Ensuites: $log(C_i) = Log(B(k + \alpha_i, n - k + \beta_i) / B(\alpha_i, \beta_i)) = log(B(k + \alpha_i, n - k + \beta_i)) - log( B(\alpha_i, \beta_i)) = LogBeta(k + \alpha_i, n - k + \beta_i)) - LogBeta(\alpha_i, \beta_i))$ En python on a par exemple utiliser la fonction scipy.special.betaln pour calculer la fonction LogBeta.

L'utilisation du log nous permet de calculer des très faible valeurs ! C'est comme quand on utilise la LogVraisemblance au lieu de la vraisemblance classique car sinon le prosuit des proba sera trop faible et serait estimé à 0. En utilisant le log du produit des proba on arrive à l'estimer, nous c'est pareil !

Une fois que l'on a calculer tout les $log(k_i C_i)$ il faut calculer leur somme pour ensuite normaliser et obtenir $W_i$, on cherche donc $log(\sum_{i}^{n} k _i C_i)$ Attention: Le log d'une somme n'est pas la somme des Log !!
Heuresement il existe une technique pour calculer cela, en passant notamment par l'exponontielle à l'interieur de la somme :
$log(\sum_{i}^{n} k _i C_i) = log(\sum_{i}^{n} exp(log(k_iC_i)))$. On connsait en effet $log(k_i C_i)$, et en utilisant par exemple la fonction scipy.special.logsumexp on va pouvoir calculer le log de la somme: $log(\sum_{i}^{n} k _i C_i)$ :)
Ensuite on fait simplement $log(k_i C_i) - log(\sum_{i}^{n} k _i C_i)$

Ensuite on a juste à repasser à l'exponentielle ! Étant une proportion, il est beaucoup plus facile de revenir à ce moment là




Pour la CDF (Cumulative Distribution Function) c'est aussi très simple:

Si $f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) = \sum_i k_{i}f(x; \theta_i)$ alors on cherche : $P(x \le z)$

$P(x \le z) = \int_{0}^{z} f(x; \theta_{1}, \theta_{2}, ..., \theta_{n}) dx = \int_{0}^{z} \sum_i k_{i}f(x; \theta_i) dx$

Or l'intégrale d'une somme étant la somme des intégrale et $k_i$ ne dépendant pas de $x$, on a :

$$P(x \le z) = \sum_i k_{i} \int_{0}^{z}f(x; \theta_i) dx$$

Ainsi, on pour obtenir la CDF d'une distribution de mélange , il suffit de combiner les CDF des composantes du modèles par rapport aux poids $k_i$, en gros: 

$$P(x \le z) = \sum_i k_{i} P_{i}(x \le z)$$


## Précisions (2)

Dans notre cas on cherche à déterminer $f_{post}(p|k, n)$ doit la distribution du paramètre $p$ en fonction de nos données observée. Mais en réalité ces données sont observées sur un composé particulier, pour lequel on va disposer d'un prior spécifique de ce composé. Donc, si on voulait être complètement rijgoureux, il faudrait également préciser que l'on conditionne par rapport à la specie considéré $S_i$, on se place dans la condition particulière ou l'on étudie $S_i$. Heureusement Grâce au théroême des probabilité conditionnelle, ça ne change rien à la formule ! En effet, si on note $Q_i(p|k,n)$ la distribution de probabilité de $p$ par rapport à nos observations $k,n$ et conditionné par rapoort au fait que l'on observe la specie $S_i$, on a que :


Pour simplifier l'écriture on dégage le $n$. En fait, même dans les formule du haut, je suis pas sur que pour faire propre il faille écrire le $n$ dans la formule... Car même si c'est un paramètre de la loi Binomiale, il est fixé et n'a pas de 'proba' derrière ... Car au final le vrai paramètre de la loi Binomiale c'est surtout $p$ Enfin bon c'est du détail ^^

$Q_i(p|k) = \frac{Q_i(k|p)Q_i(p)}{Q_i(k)} = \frac{p(k|p \cap S_i) p(p| S_i)}{p(k|S_i)} = \frac{p(k|p \cap S_i) \frac{p(p \cap S_i)}{p(S_i)}}{p(k|S_i)} = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(k|S_i)}$

$p(k|S_i)$ est bien sur donc égal à $p(k|S_i) = \int p(k|p \cap S_i) p(p| S_i) dp$, masi on garde la forme comptacte pour simplifier !

Donc $Q_i(p|k) = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(k|S_i)} = \frac{p(k \cap p \cap S_i)/p(S_i)}{p(p \cap S_i)/p(S_i)} = \frac{p(k \cap p \cap S_i)}{p(p \cap S_i)} = p(p | k, S_i)$

Et donc on retrouve bien que l'on cherche la distibution de $p$ par rapport aux comptages observée sachant qe l'on étudie $S_i$. Tout cela est garanti par le théorême des probabilités conditionnelles.




## Donc en résumé notre approche :

1- Déterminer les probas de transitions: On cherche à déterminer la proportion de marcheurs partant de l’ensemble des autres métabolites z, qui arrivent sur notre métabolites cible x. Pour la calculer, on chercher premièrement à déterminer pour chaque métabolites son PPR, sans considérer le métabolites en lui même. 
Pour cela, on travaille, sur un réseau modifié, dans lequel le métabolite cible a été retiré. Le vecteur de restart de notre PPR va alors être composé des voisins directs de notre mlétabolites cible. De cette manière, a chaque restart, on va recommencer à partir de l'un des voisins de notre noeud cible, ce qui est comme si on était reparti de notre noeud cible et que l'on avait directement emprunté une transition vers l'un de ces voisins. L'important est que l'on ai une probabilié à 0 pour notre noeud cible car pour la suite on ne souhaite pas qu'il est d'influence sur notre prior.

Donc on pose :

$$M = \alpha P + (\alpha* a + (1 - \alpha) e)v^\intercal$$
Tout les vecteurs sont des vecteurs colonnes.
- $P$ est notre matrice de probabilité obtenue en ayant préalablement supprimé la colonnes et la ligne correspond à l'index de notre noeud cible
- $\alpha$ est le damping factor: la probabilité de transité vers un voisin. $(1 - \alpha)$ est la probabilité de restart
- $a$ est un vecteur colonne égal à 1 pour les noeuds puits (sink nodes)
- $e$ est un vecteur colonne de $1$
- $v$ est le vecteur colonne contenant les probabilité de restart conduisant aux voisins du noeud cible.


En utilisant la méthode de la puissance itérée (pas besoin de sortir des algo shiny nos graph sont suffisament petit pour que la power method s'applique)

Le vecteur de probabilité obtenu s'interprête comme étant la distribution stationnaire des probabilités liée à notre marche aléatoire. On peut interpréter ce vecteur comme la proportion du temps passé sur chaque composé $i$ en réalisant une marche partant (et recommençant avec un facteur $(1-\alpha)$) des voisins de notre noeud cible. Si on imagine un cas simpliste avec un vecteur final $(0,0,0,0.5,0.3,0.2)$, on peut également imaginer que si 100 marcheurs partaient des voisins de mon noeud cible, au cours de la marche aléatoire on verrait en moyenne 50 marcheuurs sur le noeud 4, 30 sur le noeud 5 et 20 sur le noeud 6.

Ainsi pour chaque noeud ce vecteur me donne les probabilitées d'une marche aléatoire partant des voisins de ce noeud. De cette manière le noeud en lui même n'est jamais visité car il est préalablement supprimer du réseau et ainsi on ne passe jamais par le noeud cible au cours de notre marche aléatoire. On simule en quelque sorte que l'on transite directement de celui-ci vers un de ces voisins en utilisant le vecteurt de restart correspondant à ces voisins.

On va donc utiliser ce vecteur pour calculer le vecteur des probabilités, des proportions, des marcheurs qui se trouve sur notre noeud cible au cours d'une marche aléatoire partant des autres noeuds du réseau. Dans le cas précédent, le noeud cible était l'émetteur de l'information et les autres noeuds les récepteurs: les marcheurs partaient de notre noeuds et se diffuser dans le réseau. Maintenant, on considère que tous les autres noeuds envoient des marcheurs dans le réseau, et on veut chercher à déterminer, parmis tous ceux qui arrivent sur notre noeud cible, qu'elle proportion est originaire du noeud A, du noeud B, etc .... Ici, ce sont les autres noeuds qui émettent l'information et le noeud cible qui capte l'info des autres noeuds ! C'est beaucoup plus dans la philosophie que l'on recherche, où on souhaite construire un prior à partir de l'information des voisins du noeuds. On veut donc que ce soit les autres noeuds qui "diffuse" leur info.

Pour constuire ce nouveau vecteur de probabilité, on va simplement pondéré la probabilité, le temps passé, par l'ensemble des autres noeuds sur le noeud cible $x$. En convertissant mes proba et nombre de marcheurs, si je multiplie tout les vecteurs obtenus précédement par 100 par exemple: j'obtiens pour chacun, le nombre attendu de marcheurs qui, partant des voisins de leur noeud de départ, se trouve sur le noeud $x$. Par exemple, je pourrais observer que sur ce noeud $x$, on a 40 marcheurs qui proviennent du noeud A, 20 marcheurs du neoud B, 30 de C et 10 de D. Ainsi pour ce noeud je pourrais construire le vecteur $(0.4,0.2,0.3,0.1)$. Ainsi lorsqu'un marcheur se trouve sur mon noeud, il a 40% de chance d'être originaire du noeud A, 20% de chance d'être originaire du noeud B etc ...
Quand on dispose de notre matrice des vecteur de PPR en colonne, il s'agit simplement de calculer les proportion des valeurs sur chaque ligne !
C'est exactemement ce que l'on veut représenter pour, l'information a priori: on veut que l'information apporté par le noeud X au prior, soit proportionnelle à la proportion de l'information arrivé sur le noeud cible qui est originaire de X. 
Comme précédement, sachant que mes proba sont à 0 pour le noeud en lui-même, dans le calcul de ce nouveau vecteur il ne compte toujours pas ! :)

Cf. figure propagation

Globalement les deux approches ont des résultats proches (SFT ou FOT) mais il semble que :
SFT soit plus sensible à ces voisins “hub” c a d ces voisins qui ont une forte connectivité (le marcheur partant du noeud cible passe effectivement plus souvent par eux)
FOT est plus sensible à ces voisins directe et pénalise les Hubs (les marcheurs partant des voisins directs passe souvent par le noeud cible et ceux partant des noeuds ont tendance à se perdre

L'idée étant d'utiliser ce vecteur FOT de probabilité comme vecteur de pondération dans notre mélange de Beta.


Notre modèle de mélange se décompose en plusieurs étapes: 

On étudie une association entre une specie $s$ et un MeSH $m$

**L'hypothèse de base: On suppose que tous les composés discuttent de tous les MeSH ! On fait ainsi l'hypothèse que toutes probabilité ** $p(m|s) > 0$**, c'est à dire pour tout composé, la probabilité qu'un de ces articles mentionnent n'importe quel MeSH est strictement postive. Récipriquement, on dit que la probabilité qu'un article parlant d'un composé, ne metionne JAMAIS un terme MeSH quelconque est nulle. Néanmoins ces probabilités peuvent être extrêment faibles pour certains MeSH, on en discutte jamais et très grande pour d'autres: dès que je parle de mon composé j'ai de grande chance de parler également de mon mesh. On suppose ainsi que toutes les fois où l'on observe une coocurence à 0 entre un composé et un MeSH, c'est simplement car on a pas assez d'échantillons d'articles pour représente notre composé. On imagine que si je piochais 1 millions de nouveaux articles discuttant de mon composé, je verrai enfin une coocurence avec mon MeSH. Elle sera très très rare et non significative, mais sa probabilité sera > 0**

## Simplification du modèle :

Notre modèle où l'on souhaite modéliser le paramètre *s* en fonction de la taille du corpus MeSH est peut être biaisé :

D'un coté en formulant les choses de cette manière, le modèle "semble" fonctionner correctement, mais quand j'essaie de me l'expliquer, je trouve qu'il y a des éléments dont je ne suis pas certains. Est-ce qu'au final ces différences de variabilité que j'observe et que je veux représenter, est-ce que c'est pas simplement des artefacts, justement due aux différences que j'ai sur les tailles de corpus de mes MeSH. Mais, si tout mes composés était a peu près représentés de la même manière dans la litterature (le cas idéal), est-ce que je verrais également ça ? Pas sur ...
Il y a une phrase dans un cours (Cf. Ref) que j'ai lu qui dit : "la valeur que nous choisissons pour le k (le paramètre de taille d'échantillon) peut être pensée de cette façon : C'est le nombre de nouveaux coups de pièce de monnaie dont nous aurions besoin pour nous faire osciller entre les nouvelles données et la croyance antérieure concernant $\mu$. Si nous n'avions besoin que de quelques nouveaux coups de pièce de monnaie pour faire osciller nos croyances, alors nos croyances antérieures devraient être représentées par un petit k. Si nous avons besoin d'un grand nombre de nouveaux mouvements pour nous éloigner de nos convictions antérieures sur $\mu$, alors nos convictions antérieures valent un très grand k."
Je vais poser s (le paramètre de taille d'échantillon) comme un hyperparamètre en disant: Je pose un prior en considérant qu'il a été construit sur la base de s articles, ainsi on peut s'attendre à ce que seul les composé avec au moins s articles seront en mesures de faire changé d'avis mon prior. C'est à dire que rajouter au moins autant d'observations qu'il en a été utilisé pour construire le prior
De la sorte, j'impose une variabiltié sur le prior de tout les MeSH, $V(X) = \frac{\mu (1 - \mu)}{(1 + s)}$. Ainsi elle sera nécéssairement croissante en fonction de mu (jusqu'a 0.5 mais celui-ci n'est jamais atteint) et sinon sera décoissante par rapport à s
Ça me semble relativement propre ...
On connait facilement la moyenne et la variance de notre prior. On l'imagine donc comme étant issus de pseudo observations. On a donc un prior construit avec $mu s$ pseudo-succès et $(1 - mu)s$ pseudo échecs
Après peut être que c'est un hyperparamètre que l'on a envie de faire varié pour rendre plus ou moins "fort" le prior ....

**Cependant,** vue que dans la majorité de nos cas notre alpha sera < 1, et pour la beta il semble que l'on puisse interpréter alpha - 1 en terme de pseudo count, et non alpha ! : Cf Wiki :
In Bayesian inference, using a prior distribution Beta($\alpha$Prior,$\beta$Prior) prior to a binomial distribution is equivalent to adding ($\alpha$Prior - 1) pseudo-observations of "success" and ($\beta$Prior - 1) pseudo-observations of "failure" to the actual number of successes and failures observed, then estimating the parameter p of the binomial distribution by the proportion of successes over both real- and pseudo-observations. A uniform prior Beta(1,1) does not add (or subtract) any pseudo-observations since for Beta(1,1) it follows that ($\alpha$Prior - 1) = 0 and ($\beta$Prior - 1) = 0. 
Donc en fait si mon composé n'a pas d'observations succès, son nombre de pseudo-counts success sera négatif !!!
Peut être que dans ce cas il vaut mieux le dégager du prior ...
Pour ré-équilibrer les weights on a juste à recalculer les proportions sans lui ... et comme ça on ne le prends plus en compte. Si aucun voisin ne reste, peut être que l'on ne peut rien faire ...

A voir avec Pablo et tout !!

Ce qui dis Wikipedia : 
The Haldane prior Beta(0,0) subtracts one pseudo observation from each and Jeffreys prior Beta(1/2,1/2) subtracts 1/2 pseudo-observation of success and an equal number of failure. This subtraction has the effect of smoothing out the posterior distribution. If the proportion of successes is not 50% (s/n $\neq$ 1/2) values of $\alpha$Prior and $\beta$Prior less than 1 (and therefore negative ($\alpha$Prior - 1) and ($\beta$Prior - 1)) favor sparsity, i.e. distributions where the parameter p is closer to either 0 or 1. In effect, values of $\alpha$Prior and $\beta$Prior between 0 and 1, when operating together, function as a concentration parameter. 

Après comment choisir un *s* par défaut ?


Alors j'ai peut être trouvé une méthode pas trop mal pour ce donner une idée :

Je récupère des associations significatives de FORUM (q.value <= 1e-6), sans weakness et parlant de diseases, j'en échantillonnage 1 million par exemple 

Vue que dans notre modèle thérorique mu est fixé par la taille du corpus du MeSH, seul s, la taille d'échantillon peut varier

On sélectionne plusieurs valeurs possible de s, j'ai pris

Pour chaque valeur de s, on calcule la distribution à posteriori que l'on obtiendrait pour chaque observation en utilisant un prior construit avec s

Pour chaque distribution à posteriori obtenu, on calcule la valeur P(p <= mu), qui est la probabiltié selon ma distribution que la valeur de p soit inférieure à la probabilité d'observer le MeSH de manière générale. Plus elle est faible plus on est certain que notre association est signficative, notre proba est très certainement supérieure à la proba d'observer le MeSH de manière générale. C'est notamment comme ça que je test l'asssociation avec le modèle de mélange à la fin.

Enfin, on calcule la proportion d'associations pour lesquelles P(p <= mu) indique toujours que l'asso est significative. Cela indique que notre prior n'a pas écrasé notre observation et que ceux qui était significatif l'est toujours selon la distribution

J'ai vérifié initialement et en utilisant un prior non-informatif 100% des assos sont retrouvé donc pas de biais :wink:


Et on peut faire pareil pour els assos Négatives !!! 

Ça nous donne une idée de comment notre paramètre *s* influe sur les taux de faux-positifs et négatifs ;)

### Nouvelle manère de voir les choses :

En fait c'est pas si différent mais dans mon approche actuelle il y a peut être quelque chose de mal gérer au niveau de mes poids
- initialement j'utilise ma probabilité FOT, qui représente en gros la proportion de marcheurs qui arrivent sur mon noeud cible. La chose étant, mes distribution à posteriori représente la probabilité qu'un article parlant du composé $i$ discutte du MeSH $M$. Ainsi, d'un coté je discutte de marcheurs qui représente en gros des paquets d'information et de l'autre coté on a des articles, c'est pas vraiment la même chose... En fait je pense que pour que mon modèle soit vraiment fluide, il faudrait que j'utilise les probabilité qu'un *article* du composé $i$ arrive sur mon composé cible. De cette manière, mon poids dans la mixture correpondent à la probabilité qu'un article arrivé sur le composé cible proviennent du composé $i$, et les proba associé correspondent à la probabilité qu'un article provenant du composé $i$, discutte de mon MeSH. Cela semble collé.

C'est pablo qui a soulevé le problème. En effet mes probabilité jointe, qu'un article arrivé mon composé cible discutte du composé $i$ **ET** du MeSH $M$ snt assez flou car ce que je fais c'est une moyenne à la fin ... "Define the full joint probability of each node, which is not the same as averaging the outputs of the other nodes"

**ATTENTION: ** Quand je faisait mon test de Fisher, je considéré mes composés entre eux, et mes MeSH entre eux, comme étant **des variables différentes** et non **des modalités** d'une même variables !! Dans mon test de Fisher l'évènement: l'article discutte du composé $i$ est une variable binaire: l'article discutte ou non du composé $i$, pareil pour les MeSH ! On avait jamais à prendre en compte le cas où un article parlait de plusieurs composés ou MeSH ... MAIS là, si on veut les considéré comme des modalités d'une même variable ça va être différent !!!!

Ainsi, on considère la variable **composé** comme étant les modalité d'une même variable du type: "la mention dans la literature, implique le composé $i$" **MAIS** la variable MeSH est toujours du type binaire, la mention implque ou non, le mesh $m$. Ainsi, on duplique les associations uniquement par composé mais pas par MeSH.
J'ai fait un schema pour mieux comprendre.

Mais par contre: si on divise également nos mentions par MeSH: a ce moment toutes les variables deviennent des variables à modalité et à ce moment là, je poourrait utiliser une distribution de dirichlet comme prior !!!

Ainsi, peut être qu'il faut que l'on discutte en terme d'association ! "Une asociation implique mon composé et mon MeSh" Ainsi, ce que l'on diffuse depuis les composés dans le réseau, ce sont des citation, des mentions, d'un composé avec un MeSH dans un article

Cela change donc la probabilité simple d'observer 1 MeSH !!!
En effet, désormais on ne cherche plus à savoir la probabilité d'un article discutte du MeSH, mais à savoir la probabilité qu'une "mention" dicutte du MeSH
Néanmoins on peut faire la même hypothèse en terme d'indépendance ! La proabilité qu'une mention implique le MeSH $M$, sachant qu'elle implique dle composé $i$ est égale à la proba générale qu'une mention implique le MeSH $M$.

Le modèle posé ne change pas :

pour un composé $C_i$, j'ai $N_i$ article associé, formant $N_i$ mention de ce composés avec $k_i$ impliquant également mon MeSH $M$
Pour une mention quelconque impliquant ce composé $C_i$, il a une probabilité $p_i$ qu'elle implique mon MeSH $M$. Si on note $X$ la variable aléatoire suivant un loi de Bernouli avec $X = 1$: "la mention parle du MeSH $M$" et $X = 0$: "la mention ne parle pas du MeSH $M$". J'ai $X \sim Be(p_i)$
Cet épreuve est répété $N_i$ fois, et donc j'ai une suite de variables $X_1^i,X_2^i, ..., X_{N_i}^i$ qui suivent cette loi de Bernouli $Be(p_i)$

On peut donc dire que leur somme pour le composé $i$, $\sum_{k = 1}^{N_i} X_k^i$, suit une loi Binomiale de même paramètre $p_i$, $Bin(p_i, N_i)$

Le fait est que je dispose également d'un apriori sur ce paramètre $p_i$ représente par ma distribution Beta: $Beta(\mu \nu, (1 - \mu) \nu)$ où $\nu$ est un hyperparamètre fixé et $\mu$ représente la probabilité générale, sur l'ensemble des mentions observées, qu'une mention implique le MeSH $M$



On peut donc redéfinit notre modèle et nos probabilité marginale :

Les évènements:

- $m$: la mention parle/implique le mesh $m$

- $c_i$: la mention parle/implique le composé $c_i$

- $I = k$ la mention a atteint le composé cible $j$

$f(m|I = k) = \sum_{c= 1}^{N} p(m|c_i)p(c_i|I = k)$, soit la probabilité d'observer une mention impliquant le mesh $m$ parmis celles arrivées sur le composé d'intérêt $k$. 

$p(c_i|I = k)$ représentant la probabilité qu'une mention, arrivée sur le composé cible $k$, provienne de $c_i$, c'est les poids dans le modèle.

$p(m|c_i)$ representant la probabilité qu'une mention parlant du composé $c_i$, implique également le mesh $m$. Ceci étant indépendant de l'évènement $I = k$: la mention arrive sur le d'intérêt $k$. en gros le fait de savoir que la mention est sur le composé d'intérêt n'influe pas les proba qu'elle implique que MeSH et donc : $p(m|c_i, I = k) = p(m|c_i = k)$

$f(m|I=k)$ représente donc la probabilité qu'une mention arrivant sur le composé cible $k$ mentionne le mesh $m$.

En effet: 
$f(m, c_i) = p(m|c_i)p(c_i)$

$f(m, c_i|I=k) = p(m|c_i, I=k)p(c_i|I=k) = p(m|c_i)p(c_i|I=k)$ car indepéndace à $p(m|c_i, I=k) = p(m|c_i)$

Et donc si on somme pour tous les $c_i$, on a bien que $f(m|I=k) = \sum_{c=1}^{N} p(m|c_i)p(c_i|I=k)$



Ensuite, à partir de cette distriubution, on calcule la probabilité à posteriori comme avant en ajoutant les observations qui sont propre au composé cible !

Don en gros les choses qui changent par rapport àce que j'avais prévu intialement sont :

- On considère des mentions et non des articles : Donc on calcule les probabilité marginales $p(m)$ avec la somme des co-occurences (qui représente les mentions) et non plus en utilisant le nombre total d'article.

$N_i$ est le nombre total de mentions de composé $i$, ce qui correspond toujours à son nombre d'article annotés

$N_i^m$ est le nombre de mentions du composé $i$ impluquant également le mesh $m$, c'est toujours la coocurence.

$p(m|c_i) = \frac{N^m_i}{N_i}$, c'est la probabilité qu'une mention du composé $i$, implique le mesh $m$, c'est toujours la co-coocurence sur la taille du corpus du composé

$p(c_i) = \frac{N_i}{\sum_j N_j}$, cette probabilité n'est plus calculé en utilisant l'Univers, le nombre d'articles distincts, mais, en utilisant la somme du nombre de mentions (aka nombre d'articles) annotés à chaque composé

$p(m) = \sum_i p(m|c_i) p(ci) = \sum_i \frac{N_i^m}{Ni} \frac{N_i}{\sum_j N_j} = \sum_i \frac{N_i^m}{\sum_j N_j}$

- Ainsi mes poids dans ma mixture : $p(c_i|I = k) = \frac{p(c_i, I=k)}{p(I=k)} = \frac{p(c_i, I=k)}{\sum_jp(c_j, I=k)} = \frac{P(I=k|c_i)p(c_i)}{\sum_j P(I=k|c_j)p(c_j)}$

Cela représente la probabilité qu'une mention arrivé sur le composé $k$ provienne du composé $C_i$

### Comment définit on $P(I = k|c_i)$ ?

Pour définir cette probabilité, on souhaite appliquer plusieurs contraintes :
- 1) Un composé ne peut pas contribuer à lui-même
- 2) chaque composé à un certain **voisinnage d'influence** c'est à dire un set de composé pour lesquels il est légitime qu'il contribue à la construction du prior. Cette deuxième contrainte à notamment due être mise en place car il se trouve que sinon, les STARs pouvant globalement contribuer à tous les éléments du réseau, peuvent être des contributeurs majoritaires dans des zones où aucun voisin n'a de littérature. Pour autant ces STARs se situent généralement bcp plus loin dans le réseau et ne sont pas pertinentes pour contruire le prior de ces composés. Dans ces cas là, on préfèrera utiliser le prior par défaut. C'est pour cela que l'on définie une zone d'influence pour chaque composé.

La solution retenue est de créer une matrice de containte afin de sélectionner a quel autre composé, un composé peut il contribuer de manière légitime. Aussi, on empêche toujours un composé de contribuer à lui-même.
Donc, pour contruire cette matrice de contrainte, on va demandé à ce que la probabilité d'atteindre un noeud $k$ depuis un potentiel contributeur $i$ (hors du noeud cible) soit au moins supérieure à $\frac{1}{n -1}$ (n étant le nombre total de noeud, on fait -1 car on compte pas le noeud lui-même) c'est à dire à la probabilité de sélectionner ce noeud au hasard. De manière plus formelle, on demande donc que :

$\frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$ soit que la probabilité d'atteindre $k$ depuis $i$ (sans considéré les marches retournant à $i$) est supérieure à la proba de choisir $k$ au hasard parmis tout les noeud du réseau.

Ceci équivaut donc aussi à $\frac{\pi_{k,i}}{(1 - \pi_{i,i})} (n - 1) > 1$ ce qui offre une interprétation un peu plus intuituve. Imaginons que chaque composé dispose de $n - 1$ cartons d'invitation, donc 1 pour chaque autre noeud dans le réseau, c'est comme si on simulait la propagation de ces cartons suivant la marche définie et ensuite on ne récupère que les composé qui ont reçu au moins un carton d'invitation. Ainsi les composés qui peuvent influé $k$ sont les composé pour lesquels $k$ à reçu au moins un carton d'invitation provenant d'eux.

En se positionnant sur le composé cible, on va donc chercher à déterminer l'ensemble des noeuds qui semblent légitime, d'après cette contrainte, pour contribuer à son prior.
On va appelé les composés qui respectent cette régle les **contributeurs** de $k$ (les noeuds pouvant influencer $k$) que l'on notera $CTB_k$. 
Ainsi, $\forall i \in CTB_k, i \neq k: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$. Vue que l'on empêche les composés de contribuer à eux-même, le composé $k$ ne peut pas faire partie de ses contributeurs donc il ne contribue toujours pas au prior ! On peut donc également dire que pour être selectionné comme contributeur d'un composé, il faut atteindre ce composé via une marche aléatoire, avec une probabilité qui soit supérieure à celle d'être selectionnée dans le cas d'une selection au hasard des contributeurs (aussi $\frac{1}{n -1}$).

Symétriquement, on peut définir le **voisinnage d'influence**, noté $IN_i$ de chaque composé $i$ comme étant le set de composé qu'il peut légitimement influencer au vue des contraintes que l'on a posé !

 $\forall k \in IN_i, k \neq i: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$

On impose que $i \notin IN_i$ 


Comme on le faisait avant, on va donc re-estimer les probabilité pour qu'elles ne se concentre sur les composé que $i$ peut légitimement influé. On va donc dire que :

$\forall k \in IN_i: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$ On re-proportionne les probas en se basant uniquement sur le set que $i$ peut influé.

$\forall k \notin IN_i: P(I = k|C_i) = 0$ avec $i \notin IN_i$ 

Réciproquement,

$\forall i \in CTB_k: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$

Sinon, 

$\forall i \notin CTB_k: P(I = k|C_i) = 0$ avec $k \notin CTB_k$


C'est comme ça que donc définies les probabilités $P(I = k|C_i)$ que l'on va ensuite utilisées pour calculer les poids dns notre modèle : $P(C_i|I = k)$

====================

Donc :

- Ainsi, pour $i \notin CTB_k$ (dont $k$ lui-même donc):

$p(c_i|I = k) = \frac{P(I=k|c_i)p(c_i)}{\sum_j P(I=k|c_j)p(c_j)} = 0$ car $P(I = k|c_i) = 0$ 

- Pour $i \in CTB_k$:

$p(c_i|I = k) = \frac{P(I=k|c_i)p(c_i)}{\sum_{j \in CTB_k} P(I=k|c_j)p(c_j)} = \frac{\frac{\pi_{k,i} }{\sum_{v \in IN_i} \pi_{v,i}} \frac{N_i}{N} }{\sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} \frac{N_j}{N} } = \frac{\frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}} N_i}{\sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} N_j}$

Pour calculer $P(I = k)$ au dénorminateur, on a juste besoin de faire la somme sur les $j \in CTB_k$ car si $j \notin CTB_k$, alors $P(I = k|c_j) = 0$

Ainsi, on calcule les poids en utilisant uniquement les probabilités que des contributeurs légitimes envoient une de leurs mentions vers le composé cible. Néanmoins, il peut arriver que seule une sous-partie (ou voir même aucun des contributeurs légitime) n'est de mentions à partager car il ne dispose pas de littérature. A ce moment là, la somme des $p(c_i|I = k)$ pour tous les $c_i$ est nulle, c'est à dire qu'aucune mention venant des contributeurs de $k$ ne peut arrivé sur $k$ car ils n'ont pas de mention à partagé. A ce moment là, on utilise le prior par défaut. Dans ce cas précis, on peut constater que $P(I = k) = 0$ car $P(I = k) = \sum_{j \in CTB_k} P(I=k|c_j)p(c_j) = \sum_{j \in CTB_k} \frac{\pi_{k,j} }{\sum_{v \in IN_j} \pi_{v,j}} \frac{N_j}{N}$ Or si tous les $N_j = 0$ pour les $j \in CTB_k$, alors on a effectivement $p(I = k)$ et donc il est déjà impossible qu'une mention arrive sur le composé $k$

La matrice $pi$ contient les vecteurs de résultats du PPR rangées **en colonnes** ! Ainsi, $\pi_{k,i}$ représente la probabilité attendu d'atteindre le noeud $k$ dans le réseau pour une marche aléatoire de type PPR partant du noeud $i$.

Donc voilà pour les poids ! :)

- Indépendance conditionnelle de  $p(m|c_i, I=k) = p(m|c_i)$:

En fait l'indépendance conditionnielle nous dis que : $P(m, I = k | c_i) = P(m| c_i) P(I = k | c_i)$. C'est à dire que les évènement $m$ et $I = k$ sont indépendant conditonnellement par rapport $c_i$. Littéralement, cela nous dit que la probabilité qu'une mention parlant de $c_i$, mentionne le MeSH $m$ et arrive sur le composé $k$ ($I = k$) est égal à la proba qu'une mention de $c_i$ parle de $m$, multiplié par la proba qu'une mention de $c_i$ arrive sur $k$. le fait que la mention parle de $m$ n'influe pas les proba qu'elle arrive sur $k$ et inversement !

Ainsi: $P(m|c_i, I=k) = \frac{P(m, c_i, I=k)}{P(c_i, I=k)} = \frac{P(m, I=k | c_i) P(c_i)}{P(I = k|c_i)P(c_i)} = \frac{p P(m| c_i) P(I = k | c_i) P(c_i)}{P(I = k|c_i)P(c_i)} = P(m| c_i)$


En effet, si on voulait déterminer $p(m|c_i, I=k)$ on ferait $p(m|c_i, I=k) = p(m|c_i) = \frac{N_i^m}{N_i}$

Donc finalement avec toutes ses probabilités, on va pouvoir faire la même chose ! C'est juste que maintenant notre prior parle en terme de mentions et non plus d'articles mais l'idée d'indépendance qu'il soutient est toujours la même. Sous hypothèse d'indépendance $p(m|c_i) = p(m)$, le fait que la mention implique/provienne du composé $c_i$ n'a aucun effet sur le fait qu'elle implique le MeSH $m$, donc on peut toujours mettre notre $\mu$ à $p(m)$

Ensuite on ajoute effectivement des observations. Pour chaque composé $c_i$, initialement j'ai un apriori sur $p(m|c_i)$, dans lequel $\mu = p(m)$. Mais j'observe ensuite mes observations $N$ et $N_i$ qui me permettent de mettre à jour ma probabilité $p(m|c_i)$, qui est toujours indépendance de $I=k$. Ensuite je peux utiliser cette probabilité pour construire mon prior pour le composé cible. Je cherche alors $p(m|I=k)$ qui est la probabilité qu'une mention parle du MeSH $m$ sachant qu'elle est arrivé sur le composé cible ! Alors, $p(m|I=k) = \sum_{c= 1}^{n} = p(m|c_i)p(c_i|I=k)$, où $p(c_i|I=k)$ est simplement la proba qu'une mention ayant atteint le composé cible $k$, provienne de $c_i$ !


$\sigma_i$ étant simplement la probabilité qu'une mention atteigne le composé cible (évènement $I$), sachant qu'ell est partie du composé $c_i$

### Ce qui change :

Désormais, les poids dans le modèle de mélange sont également fonction de la taille du corpus du composé (en terme de nombre de *mentions* à des articles)
Ceci implique que **les composés sans literature ne participent PAS à la construction des priors**

Ainsi, si un composé est entouré uniquement de composés non-informatif (en utilisant $\alpha = 0$ par exemple), il se peut que l'on ne puisse pas lui définir de prior car tous ces voisins sont non-informatif. A ce moment là, on affiche un Warning: "Neiborhood literature information does not reach the targeted compound. You should increase the damping factor. Use default prior." et on va réaliser l'analyse en considérant comme un prior, le prior général associé au MeSH, déterminé en fonction de $\mu$ et $\nu$. Ainsi, il n'y **pas** de distribution de mélange construite, c'est un prior simple !
On renvoie les même valeurs que pour une analyse classique (Mean, CDF, log2FC) mais pas le *Log2CDFRatio* puisque notre prior est exactement le même que celui qui est prévu de manière générale ..

Une autre chose à bien voir est que même si l'on utilise la taille de corpus dans le calcul des poids, on utilise seulement le corpus effectif associé à chaque composé, on ne compte pas les articles *imaginaires* qui sont issus du prior général dans ce calcul ! Ces articles *imaginaires*, dont le nombre est gérer par l'hyperparamètre $\nu$, ne sont utilisés que pour éviter des biais par rapport à des voisins qui aurait peu de literature et dont les probabilités seraient peu fiables. Ainsi en réalité ce prior général n'est appliqué que sur les composés qui disposent de literature car les autres composés n'ayant pas de littérature et donc un poids nul, ne sont pas pris en compte dans l'analyse.


L'avantage par rappport à l'ancienne version est que l'on perd le coté ultra conservateur de la méthode. Avant, sachant que les poids était simplement calculé en fonction du PPR, un métabolite avec 10 articles associés pouvait avoir autant de poids dans le prior qu'un composé avec 10000 article. Ce qui fait que même dans des cas où 9 voisins sur 10 par exemple indiquaient un lien avec le MeSH, il suffisant qu'un seul des voisins ne l'indique pas pourque le modèle rejette l'association. Maintenant que les poids de le modèle sont calculé en fonction de la littérature, il faudra que ce dernier voisin est beaucoup de littérature pour faire penché la balance.




### Loi géométrique et longueur moyenne des marches

Ref: Cf. cours/stats.oagerank.pdf

Pour modéliser la taille moyenne des marches, on peut utiliser la loi géométrique.
La loi géométrique  définit la probabilité $P(X = k)$ de la distribution du nombre $k$ d'échecs pour une série d'expérience de Bernouli, avant le premier succès :
$P(X = k) = (1 - p)^k p$ pour $k \in \{1,2, ..., n\}$

Ici $p$ est la probabilité de succès, on réalise $k$ échecs avec une proba $(1 - p)$ avant le premier succès avec une proba $p$

Dans notre cas, on définit comme succès, le fait de restart notre marche, la marche donc d'une marche. Donc $p = (1 - \alpha)$

Dans notre PPR classique, on a donc $P(X = k) = \alpha^k (1 - \alpha)$

La moyenne de cette distribution est alors $\frac{\alpha}{(1 - \alpha)}$

Sachant que notre PPR restart sur notre noeud cible, cela nous montre donc la taille moyenne d'une marche partant de notre noeud cible. ON en en moyenne visiter $k$ noeud, avant de revenir sur notre noeud de départ. 

On peut également constater que le ratio entre les probabilité à $k+1$ et $k$ va être constant et fonction de $\alpha$ $\frac{P(X = k)}{P(X = k+1)} = \frac{\alpha^k (1 - \alpha)}{\alpha^{k+1} (1 - \alpha)} = \frac{\alpha^k}{\alpha^{k+1}} = \frac{1}{\alpha}$
Par exemple donc pour $\alpha = 0.1$ on a donc 10 fois plus de chance de faire une marche de $X = k$ que de $X = k+1$ 

### Résultats de validation

Les résultats de validation montre les ROC curves par rapport aux différentes indices utilisés: CDF, Log2FC, ou le score.

- Globalement Score et Log2FC semble donner le même score lorsque l'on considère notre set négatifs comme étant des associations sous-représentées (OR < 0.1 & q.value = 1). C'est la manière la plus simple de définir le set de négatif, car dans le cas là on limite de taux de faux-négatif en utilisant ce dont le odd-ratio tend à représenter une sous-représentation.

- Néanmoins, le score que l'on a développé semble pertinent pour du moins ranker les associations

Les effet des facteurs alpha et sample_size sont également nets.

On a réalisé les barplots de TPR et FPR en utilisant différents seuil sur le Log2FC: 1, 2, 3

Globalement, le taux de True-positive et de FPR diminue avec alpha et également avec sample_size.

Comme attendu, plus on va chercher des articles loin dans le réseau plus :
  1) On prend le risque de récupèrer des articles de composé 'trop aloignés' pour être également associé à notre concept d'intérêt
  2) Si le prior tend à confirmer l'association, il faut que nos voisins les plus proches est beaucoup de litterature pour rester majoritaire dans le prior

Pour le paramètre sample_size, comme on peut le constater plus il est élevé plus notre TPR et FPR diminue, le prior devenant trop puissant pour faire changer d'avis. Vue qu'il représente en gros l'indépendance, on a aucun positif.


### Focus analysis
Comme l'analyse globale que l'on a réalisée dans FORUM, la principale faiblesse de notre approche est que nous testons des relations et construisons des aprioris sur des relations sans connaitre la nature de cette relation. 
On pourrait donc très bien imaginer que 2 composés contribuent de manière équivalente à un prior, mais que l'un indique en fait une utilisation thérapeutique, alors que l'autre est un biomarker. Ici les deux composés contribuent de manière équivalente à l'apriori que l'on va poser sur la relation, mais décrivent deux relations différentes, que l'on utiliserait de manière équivalente pour construire un prior sur une relations sans nature précise, ce qui est problématique
Il faut donc arriver à extraire un set d'article associés à une relation spécifique (therapeutiques, biomarqueurs, etc ...) entre la molécule et la maladie
Pour cela on va utiliser les qualifiers associés aux termes MeSh des maladies pour notamment distinguer les mentions de composés dans des artciles dans des cas thérapeutiques, on de diagnostiques par exemple.

https://www.nlm.nih.gov/mesh/qualifiers_scopenotes.html

Quelques qualifiers qui pourrait nous être utiles: 

Tout n'est pas lié aux relations chemicals - **diseases**, mais 

- Chemically Induced:  	Used for biological phenomena, **diseases**, syndromes, congenital abnormalities, or symptoms caused by endogenous or exogenous substances.

- Complications: Used with **diseases** to indicate conditions that co-exist or follow, i.e., co-existing diseases, complications, or sequelae.

- Diagnosis: Used with **diseases** for all aspects of diagnosis, including examination, differential diagnosis and prognosis. Excludes diagnosis using imaging techniques (e.g. radiography, scintigraphy, and ultrasonography) for which "diagnostic imaging" is used.

- Drug Therapy:	Used with **disease** headings for the treatment of disease by the administration of drugs, chemicals, and antibiotics. For diet therapy and radiotherapy, use specific subheadings. Excludes immunotherapy for which "therapy" is used.

- Therapy: Used with **diseases** for therapeutic interventions except drug therapy, diet therapy, radiotherapy, and surgery, for which specific subheadings exist. The concept is also used for articles and books dealing with multiple therapies.

- Metabolism: Used with organs, cells and subcellular fractions, organisms, and **diseases** for biochemical changes and metabolism. It is used also with drugs and chemicals for catabolic changes (breakdown of complex molecules into simpler ones). For anabolic processes (conversion of small molecules into large), BIOSYNTHESIS is used. For enzymology and pharmacokinetics, use the specific subheadings.

- Used with drugs, biological preparations, and physical agents for their use in the prophylaxis and treatment of disease. It includes veterinary use.

- Toxicity: Used with drugs and chemicals for experimental human and animal studies of their ill effects. It includes studies to determine the margin of safety or the reactions accompanying administration at various dose levels. It is used also for exposure to environmental agents. Poisoning should be considered for life-threatening exposure to environmental agents.


Après dans notre cas ce sont les diseases qui sont représentés par des MeSH  et donc pour lesquelles on peut facilement utiliser les qualifiers. Dans le cas des composés, certes ils peuvent être décrits à l'aide de meSH indéxant la publication, néanmoins dans notre cas, c'est en utilisation les NCBI Eutilities que l'on va pouvoir les déterminer.
  

Si on regarde un peu les résultats

HE/Metabolism VS (HE/Metabolism - HE/Therapy&Co)

Pas de grosse diff, les molécules sont relativement les même mais apparaisse dans des ordres légèrement différents.


Lorsque l'on regarde sans filtre sur les qualifiers (les assos classiques) il y a deux molécules qui arrivent en top liste mais qui pourtant sont beaucoup bas dans la liste lorsque l'on regarde seulement metabolism par exemple, c'est "bilirubin" et "acetaminophen/paracetamol"

Pour "acetaminophen/paracetamol", on le retrouve notamment attaché au qualifier diagnostic, mais non pas car cette molécule est dosé pour diagnostiqué la maladie, mais car on diagnostique des HE chez des patients ayant reçu de forte dose de paracétamol car cela peut entrainé des liver failures dont l'HE.

Bilirupin quant à elle est bien dosé pour diagnostiquer l'HE, mais pourtant elle n'est pas mentionné souvent lorsque l'on discutte du métabolisme de l'HE

Il faut peut être voir à se focaliser plus sur 'diagnostic' mais en intégrant également des filtres sur "Complication" et "Chemically induiced" pour bien spécifier que l'on veut parler de diagnostic au sens : on dose la molécule pour diagnostiquer la pathologie.




### Limite sur le damping factor alpha: 

Lorsque l'on réalise la truncation de la matrice d'adjascence, on supprime les noeud cible et tout les liens qu'il créée, il est possible que l'on crée des noeuds flottants, si par exemple le noeud cible était le seul voisin de ce noeud.
Dans la construction de la matrice de proba, ce problème est réglé par les sink node: si aucun noeud n'est lié à mon noeud, alors toutes ces proba seront construite à l'aide de alpha. Même si alpha = 1, ça fonctionne.


Mais la chose ce complique si au lieu de créée un noeud flottant, par suppression du noeud cible, on crée un sous-composante du graph ! Par exemple une partie des voisins du noeud target forment une chaine. lors que l'on supprime le noeud target, on se retrouve avec un chaine 'flottante' qui n'est plus relié au reste du graph ! 
Le problème c'est qu'elle ne sont pas considéré comme des sink nodes, car chaque membre de la châine a au moins 1 voisin ...
Ainsi ce qui d'habitude corrige se problème ne peut pas intervenir ici car ce ne sont pas des sink nodes , ainsi dans le calcul de la matrice de proba $M = alpha * P + (alpha * a + (1 - alpha) * e) @ v.T$ 
La partie $(alpha * a + (1 - alpha) * e) @ v.T$ est égal à 0 car ce ne sont pas des sink nodes et alpha = 1. Ainsi il n'y a pas d'ajout de proba de restart qui nous assure de pouvoir redémarrer sur les voisins du noeud et donc d'avoir une structure connexe !!
Or si on a pas de structure connexe, on ne peut pas assurer la convergence du PPR !

Ainsi pour assurer la convergence du PPR et donc que notre graph, même après suppression de notre noeud target forment toujours une composante connexe, on a besoin de la probabilité de restart, car ce restart sur les voisins du noeuds cible, compense la suppression du noeud !
Ainsi il faut poser alpha entre [0, 1[ soit 0 compris mais 1 non-compris !!


Deplus, même dans le cas où l'on pose alpha = 0, si le noeud formait le seul chemin à travers nos voisins, grâce aux probabilité de restart on reconnecte tous les voisins entre eux


EDIT: Finalement, on observe ce phénomène pour toute valeur de alpha qui tend vers 1. On peut observer la création de pseudos sous-composantes dans les graphs lorsque l'on va utiliser des valeurs de alpha proche de 1 (0.99, etc ...)

l'inconvénient est que la création de ces sous-composantes perturbe le calcul des poids. Lorsqu'une sous-composante est crée (par retrait du noeud d'intérêt) une partie, proportionnelle au nombre de voisins formant cette sous-composante, des mentions de la cible est "piégée" dans cette sous composante ...
Si par exemple pour un composé à 3 voisins, la suppression de celui-ci entraine la création d'une clique avec deux de ses voisins, c'est 66% de ces mentions (2/3) qui seront piégés dans cette cliques.
Alors certe, le PPR convergera mais on aura 66% des mentions piégé dans la clique.
Ceci peut avoir une influence importante sur le calcul des poids associés à ces voisins appartenant à la clique. 
En effet, du au fort damping factor, ils vont recevoir un faible pourcentage des mentions venant de tous les autres composé MAIS vont recevoir 33% des mentions venant du composé cible précédent ! Ce qui fait que ce composé devient forcément majoritaire dans les poids, même si sa taille de corpus est beaucoup plus faible que ces autres voisins.
Pour donner un exemple, dans le cas des poids associés au composé M_C01041 dans Recon2.03_Compound_Graph, le composé le plus majoritaire dans son prior pour un alpha proche de 1, est M_dhdascb. Or M_dhdascb est un voisin direct de M_C01041 comme M_ascb_L qui pourtant à 5x plus de mentions annotés que M_dhdascb. 
Les deux étant des voisins direct, c'est M_ascb_L qui devrait être majoritaire dans le calcul des poids car il a beaucoup plus de litterature. Or, due à la création d'une pseudo-clique lors de la suppression de M_dhdascb avec un alpha proche de 1, 33% de ces mentions viennent enrichir le prior de M_C01041, ce qui fait que M_dhdascb est majortaire.

Pour contrer ce biais, on propose une autre version du calcul des probabilités.
La suppression temporaire du noeud dans le graph peut entrainer la formation de pseudo-clique, car lorsque alpha sera proche de 1, les voisins ne seront reliés entre eux que par la probabilité de restart qui sera très faible.
Or, en ne reliant que par la probabilité de restart, on ne considère pas que de base, il existe une connexion entre eux à travers le composé cible et donc on sous-estime énormément les probabilité de passer de l'un à l'autre. La suppression temporaire du noeud cible, nous a en fait supprimer des chemins qui connecter nos voisins entre eux. Simplement les relier à l'aide de la proba de restart sous-estime beaucoup les proba de transité entre voisins lorsque alpha est proche de 1.

On va donc réaliser le PPR, toujours en utilisant un vecteur de restart par rapport aux voisinnage, car :
  - C'est le voisinnage autour du composé cible qui nous intéresse
  -  Cela permet de maintenair la propriété que si alpha = 0, seul les voisins directs comptent !

On va réaliser le PPR classique sans supprimer le noeud d'intérêt !!
En revanche à la fin on va re-estimé les probabilités. En effet, on s'interresse à la proportion du temps passé, aux probabilité d'être sur un noeud, mais HORS du noeud cible.  
Ainsi, à la fin du calcul, on attribue la valeur 0 au noeud cible dans le vecteur de probabilité stationnaire et on re-calcule les probabilité. Ce vecteur re-estimé représente alors les probabilités, la proportion du temps passé sur les noeuds, hors du noeud cible !
Ce qui à nouveau nous permettra d'estimer à l'aide des poids la proportion de mentions partant du noeud cible qui vont aller influencer le prior des voisins, sans que le noeud ne s'influence lui même !!

Globalement le petit changement que j'ai pu observer en regardant les probabilités que l'on obtenaient sur notre graph de test avec l'urée (Cf. data/cytoscape/SFT_V2.cys) c'est qu'en utilisant le graph complet, cela semble favoriser le voisinnage proche de composé (ce que l'on souhaite en plus) car le fait de conserver le noeud cible, permet de garder des chemins entre nos voisins à traves ce cible au cours de la marche, qui avant été perdu quand on supprimait le noeud.


Les résultats semblent très peu affecter par ce changement dans la méthode. Par exemple pour les résultat sur l'encephalopathy hepatique, on a une corrélation de 0.9999948 entre les valeurs de Scores obtenus entre les deux méthodes !



 ## Notes sur la nouvelle approche :
 
### Problématique :

Pour donner un petit exemple, j'ai étudié le comportement du prior avec un damping factor = 0.1 pour le composé M_CE5236. Globalement c'est un composé en bout de chaîne qui se trouve dans une région ou aucun de ces voisins est annotés dans la littérature. La problème c'est que du coup, son principal contributeur (87%) c'est l'acide formique qui est bieeeeeen plus loin dans le réseau (shortest.path = 6)
Sur la schéma du graph (Cf notes/exemple_MCE5236_M_for.png), les couleurs représente une échelle pour la taille du corpus: blanc = pas d'article et rouge = STAR
Les deux composés en mode en jaune sont en bas,  M_CE5236 et en haut dans le centre du réseau l'acide formique  

Ce que l'on montre ici, c'est que dans le cas de composé situés dans des régions sans littérature ou avec des voisins proches avec une poignée d'articles, les éléments qui peuvent contribuer le plus au prior que l'on va construire pour ce composé peuvent être des composé à forte littérature (des STARs) mais qui se trouve très éloigné du composé d'intérêt. Dans le cas de M_CE5236, l'acide formique est situé à 6 réactions du composé ! Et pourtant il représente quasiment la totalit de l'information du prior. il y a donc un problème. Dans ces cas là on doit être capable de déterminer que l'on a construit le prior avec des composés très éloignés du composé d'intérêt et donc peu pertinent, ou alors, on abandonne la construction du prior dans ce cas là et on utilise le prior par défaut.

La première solution est simple, il suffirait par exemple d'estimer la distance à laquelle se trouve les contributeur du composé d'intérêt. La première chose que l'on peut proposer est de :
1) Calculer les shortest-path entre tous les composés avec Dijkstra par exemple.
2) Estimé une moyenne, pondérée par les poids dans le prior, de la distance entre les contributeurs et le composé d'intérêt. Cette information permettrait de juger de la légitimité des contributeurs pour construire le prior.

Néanmoins, parmis tout nos contributeur, l'un d'entre eux peut être une STAR un peu loin qui va biaisé cette moyenne car il contribura de manière significative au prior. Pour autant d'autre composé dans le voisinnage avec certe moins de litterature sont pourtant pertinents pour construire ce prior.
L'idée donc de donné un indice pour estimé cela ne semble pas pratique ...

La deuxième solution que j'ai imaginé est que dans mon modèle de mélange je pourrais ajouté notre prior "fictif" composé de $\nu$ articles avec une proba $\mu$ de parlé de MeSH, mais cette fois-ci non pas pour faire du shrinkage sur les proba, mais pour qu'il représente une composante de base dans les mentions qui composent le mélange. Ainsi de base, j'aurai au moins un set de $\nu$ articles dans ceux qui contribues à mon prior. Alors, les autres composés du réseau peuvent contribuer à ce prior, si ils ont suffisament de litterature, ils écrasent ce prior et prennnent le relai, sinon il ne sont pas majoritaire et le prior de base reste la composante essentielle dans le prior. Le problème avec cette solution est qu'elle va, je pense, faire re-émerger un des premier biais de la méthode. Intialement j'utilisais les poids issus du PPR et on la méthode était très conservative car dès lors qu'un des contributeur n'était pas favorable à l'association, même s'il n'avait pas beaucoup d'articles, vue que son poids était uniquement déterminé en fonction de son PPR, ils pouvait avoir une bonne influence sur le prior. Ainsi il suffisait qu'un composé avec 10 articles n'est pas vue le MeSH, bien que 5 autres des voisins soient en faveur de la relation pour que celui-ci biaise le prior en apportant au moins x% de chances que l'on ne discutte pas de ce MeSH. Avec l'utilisation des mentions et la prise en compte de la taille des corpus dans le poids, on avait réglé ce problème.
Mais on pourrait le re-créer en ajoutant ce set d'article *imaginaires* comme composante dans le modèle de mélange. Par exemple, si 1000 articles arrivent pour construire mon prior depuis les autres composé, ce set d'article *imaginaires* représenterait quand même 10% de mon prior et vue qu'il dirait qu'il n'y a pas de relation, le problème est le même !

La solution qui semble pertinente est de restreindre les contributeur au voisinage à N réactions. C'est un peu la même idée que le damping factor du PPR où l'on souhaite représenté un voisinnage de plus en plus proches, mais le problème est qu'à partir d'une $\alpha > 0$, en fait la probabilité qu'une mention partant de n'importe quel noeud pour arrivée sur n'importe quel autre noeud, est > 0 Et donc finalement tout composé contribue au prior de tout les autres. Quand on est dans des régions bien couvertes en littérature c'est pas un problème car les voisins proches sont largement marjoritaire et la contribution de composé éloigné est infime et négligeable. Mais dans des régions sans littérature, comme avec M_CE5236, ces faibles quantité de mentions qui arrive jusqu'au composé sont tout de même majoritaire.
L'idée serait donc de restreindre les contributeur en fonction de leur distance au composé cible, par exemple on autorise seulement les composés distant à moins de 3 réactions du composé cible à contribuer au prior. Pour cela, on utiliserait par exemple la matrice des plus court chemin et on l'utiliserait pour filtrer au moment du calcule des poids.
Ainsi, si dans un rayon de 3 réactions, aucun composé n'est pas capable de nous approté de l'inforamtion, le prior est abandonné et on utilisera le prior par défaut !

Ce serait cool d'estimé un bon paramètre K comme ça en utilisant le graph que l'on à fait pour la proximité des STARs en fonction de la taille des marches ! Il faut regarder la fonction par contre je vois des trucs byzarre ... Il me dit que M_tststerone atteint M_CE5236 avec une marche de taille 4, alors que Dijktra trouve que le plus court chemin c'est 6 ... donc à revoir ...


Alors il y une question de choix entre considéré le plus court chemin ou des marches ... L'inconvénient d'utiliser strictement le plus court chemin est que dans le cas de chaîne c'est un peu dommage car on sait que les articles vont forcément ce propager dans ce sens là et c'est peut être pertinent d'utiliser un composé même à plus de trois réactions si on se trouve dans une chaine.

Après ça implique par contre de définir ce fitlre autrement avec un % peut être du nombre de marche qui atteigne le composé cible ... mais ça fait un peu redondant avec le PR et ça semble difficle de poser un threshold la dessus sans avoir un truc complètement arbitraire.

Donc peut être testé en premie cet histoire de distance max que peut avoir un contributeur ça reflète un peu ce que je voulais faire avec le damping factor du PPR, mais il semble que ce ne soit pas suffisant dans certains cas. Après il faut je pense garder le PPR, car les random walks ça permet quand même d'approter plus de mentions à un voisin avec lequel on a plus de connections par exemple donc c'est cool !!

=========================================

Solution retenue:  La solution retenue est de créer une matrice de containte afin de sélectionner a quel autre composé, un coposé peut il contribuer de manière légitime. Aussi, on empêche toujours un composé de contribuer à lui-même.
Donc, pour contruire cette matrice de contrainte, on va demandé à ce que la probabilité d'atteindre un noeud (hors du noeud cible) soit au moins supérieure à $\frac{1}{n -1}$ (n étant le nombre total de noeud, on fait -1 car on compte pas le noeud lui-même) c'est à dire à la probabilité de sélectionner ce noeud au hazard. De manière plus formalisé, on demande donc que :
$\frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$ soit que la probabilité d'atteindre $k$ depuis $i$ (sans considéré les marches retournant à $i$) est supérieure à la proba de choisir $k$ au hasard parmis tout les noeud du réseau.

Ceci équivaut donc aussi à $\frac{\pi_{k,i}}{(1 - \pi_{i,i})} (n - 1) > 1$ ce qui offre une interprétation un peu plus inuitutive. Imaginons que chaque composé dispose de $n - 1$ cartons d'invitation, donc 1 pour chaque autre noeud dans le réseau, c'est comme si on simulait la propagation de ces cartons suivant la marche définie et ensuite on ne récupère que les composé qui ont reçu au moins un carton d'invitation. Ainsi les composés qui peuvent influé $k$ sont les composé pour lesquels $k$ à re_u au moins un carton d'invitation.

En se positionnant sur le composé cible, on va donc chercher à déterminer l'ensemble des noeuds qui semblent légitime d'après cette contrainte pour contribuer à son prior.
On va appelé les composés qui respectent cette régle les **contributeurs** de $k$ (les noeuds pouvant influencer $k$) que l'on notera $IN_k$. 
Ainsi, $\forall i \in CTB_k: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$. Vue que l'n empêche les composés de contribuer à eux-même, le composé $k$ ne peut pas faire partie de son voisinnage d'influence donc il ne contribue toujours pas au prior !

Symétriquement, on peut définir le voisinnage d'influence, noté $IN_i$ de chaque composé $i$ comme étant le set de composé qu'il peut légitimement influé au vue des contraintes que l'on a posé !

 $\forall k \in IN_i, k \neq i: \frac{\pi_{k,i}}{(1 - \pi_{i,i})} > \frac{1}{n -1}$

Où on impose que $i \notin IN_i$ 


Comme on le faisait avant, on va donc re-estimer les probabilité pour qu'elles ne se concentre sur les composé que $i$ peuvent légitimement influé. On va donc dire que :

$\forall k \in IN_i: P(I = k|C_i) = \frac{\pi_{k,i}}{\sum_{v \in IN_i} \pi_{v,i}}$ On re-proportionne les probas en se basant uniquement sur le set que $i$ peut influé.

$\forall k \notin IN_i: P(I = k|C_i) = 0$ avec obligatoirement $i \notin IN_i$ 


C'est comme ça que don donc définies les probabilités $P(I = k|C_i)$ que l'on va ensuite utilisées pour calculer les poids dns notre modèle : $P(C_i|I = k)$



# Trouver des exemples :
Cf. le google-sheet - "Exemple propagation"
- Trouver des cas où on peut avoir des bonnes prédiction avec la méthode de propagation, c'est facile on a pu en trouver, celui de l'asthme et 5-15DiHETE avec la lipoxin B4 est particulièrement intéressant.

- Trouver des cas de détection d'anomalie c'est assez difficle car en fait même en utilisant les relations pour lesquelles on a identifier de la weakness sont très probablement de bonnes associations grâce notre threshold de 1e-6 est très stringent et on l'a justement choisit pour avoir un taux super faible de faux-postitifs !!



### TESTER ou non sur d'autres réseau ?

Dans un premier temps je dirai que non ... Si Fabien ou autre n'en voit pas l'utilité je ne pense pas ...

L'idée est avant tout d'utiliser cette paroche pour prédire des biomakers donc en utilisant la littérature associées aux maladies. La méthode fonctionne avec toute la littérature mais il parait déjà moins pertinent d'essayer de prédire une fonction biologique (e.g vasodilateur) à partir du réseau. Là où autant on peut dire que si un précurseur est impliqu" dans une maladie alors on peut également regarder son produit, pour prédire une fonction biologique le lien est moins fort ...
Sachant que la majorité de la littérature PubMed est orienté autour de l'homme, la souris et le rat, en tout cas particulièrement pour ce qui est de la littérature sur les maladies, on pourrait testé sur ces réseaux car ils sont relativement proches. Beaucoup d'étude sur des maladies sont réalisés sur la souris et le rat donc pourquoi pas. Par contre sur un réseau comme la levure par exemple, ça parait étrange de testé car on propagerait de la littérature associé à une maladie sur le réseau de la levure, mais ce serait une maladie humaine qui n'affecte pas la levure, donc le contexte n'est je pense pas adapté.



### CDF to Odds

Une nouvelle feature qui a été proposé est le Odds: où $Odds = \frac{1-CDF}{CDF}$ ce qui nous donne le "cote" entre le fait que "la probabilité qu'un article discuttant de mon composé discutte aussi de mon MeSH" ($p$) **soit supérieur** à "la fréquence générale de ce MeSH dans des mentions de composés dans des articles" ($\mu$), soit $P(p > \mu) = 1-CDF$ par rapport à la probabilité qui soit inférieur à cette fréquence ($CDF$)

- Différence avec le Odds ratio: 
Le Odds ratio est un rapport de cotes entre les cotes obtenus sous la condition *test* et les cotes obtenues dans l'autre condition, souvent employé pour mesuré l'indépendance ou dans des tests exposures/control. Il s'agit donc des cotes obtenues pour l'exposure, normalisé par les cotes controls.
Dans notre cas, on test si la variable aléatoire $p$ est supérieure à $\mu$, pour nous rapprocher d'une mesure d'indépendance. Mais, on ne normalise pas par rapport unc condition control qui serait dans notre cas la distribution de $p$ lorsque l'on ne parle pas de composés, car on ne peut pas obtenir cette distribution dans notre framework.
En fait dans notre cas c'est déjà un peu pris en compte ! La normalisation dans le odds est là pour contrôler et déterminer si les odds observés pour ma condition test sont réellement significatif, il servent donc de background. Or dans notre cas, dans notre Odd, on compare directement $p$ à $\mu$ et ainsi $\mu$ la fréqunce générale dans la population joue ce rôle. Elle nous permet de nous fixer par rapport à un background, la fréquence générale du MeSH et ainsi regarder si notre densité de proba se trouve plus à gauche ou à droite de cete proba ! Donc je pense que cela joue un peu le rôle de la normalisation dans le odd ratio, en tout cas je pense que l'on peut aussi l'interpréter pour estimer la force d'une association.

Ensuite, le fait de calculer le Odds: où $Odds = \frac{1-CDF}{CDF}$, je trouve que ça permet de représenter à la fois la signficativité: si il est > 1 c'est que la majorité de mon posterior est pour $P(p > \mu)$. Je pense que cette valeur peut à la fois remplacer la CDF et le Log2FC en donnant une valeur résumant les 2.

On pourrait aussi calculé le Lod(Odds) ça se fait beaucoup.



### SKEWNESS

La skewneess d'une distibution se définie comme l'asymétrie d'une distribuion. Pour la distribution $Beta$, celle-ci n'est symétrique que $\alpha = \beta$ où on a donc mean = mode = 1/2 par ailleurs. Dès que l'quilibre $\alpha$, $\beta$ se perd, on voit apparaitre un déséquilibre dans la distribution.

![Skewness](Relationship_between_mean_and_median_under_different_skewness.png)

On parle de positive Skewness, lorsque que la queue droite de la distribution est plus longue que la queue gauche. A ce moment là, on a un déséquilibre de la masse de probabilité vers la gauche. Comme on peut le voir la mediane est inférieure à la moyenne. Or si la mediane ezst la valeur séparant 50% de la masse de la distribution, alors si la median est inférieure à la moyenne, on sait qu'il y a **plus** de 50% de la distribution qui est inférieure à la moyenne. 

C'est l'inverse pour la Negative Skewness


Ce que l'on sait (Cf. Wikipedia Beta Skewness) également c'est que lorsque $\alpha$ ou $\mu$ (associé à la distibution Beta) tend vers 0, alors la Skewness est positive et si $\alpha$ ou $\mu$ tend vers 1, alors c'est une Skewess negative. 


Or dans notre cas, on est exclusivement dans des cas où la fréquence générale des MeSH est $\le 0.5$ ! Ainsi notre premier prior qui est appliqué sur les contributeurs et possède comme moyenne $\mu$, la fréquence générale du MeSH a forcément une Skewness positive ! Ce qui signifie que la mojarité de la masse de proba est à gauche (inférieure) de la moyenne $\mu$. Ce comportement est valable que $\alpha$ soit > ou $\le$ à 1 comme dans nos cas ;)

Ainsi lorsque je pose ma distribution par défaut, finalement celle-ci impose une CDF "non-significative" par nature. En effet, la moyenne $\mu$ de ma distribution étant la fréquence $\mu$ générale du MeSH dans ma littérature, si j'ai une Skewness positive alors on sait que $P(p \le \mu)$ > 0.5, Donc Odds et CDF contre l'hypothèse.
Ceci implique que dès que la fréquence de mon MeSH est $\le 0.5$ alors si je n'ai aucune observation je rejetterai toujours mon hypothèse, en tout cas j'aurait une CDF > 0.5 ou un Odds $\le 0$.

Ce comportement est intérressant et fait même du sens par rapport à notre hypothèse : Plus un MeSH est rare (soit $\mu \rightarrow 0$) plus la CDF du prior de base est grande, donc plus on rejette ! Ainsi plus un MeSH est rare plus par défaut on ne s'attend pas à être associé avec lui et il va falloir amené des observations pour faire dévier ce prior. En revanche, plus la fréquence du MeSH augmente plus la CDF va diminuer et donc plus apriori on est enclin a considérer que l'on pourrait y être associé.

Cette comportementest important car en effet on ne voudrait pas considérer les même chances aprior d'être associé à un MeSH très rare ou à un MeSH très fréquent. De base c'est plus de chance d'être associé à *Neoplasm* par exemple qu'une maladie rare. Le fait que la distribution Beta soit déséquilibrer prend en compte cela naturellement. **Ce déséquilibre indique que: Plus la fréquence générale d'un MeSH est faible moins j'ai de chance que ma probabilité d'en parler soit supérieure à sa fréquence générale, a priori (et inversement)**

## Détails sur les TPR, FPR a seuil proches de 0.5 pour des valeurs de $\nu$ élevée !

La skewness influence également nos résultats lorsque choisit un threshold sur la CDF à 0.5 (ou 0 sur le logOdds). En effet, lorsque l'on va utiliser un $\nu$ très grand (10e5 ou plus) on va réduire l'effet de la skewness, c'est comme si on recentrait la courbe. Ainsi cela fait tendre toutes les CDF vers 0.5 en moyenne et les Log2FC par exemple vers 0. Dans cette configuration, toutes les associations pour lesquels les contributors tendent un peu en faire pencher l'association côté positive par des co-occurence fréquente font facilement faire passer la CDF sous la barre des 0.5, mais cela peut également arrtiver pour des faux positif ! L'effet qui est non-souhaitable, c'est que ça a tendance à 'lisser' toutes nos prédictions ! Elles sont toutes tirés vers une CDF proches de 0.5 ...

En fait ce que l'on observe c'est que globalement on a plus de positif qui sont "trouvés" au seuil de CDF < 0.5, mais en revanche ayant lissé nos résultats on a moins d'association vraiment significative avec un CDF < 0.1 par exemple ! C'est donc plus facile de récupérer les positives, on a un meilleur TPR par contre toutes nos prédictions se ressemble car écraser par le prior .... initial. Deplus on récupère aussi plus de FP !Dans tout les cas mettre des valeurs de $\nu$ trop hautes écrase l'expression des contributeur et du composé en lui-même !

Pareil pour le LogOdds, on veut pas juste sortir des associés en mode 50/50 !

Aussi, on veut quand même être suffisament précis car on sait que lorsque l'on va testé sur tout nos MeSH: par exemple les 5000 MeSH disease, la majorité des associations que l'on testent sont négatives car en réalité un métabolites ne devrait être associé qu'à un poignet de maladie, on veut donc chercher à miniser au maximum le nombre de Faux-positifs. J'ai par exemple 4900 associations négatives et parmis elles je veux avoir le plus faible tôt de Faux-positif car en terme de nombre de prédictions brutes, ça ferait beaucoup de mauvaise prédicition par exemple à un taux de 0.1 !!



Dans le papier An empirical Bayes Mixture method for effect size and false discovery rate, il montre justement ce ue je fais !
Dans cet article p.428, leur $p_j(H_j)$ correspond à mon $W_{i,k}$ où à l'intérieure leur $f^{(j)}(H_i; N_i)$ correspond à mon $C_{i,k}$ (moi je mets $k$ pour indiquer le targeted compound mais pas obligé) !

D'alleurs ils propose une autre interprétation de $W_{i,k}$ (pour eux  $p_j(H_j)$), ils disent: Let $p_j(H_j) = \frac{\pi_j f^{(j)}(z)}{f(z)}$ be the posterior probability that $z$ came from group $j$, and $g_j(δ|z)$ be the posterior for the $j$ th group (that is, the posterior corresponding to prior $g_j$ ) -> Voir l'article mais $g_j(δ|z)$ c'est l'équivalent de notre $f^{(2)}_i(p)$

(où $f(z) = \sum_j \pi_j f^{(j)}(z)$, c'est la distribution marginale comme nous lorsque l'on fait $\sum_{i' \in T_k} w_{i',k} C_{i', k}$)

En effet, on peut peut être faire la même interprétation dans notre cas :
$W_{i, k} = \frac{ w_{i, k} C_{i, k}} { \sum_{i' \in T_k} w_{i',k} C_{i', k} }  \text{ with } C_{i, k} = \binom{n_k}{c_{k}} \frac{B(\alpha^{(2)}_{i}, \beta^{(2)}_{i})}{B(\alpha^{(1)}_{i}, \beta^{(1)}_{i})}$ En notant que  $C_{i, k}$ représente la probability d'observer les données ($k$ successes over $n$ trials) depuis une distribution Beta-Binomial avec un prior sur $p$ (the success probability) définis par $B(\alpha^{(1)}_{i}, \beta^{(1)}_{i})$.



# Stein estimator et l'explication du paradoxe: Stein's paradox in statistics. Dans le paradox de stein on s'intéresse à l'estimation de plusieurs moyenne k > 3, pour des variables qui peuvent être complètement indépendante. Ce que montre Stein, et qui est paradoxal, c'est que en "shrinkant" les estimateurs des moyennes individuelles vers la "grand-average" (la moyenne des moyennes) ont obtient *marginalement" de meilleurs estimateurs des moyennes individuelles, même si de base les différentes variables n'ont rien à voir entre elles. De ce que j'ai compris c'est bien un estimateur marginal c'est à dire que c'est mieux globalement, mais que ce n'est pas forcémnt un meilleur estimateur de toutes les moyennes individuelles. Il parle aussi d'admissibilité qui peut un peu faire pensser à la notion de pareto, ca d qu'un estimateur est admissible si aucun autre n'est meilleur ou aussi bon que lui sur tout l'espace du paramètre estimé. D'ailleurs Stein n'est pas n'ont plus admissible ^^

Le rapport à le Bayesian shrinkage est qu'il montre que l'on peut retrouver l'estimateur de Stein avec la règle de Bayes, en utilisant un prior sur la moyenne issue de toutes les données.

Lire aussi https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2872278/ pour un petit historique

Même si dans l'article on utilise pas nécéssaire la moyenne, on montre dans les exemples que l'estimateur de la moyenne que l'on obtient est moins biaisé. effectivement par définit l'estimateur bayesien, la posterior mean ($E[\sigma|x]$) sui minimise le minimum mean square error (MMSE)


https://en.wikipedia.org/wiki/Bayes_estimator

https://www.youtube.com/watch?v=b1GxZdFN6cY

https://www.youtube.com/watch?v=fjqd0gG-5OE

Lire aussi les docs dans /home/mxdelmas/Documents/Thèse/cours/stats/Stein_bayes_risk

Ce qui faut retyenir c'est que l'estimateur à posterior est celui qui minimise le risque bayesien